---
title: "Random Forest Study - Thu·∫≠t to√°n Ensemble Learning v·ªõi Bootstrap Aggregating"
pubDatetime: 2025-09-21T10:00:00Z
featured: false
description: "T√¨m hi·ªÉu chi ti·∫øt v·ªÅ Random Forest, thu·∫≠t to√°n ensemble learning s·ª≠ d·ª•ng bootstrap aggregating v√† feature bagging"
tags: ["machine-learning", "random-forest", "ensemble", "bootstrap", "bagging", "algorithm"]
---

# Random Forest Study

> **üìö Repo tham kh·∫£o:** [https://github.com/tandat8896/ml-from-the-scartch/tree/master/random_forest](https://github.com/tandat8896/ml-from-the-scartch/tree/master/random_forest)

## Thu·∫≠t to√°n Random Forest

Random Forest l√† m·ªôt thu·∫≠t to√°n **ensemble learning** k·∫øt h·ª£p nhi·ªÅu **decision trees** th√¥ng qua:
- **Bootstrap Aggregating (Bagging)**: T·∫°o nhi·ªÅu subset ng·∫´u nhi√™n t·ª´ dataset
- **Feature Bagging**: Ch·ªçn ng·∫´u nhi√™n subset features cho m·ªói tree
- **Majority Vote**: K·∫øt h·ª£p predictions t·ª´ t·∫•t c·∫£ trees

---

## C√¥ng th·ª©c to√°n h·ªçc c·ªßa Random Forest

### **B∆∞·ªõc 1: Bootstrap Sampling**

T·∫°o N bootstrap samples t·ª´ dataset g·ªëc D:
$$D_i = \mathrm{Bootstrap}(D), \quad i = 1, 2, \ldots, n_{estimators}$$

M·ªói bootstrap sample c√≥ k√≠ch th∆∞·ªõc b·∫±ng dataset g·ªëc, ƒë∆∞·ª£c t·∫°o b·∫±ng c√°ch:
- **Sampling with replacement** (c√≥ ho√†n l·∫°i)
- M·ªôt s·ªë samples c√≥ th·ªÉ xu·∫•t hi·ªán nhi·ªÅu l·∫ßn
- M·ªôt s·ªë samples c√≥ th·ªÉ kh√¥ng xu·∫•t hi·ªán (out-of-bag samples)

### **B∆∞·ªõc 2: Feature Bagging**

Cho m·ªói tree i, ch·ªçn ng·∫´u nhi√™n subset features:
$$\text{max\_features} = \sqrt{\text{total\_features}} \quad \text{(default)}$$

$$\text{selected\_features}_i = \text{RandomChoice}(\text{all\_features}, \text{max\_features})$$

### **B∆∞·ªõc 3: Tree Training**

M·ªói tree ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n:
- **Bootstrap sample** $D_i$
- **Selected features** $\text{selected\_features}_i$

$$T_i = \mathrm{TrainDecisionTree}(D_i, \mathrm{selected\_features}_i)$$

### **B∆∞·ªõc 4: Prediction Aggregation**

**Classification (Majority Vote):**
$$\hat{y} = \mathrm{mode}\{T_1(x), T_2(x), \ldots, T_n(x)\}$$

**Regression (Mean):**
$$\hat{y} = \frac{1}{n} \sum_{i=1}^{n} T_i(x)$$

---

## V√≠ D·ª• T√≠nh Tay - Random Forest Classification

### **Dataset Classification**

| ID | Age (X1) | Income (X2) | Label (y) |
|:---:|:---:|:---:|:---:|
| 1 | 25 | 30 | 1 |
| 2 | 30 | 50 | 1 |
| 3 | 35 | 40 | 0 |
| 4 | 40 | 60 | 0 |
| 5 | 45 | 70 | 1 |
| 6 | 50 | 80 | 0 |

**M·ª•c ti√™u:** D·ª± ƒëo√°n Label d·ª±a tr√™n Age v√† Income v·ªõi 3 trees

---

### **Step 1: Bootstrap Sampling**

**T·∫°o 3 bootstrap samples t·ª´ dataset g·ªëc (6 samples):**

| Tree | Bootstrap Sample | Selected Samples |
|:---:|:---:|:---:|
| 1 | D1 | [1, 2, 3, 4, 5, 6] ‚Üí [1, 1, 3, 4, 5, 6] |
| 2 | D2 | [1, 2, 3, 4, 5, 6] ‚Üí [1, 2, 2, 4, 5, 6] |
| 3 | D3 | [1, 2, 3, 4, 5, 6] ‚Üí [1, 2, 3, 4, 6, 6] |

**Gi·∫£i th√≠ch Bootstrap Sampling:**
- M·ªói sample c√≥ th·ªÉ ƒë∆∞·ª£c ch·ªçn nhi·ªÅu l·∫ßn (replacement)
- M·ªôt s·ªë samples c√≥ th·ªÉ kh√¥ng ƒë∆∞·ª£c ch·ªçn (out-of-bag)
- K√≠ch th∆∞·ªõc m·ªói bootstrap sample = 6 (b·∫±ng dataset g·ªëc)

---

### **Step 2: Feature Bagging**

**T√≠nh max_features:**
$$\text{max\_features} = \sqrt{2} = 1.41 \approx 1$$

**Ch·ªçn features cho t·ª´ng tree:**

| Tree | Selected Features | Feature Indices |
|:---:|:---:|:---:|
| 1 | [Age] | [0] |
| 2 | [Income] | [1] |
| 3 | [Age] | [0] |

---

### **Step 3: Tree Training**

#### **Tree 1: Features [Age]**

**Bootstrap Sample D1:**
| ID | Age | Label |
|:---:|:---:|:---:|
| 1 | 25 | 1 |
| 1 | 25 | 1 |
| 3 | 35 | 0 |
| 4 | 40 | 0 |
| 5 | 45 | 1 |
| 6 | 50 | 0 |

**T√¨m best split cho Age:**
| Threshold | Left (‚â§) | Right (>) | Left Label | Right Label | Gini | Split Info | Gain |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 27.5 | [1,1] | [3,4,5,6] | [1,1] | [0,0,1,0] | 0.0 | 0.0 | 0.0 |
| 32.5 | [1,1,3] | [4,5,6] | [1,1,0] | [0,1,0] | 0.0 | 0.0 | 0.0 |
| 37.5 | [1,1,3,4] | [5,6] | [1,1,0,0] | [1,0] | 0.0 | 0.0 | 0.0 |
| 42.5 | [1,1,3,4,5] | [6] | [1,1,0,0,1] | [0] | 0.0 | 0.0 | 0.0 |
| 47.5 | [1,1,3,4,5,6] | [] | [1,1,0,0,1,0] | [] | 0.0 | 0.0 | 0.0 |

**T·∫•t c·∫£ thresholds ƒë·ªÅu c√≥ Gini = 0, ch·ªçn threshold = 32.5**

**Tree 1 Structure:**
```
Root: Age <= 32.5?
‚îú‚îÄ‚îÄ Yes: Label = 1 (samples 1,1,3)
‚îî‚îÄ‚îÄ No: Label = 0 (samples 4,5,6)
```

#### **Tree 2: Features [Income]**

**Bootstrap Sample D2:**
| ID | Income | Label |
|:---:|:---:|:---:|
| 1 | 30 | 1 |
| 2 | 50 | 1 |
| 2 | 50 | 1 |
| 4 | 60 | 0 |
| 5 | 70 | 1 |
| 6 | 80 | 0 |

**T√¨m best split cho Income:**
| Threshold | Left (‚â§) | Right (>) | Left Label | Right Label | Gini | Split Info | Gain |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 35 | [1] | [2,2,4,5,6] | [1] | [1,1,0,1,0] | 0.0 | 0.0 | 0.0 |
| 45 | [1,2,2] | [4,5,6] | [1,1,1] | [0,1,0] | 0.0 | 0.0 | 0.0 |
| 55 | [1,2,2,4] | [5,6] | [1,1,1,0] | [1,0] | 0.0 | 0.0 | 0.0 |
| 65 | [1,2,2,4,5] | [6] | [1,1,1,0,1] | [0] | 0.0 | 0.0 | 0.0 |
| 75 | [1,2,2,4,5,6] | [] | [1,1,1,0,1,0] | [] | 0.0 | 0.0 | 0.0 |

**T·∫•t c·∫£ thresholds ƒë·ªÅu c√≥ Gini = 0, ch·ªçn threshold = 45**

**Tree 2 Structure:**
```
Root: Income <= 45?
‚îú‚îÄ‚îÄ Yes: Label = 1 (samples 1,2,2)
‚îî‚îÄ‚îÄ No: Label = 0 (samples 4,5,6)
```

#### **Tree 3: Features [Age]**

**Bootstrap Sample D3:**
| ID | Age | Label |
|:---:|:---:|:---:|
| 1 | 25 | 1 |
| 2 | 30 | 1 |
| 3 | 35 | 0 |
| 4 | 40 | 0 |
| 6 | 50 | 0 |
| 6 | 50 | 0 |

**T√¨m best split cho Age:**
| Threshold | Left (‚â§) | Right (>) | Left Label | Right Label | Gini | Split Info | Gain |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 27.5 | [1] | [2,3,4,6,6] | [1] | [1,0,0,0,0] | 0.0 | 0.0 | 0.0 |
| 32.5 | [1,2] | [3,4,6,6] | [1,1] | [0,0,0,0] | 0.0 | 0.0 | 0.0 |
| 37.5 | [1,2,3] | [4,6,6] | [1,1,0] | [0,0,0] | 0.0 | 0.0 | 0.0 |
| 42.5 | [1,2,3,4] | [6,6] | [1,1,0,0] | [0,0] | 0.0 | 0.0 | 0.0 |
| 47.5 | [1,2,3,4,6,6] | [] | [1,1,0,0,0,0] | [] | 0.0 | 0.0 | 0.0 |

**T·∫•t c·∫£ thresholds ƒë·ªÅu c√≥ Gini = 0, ch·ªçn threshold = 37.5**

**Tree 3 Structure:**
```
Root: Age <= 37.5?
‚îú‚îÄ‚îÄ Yes: Label = 1 (samples 1,2,3)
‚îî‚îÄ‚îÄ No: Label = 0 (samples 4,6,6)
```

---

### **Step 4: Prediction Aggregation**

**D·ª± ƒëo√°n cho t·ª´ng sample (Majority Vote):**

| ID | Age | Income | T1 | T2 | T3 | Majority Vote | True Label |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 1 | 25 | 30 | 1 | 1 | 1 | 1 | 1 |
| 2 | 30 | 50 | 1 | 1 | 1 | 1 | 1 |
| 3 | 35 | 40 | 0 | 1 | 1 | 1 | 0 |
| 4 | 40 | 60 | 0 | 0 | 0 | 0 | 0 |
| 5 | 45 | 70 | 0 | 0 | 0 | 0 | 1 |
| 6 | 50 | 80 | 0 | 0 | 0 | 0 | 0 |

**Gi·∫£i th√≠ch:**
- **T1**: Age ‚â§ 32.5 ‚Üí 1, Age > 32.5 ‚Üí 0
- **T2**: Income ‚â§ 45 ‚Üí 1, Income > 45 ‚Üí 0
- **T3**: Age ‚â§ 37.5 ‚Üí 1, Age > 37.5 ‚Üí 0
- **Majority Vote**: L·∫•y gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu nh·∫•t

**Accuracy:** 4/6 = 66.7%

---

## **T√≥m t·∫Øt Random Forest**

### **Quy tr√¨nh ho√†n ch·ªânh:**

1. **Bootstrap Sampling** - T·∫°o nhi·ªÅu subset ng·∫´u nhi√™n t·ª´ dataset g·ªëc
2. **Feature Bagging** - Ch·ªçn ng·∫´u nhi√™n subset features cho m·ªói tree
3. **Parallel Training** - Hu·∫•n luy·ªán c√°c decision trees ƒë·ªôc l·∫≠p
4. **Majority Vote** - K·∫øt h·ª£p predictions b·∫±ng majority vote

**∆Øu ƒëi·ªÉm c·ªßa Random Forest:**
- **Gi·∫£m overfitting** nh·ªù bootstrap sampling
- **TƒÉng accuracy** nh·ªù ensemble learning
- **X·ª≠ l√Ω missing values** t·ªët
- **Feature importance** c√≥ th·ªÉ t√≠nh ƒë∆∞·ª£c
- **Parallel training** nhanh ch√≥ng
- **Robust** v·ªõi outliers

**Nh∆∞·ª£c ƒëi·ªÉm:**
- **Memory usage** cao v·ªõi nhi·ªÅu trees
- **Interpretability** th·∫•p h∆°n single tree
- **Training time** tƒÉng theo s·ªë trees
- **C√≥ th·ªÉ overfitting** v·ªõi dataset nh·ªè

---

## **So s√°nh v·ªõi c√°c thu·∫≠t to√°n kh√°c**

| ƒê·∫∑c ƒëi·ªÉm | Random Forest | AdaBoost | XGBoost | LightGBM |
|:---:|:---:|:---:|:---:|:---:|
| **Method** | Bagging | Boosting | Boosting | Boosting |
| **Training** | Parallel | Sequential | Sequential | Sequential |
| **Sampling** | Bootstrap | Weighted | All samples | GOSS |
| **Features** | Random subset | All features | All features | All features |
| **Aggregation** | Majority Vote | Weighted | Weighted | Weighted |
| **Speed** | Fast | Medium | Fast | Fastest |
| **Memory** | High | Medium | High | Low |
| **Accuracy** | Good | Good | Excellent | Excellent |

---

## **Hyperparameters quan tr·ªçng**

- **`n_estimators`**: S·ªë l∆∞·ª£ng trees (100-1000)
- **`max_features`**: S·ªë features m·ªói tree (sqrt, log2, ho·∫∑c s·ªë c·ªë ƒë·ªãnh)
- **`max_depth`**: ƒê·ªô s√¢u t·ªëi ƒëa c·ªßa tree (None, 10-20)
- **`min_samples_split`**: S·ªë samples t·ªëi thi·ªÉu ƒë·ªÉ split (2-10)
- **`min_samples_leaf`**: S·ªë samples t·ªëi thi·ªÉu ·ªü leaf (1-5)
- **`bootstrap`**: C√≥ s·ª≠ d·ª•ng bootstrap sampling (True/False)
- **`random_state`**: Seed cho reproducibility

---

## **Khi n√†o n√™n s·ª≠ d·ª•ng Random Forest**

‚úÖ **N√™n d√πng khi:**
- C·∫ßn model ·ªïn ƒë·ªãnh v√† robust
- Dataset c√≥ nhi·ªÅu features
- C·∫ßn feature importance
- Mu·ªën gi·∫£m overfitting
- C·∫ßn model interpretable m·ªôt ph·∫ßn
- C√≥ outliers trong d·ªØ li·ªáu

‚ùå **Kh√¥ng n√™n d√πng khi:**
- Dataset r·∫•t nh·ªè (<100 samples)
- C·∫ßn accuracy cao nh·∫•t
- Memory h·∫°n ch·∫ø
- C·∫ßn model r·∫•t nhanh
- C·∫ßn interpretability cao

---

## **K·∫øt lu·∫≠n**

Random Forest l√† m·ªôt thu·∫≠t to√°n ensemble learning m·∫°nh m·∫Ω, k·∫øt h·ª£p s·ª©c m·∫°nh c·ªßa nhi·ªÅu decision trees th√¥ng qua bootstrap aggregating v√† feature bagging. V·ªõi kh·∫£ nƒÉng gi·∫£m overfitting, x·ª≠ l√Ω t·ªët c√°c v·∫•n ƒë·ªÅ v·ªÅ features v√† cho k·∫øt qu·∫£ ·ªïn ƒë·ªãnh, Random Forest l√† l·ª±a ch·ªçn tuy·ªát v·ªùi cho nhi·ªÅu b√†i to√°n classification v√† regression th·ª±c t·∫ø.

**ƒêi·ªÉm m·∫°nh ch√≠nh:**
- **Ensemble learning** hi·ªáu qu·∫£
- **Bootstrap sampling** gi·∫£m overfitting  
- **Feature bagging** tƒÉng ƒëa d·∫°ng
- **Parallel training** nhanh ch√≥ng
- **Robust** v·ªõi noise v√† outliers
- **Majority vote** ƒë∆°n gi·∫£n v√† hi·ªáu qu·∫£