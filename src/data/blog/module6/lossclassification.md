---
title: "Loss Functions cho Classification: HÃ nh trÃ¬nh tá»« \"Zero-One\" Ä‘áº¿n \"Cross-Entropy\""
pubDatetime: 2025-01-15T12:00:00Z
featured: false
description: "TÃ¬m hiá»ƒu chi tiáº¿t vá» cÃ¡c loáº¡i loss functions cho classification: Zero-One Loss, Hinge Loss, Logistic Loss, Cross-Entropy, Focal Loss, Label Smoothing vá»›i vÃ­ dá»¥ cá»¥ thá»ƒ vÃ  hÆ°á»›ng dáº«n lá»±a chá»n phÃ¹ há»£p"
tags: ["Machine Learning", "Classification", "Loss Functions", "Cross-Entropy", "Deep Learning"]
---

# Loss Functions cho Classification: HÃ nh trÃ¬nh tá»« "Zero-One" Ä‘áº¿n "Cross-Entropy"

Xin chÃ o cÃ¡c báº¡n! HÃ´m nay mÃ¬nh muá»‘n chia sáº» vá» má»™t chá»§ Ä‘á» mÃ  mÃ¬nh Ä‘Ã£ dÃ nh ráº¥t nhiá»u thá»i gian Ä‘á»ƒ tÃ¬m hiá»ƒu: **Loss Functions cho bÃ i toÃ¡n Classification**.

Khi má»›i báº¯t Ä‘áº§u vá»›i Machine Learning, mÃ¬nh Ä‘Ã£ tá»«ng nghÄ©: "Loss function thÃ¬ cÃ³ gÃ¬ Ä‘Ã¢u, cá»© dÃ¹ng Cross-Entropy lÃ  xong!" NhÆ°ng sau nhiá»u láº§n "Ä‘au Ä‘áº§u" vá»›i cÃ¡c bÃ i toÃ¡n khÃ¡c nhau - tá»« binary classification Ä‘áº¿n multi-label - mÃ¬nh má»›i nháº­n ra ráº±ng viá»‡c chá»n Ä‘Ãºng loss function khÃ´ng há» Ä‘Æ¡n giáº£n nhÆ° váº­y.

Trong bÃ i viáº¿t nÃ y, mÃ¬nh sáº½ cÃ¹ng cÃ¡c báº¡n khÃ¡m phÃ¡ cÃ¡c loáº¡i loss functions cho classification, tá»« nhá»¯ng cÃ¡i cÆ¡ báº£n nháº¥t nhÆ° Zero-One Loss Ä‘áº¿n nhá»¯ng ká»¹ thuáº­t hiá»‡n Ä‘áº¡i nhÆ° Label Smoothing. Má»—i loss function Ä‘á»u cÃ³ nhá»¯ng Æ°u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm riÃªng, vÃ  mÃ¬nh sáº½ chia sáº» nhá»¯ng tráº£i nghiá»‡m thá»±c táº¿ mÃ  mÃ¬nh Ä‘Ã£ gáº·p pháº£i.

## I. NhÃ³m HÃ m Loss CÆ¡ báº£n - Báº¯t Ä‘áº§u tá»« nhá»¯ng Ä‘iá»u Ä‘Æ¡n giáº£n nháº¥t

### 1. Zero-One Loss Function - ÄÆ¡n giáº£n nhÆ°ng... khÃ´ng thá»ƒ dÃ¹ng Ä‘Æ°á»£c!

MÃ¬nh muá»‘n báº¯t Ä‘áº§u vá»›i Zero-One Loss vÃ¬ Ä‘Ã¢y lÃ  loss function Ä‘Æ¡n giáº£n nháº¥t mÃ  báº¡n cÃ³ thá»ƒ nghÄ© Ä‘áº¿n. NÃ³ chá»‰ tráº£ vá» 0 náº¿u dá»± Ä‘oÃ¡n Ä‘Ãºng vÃ  1 náº¿u dá»± Ä‘oÃ¡n sai. Nghe cÃ³ váº» hoÃ n háº£o, pháº£i khÃ´ng? NhÆ°ng thá»±c táº¿ thÃ¬...

**CÃ´ng thá»©c:**

$$L_{0-1}(y, f(x)) = \begin{cases}
    0 & \text{náº¿u } f(x) \cdot y \geq 0 \\
    1 & \text{náº¿u } f(x) \cdot y < 0
\end{cases}$$

Trong Ä‘Ã³:
- `y`: NhÃ£n thá»±c táº¿ (1 hoáº·c -1)
- `f(x)`: Äiá»ƒm sá»‘ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh
- `f(x) Â· y â‰¥ 0`: Dá»± Ä‘oÃ¡n Ä‘Ãºng (cÃ¹ng dáº¥u)
- `f(x) Â· y < 0`: Dá»± Ä‘oÃ¡n sai (khÃ¡c dáº¥u)

**VÃ­ dá»¥ cá»¥ thá»ƒ:**

Giáº£ sá»­ chÃºng ta cÃ³ 5 máº«u phÃ¢n loáº¡i nhá»‹ phÃ¢n:

| Máº«u | NhÃ£n thá»±c táº¿ (y) | Äiá»ƒm sá»‘ dá»± Ä‘oÃ¡n f(x) | f(x) Â· y | Zero-One Loss |
|-----|-----------------|---------------------|----------|---------------|
| 1   | 1               | 0.8                 | 0.8      | 0 (Ä‘Ãºng)      |
| 2   | 1               | -0.3                | -0.3     | 1 (sai)       |
| 3   | -1              | -0.9                | 0.9      | 0 (Ä‘Ãºng)      |
| 4   | -1              | 0.2                 | -0.2     | 1 (sai)       |
| 5   | 1               | 0.05                | 0.05     | 0 (Ä‘Ãºng)      |

**Giáº£i thÃ­ch:** 
- Máº«u 1, 3, 5: Dá»± Ä‘oÃ¡n Ä‘Ãºng â†’ Loss = 0
- Máº«u 2, 4: Dá»± Ä‘oÃ¡n sai â†’ Loss = 1
- Tá»•ng Loss = 0 + 1 + 0 + 1 + 0 = **2**

**VÃ­ dá»¥ vá» thiáº¿u tÃ­nh linh hoáº¡t:**

**TrÆ°á»ng há»£p 1: Dá»± Ä‘oÃ¡n gáº§n Ä‘Ãºng (gáº§n ngÆ°á»¡ng 0)**
- NhÃ£n thá»±c táº¿: y = 1
- Äiá»ƒm sá»‘ dá»± Ä‘oÃ¡n: f(x) = 0.01 (ráº¥t gáº§n 0, nhÆ°ng váº«n Ä‘Ãºng)
- Zero-One Loss = **0**

**TrÆ°á»ng há»£p 2: Dá»± Ä‘oÃ¡n sai nhÆ°ng ráº¥t gáº§n ngÆ°á»¡ng**
- NhÃ£n thá»±c táº¿: y = 1
- Äiá»ƒm sá»‘ dá»± Ä‘oÃ¡n: f(x) = -0.01 (ráº¥t gáº§n 0, nhÆ°ng sai)
- Zero-One Loss = **1**

**TrÆ°á»ng há»£p 3: Dá»± Ä‘oÃ¡n sai ráº¥t xa**
- NhÃ£n thá»±c táº¿: y = 1
- Äiá»ƒm sá»‘ dá»± Ä‘oÃ¡n: f(x) = -10 (sai ráº¥t xa)
- Zero-One Loss = **1**

ÄÃ¢y chÃ­nh lÃ  váº¥n Ä‘á» lá»›n nháº¥t cá»§a Zero-One Loss! Cáº£ trÆ°á»ng há»£p 2 vÃ  3 Ä‘á»u cÃ³ Loss = 1, máº·c dÃ¹ má»©c Ä‘á»™ sai khÃ¡c nhau ráº¥t nhiá»u! Zero-One Loss khÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c Ä‘iá»u nÃ y. MÃ¬nh Ä‘Ã£ tá»«ng thá»­ dÃ¹ng nÃ³ vÃ  nháº­n ra ráº±ng nÃ³ khÃ´ng thá»ƒ sá»­ dá»¥ng trong gradient descent vÃ¬ khÃ´ng cÃ³ gradient. ÄÃ¢y lÃ  lÃ½ do táº¡i sao chÃºng ta cáº§n cÃ¡c loss functions khÃ¡c!

**Báº£ng Æ¯u Ä‘iá»ƒm vÃ  NhÆ°á»£c Ä‘iá»ƒm:**

| Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|---------|------------|
| ÄÆ¡n giáº£n, dá»… hiá»ƒu | Thiáº¿u tÃ­nh linh hoáº¡t trong viá»‡c Ä‘áº·t ngÆ°á»¡ng |
| Trá»±c tiáº¿p pháº£n Ã¡nh Ä‘á»™ chÃ­nh xÃ¡c | Bá» qua khoáº£ng cÃ¡ch: khÃ´ng phÃ¢n biá»‡t sai gáº§n hay sai xa |
| KhÃ´ng cáº§n gradient (khÃ´ng thá»ƒ tá»‘i Æ°u hÃ³a trá»±c tiáº¿p) | KhÃ´ng thá»ƒ sá»­ dá»¥ng trong gradient descent |
| PhÃ¹ há»£p cho Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng | KhÃ´ng cung cáº¥p thÃ´ng tin vá» Ä‘á»™ tá»± tin cá»§a dá»± Ä‘oÃ¡n |
| Dá»… tÃ­nh toÃ¡n | KhÃ´ng khuyáº¿n khÃ­ch mÃ´ hÃ¬nh cáº£i thiá»‡n dáº§n |

---

### 2. Exponential Loss Function - Kháº¯c phá»¥c Zero-One nhÆ°ng láº¡i gáº·p váº¥n Ä‘á» khÃ¡c

Sau khi nháº­n ra Zero-One Loss khÃ´ng thá»ƒ dÃ¹ng Ä‘Æ°á»£c, mÃ¬nh Ä‘Ã£ tÃ¬m Ä‘áº¿n Exponential Loss nhÆ° má»™t giáº£i phÃ¡p. NÃ³ kháº¯c phá»¥c nhÆ°á»£c Ä‘iá»ƒm cá»§a Zero-One báº±ng cÃ¡ch giáº£m Loss tá»« tá»« khi tiá»‡m cáº­n 0. NhÆ°ng nhÆ° báº¡n sáº½ tháº¥y, nÃ³ láº¡i gáº·p má»™t váº¥n Ä‘á» khÃ¡c...

**CÃ´ng thá»©c:**

$$L_{exp}(y, f(x)) = e^{-f(x) \cdot y}$$

Trong Ä‘Ã³:
- `y`: NhÃ£n thá»±c táº¿ (1 hoáº·c -1)
- `f(x)`: Äiá»ƒm sá»‘ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh
- `e`: Sá»‘ Euler (â‰ˆ 2.718)

**VÃ­ dá»¥ cá»¥ thá»ƒ:**

| Máº«u | NhÃ£n thá»±c táº¿ (y) | Äiá»ƒm sá»‘ dá»± Ä‘oÃ¡n f(x) | f(x) Â· y | Exponential Loss |
|-----|-----------------|---------------------|----------|------------------|
| 1   | 1               | 2.0                 | 2.0      | e^(-2.0) â‰ˆ 0.135 |
| 2   | 1               | 1.0                 | 1.0      | e^(-1.0) â‰ˆ 0.368 |
| 3   | 1               | 0.5                 | 0.5      | e^(-0.5) â‰ˆ 0.607 |
| 4   | 1               | -0.5                | -0.5     | e^(0.5) â‰ˆ 1.649  |
| 5   | 1               | -2.0                | -2.0     | e^(2.0) â‰ˆ 7.389  |

**Giáº£i thÃ­ch:** 
- Khi dá»± Ä‘oÃ¡n Ä‘Ãºng vÃ  tá»± tin (f(x) Â· y lá»›n): Loss giáº£m nhanh (máº«u 1: 0.135)
- Khi dá»± Ä‘oÃ¡n sai vÃ  tá»± tin (f(x) Â· y Ã¢m lá»›n): Loss tÄƒng ráº¥t nhanh (máº«u 5: 7.389)

**VÃ­ dá»¥ vá» Ä‘á»™ nháº¡y cáº£m vá»›i Outliers:**

**TrÆ°á»ng há»£p 1: KhÃ´ng cÃ³ outlier**
| Máº«u | y | f(x) | f(x)Â·y | Loss |
|-----|---|------|--------|------|
| 1   | 1 | 1.0  | 1.0    | 0.368|
| 2   | 1 | 0.8  | 0.8    | 0.449|
| 3   | 1 | 0.6  | 0.6    | 0.549|
| 4   | -1| -0.5 | 0.5    | 0.607|
| 5   | -1| -0.8 | 0.8    | 0.449|

Tá»•ng Loss â‰ˆ 2.422

**TrÆ°á»ng há»£p 2: CÃ³ 1 outlier**
| Máº«u | y | f(x) | f(x)Â·y | Loss |
|-----|---|------|--------|------|
| 1   | 1 | 1.0  | 1.0    | 0.368|
| 2   | 1 | 0.8  | 0.8    | 0.449|
| 3   | 1 | 0.6  | 0.6    | 0.549|
| 4   | -1| -0.5 | 0.5    | 0.607|
| 5   | 1 | -5.0 | -5.0   | 148.4| â† Outlier

Tá»•ng Loss â‰ˆ 150.4 (tÄƒng 62 láº§n!)

ÄÃ¢y chÃ­nh lÃ  váº¥n Ä‘á» mÃ  mÃ¬nh Ä‘Ã£ gáº·p pháº£i! Má»™t Ä‘iá»ƒm outlier cÃ³ thá»ƒ lÃ m tÄƒng tá»•ng Loss lÃªn ráº¥t nhiá»u (tÄƒng 62 láº§n!), khiáº¿n mÃ´ hÃ¬nh táº­p trung quÃ¡ má»©c vÃ o Ä‘iá»ƒm Ä‘Ã³ vÃ  dáº«n Ä‘áº¿n overfitting. MÃ¬nh Ä‘Ã£ tá»«ng dÃ¹ng Exponential Loss cho má»™t dataset cÃ³ nhiá»u noise vÃ  káº¿t quáº£ lÃ  mÃ´ hÃ¬nh cá»§a mÃ¬nh bá»‹ "Ã¡m áº£nh" bá»Ÿi nhá»¯ng Ä‘iá»ƒm khÃ³ phÃ¢n loáº¡i nháº¥t, bá» qua nhá»¯ng Ä‘iá»ƒm dá»… hÆ¡n.

**So sÃ¡nh vá»›i Zero-One Loss:**

| f(x)Â·y | Zero-One Loss | Exponential Loss |
|--------|---------------|------------------|
| 2.0    | 0             | 0.135            |
| 1.0    | 0             | 0.368            |
| 0.1    | 0             | 0.905            |
| -0.1   | 1             | 1.105            |
| -1.0   | 1             | 2.718            |
| -2.0   | 1             | 7.389            |

Exponential Loss giáº£m dáº§n khi tiáº¿n vá» 0 (khÃ´ng Ä‘á»™t ngá»™t nhÆ° Zero-One), nhÆ°ng tÄƒng ráº¥t nhanh khi sai lá»‡ch lá»›n.

**Báº£ng Æ¯u Ä‘iá»ƒm vÃ  NhÆ°á»£c Ä‘iá»ƒm:**

| Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|---------|------------|
| Kháº¯c phá»¥c háº¡n cháº¿ cá»§a Zero-One Loss | Ráº¥t nháº¡y cáº£m vá»›i outliers |
| Giáº£m Loss tá»« tá»« khi tiá»‡m cáº­n 0 | Dá»… dáº«n Ä‘áº¿n overfitting |
| CÃ³ gradient liÃªn tá»¥c, cÃ³ thá»ƒ tá»‘i Æ°u hÃ³a | TÄƒng theo hÃ m mÅ© khi sai lá»‡ch lá»›n |
| Pháº£n Ã¡nh Ä‘á»™ tá»± tin cá»§a dá»± Ä‘oÃ¡n | MÃ´ hÃ¬nh cÃ³ thá»ƒ táº­p trung quÃ¡ má»©c vÃ o Ä‘iá»ƒm khÃ³ |
| ÄÆ°á»£c sá»­ dá»¥ng trong AdaBoost | KhÃ´ng phÃ¹ há»£p khi cÃ³ nhiá»u noise trong dá»¯ liá»‡u |

--------------------------------------------------------------------------------

## II. NhÃ³m HÃ m Loss Dá»±a trÃªn Margin (Lá») - Tá»‘i Ä‘a hÃ³a khoáº£ng cÃ¡ch an toÃ n

Sau khi "vá»¡ má»™ng" vá»›i Exponential Loss, mÃ¬nh Ä‘Ã£ tÃ¬m hiá»ƒu vá» Hinge Loss - má»™t loss function Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong SVM. Äiá»u thÃº vá»‹ lÃ  nÃ³ giáº£i quyáº¿t váº¥n Ä‘á» cá»§a Exponential Loss báº±ng cÃ¡ch tÄƒng Loss theo hÆ°á»›ng tuyáº¿n tÃ­nh thay vÃ¬ hÃ m mÅ©.

### Hinge Loss - "NgÆ°á»i báº¡n" cá»§a SVM

**CÃ´ng thá»©c:**

$$L_{hinge}(y, f(x)) = \max(0, 1 - f(x) \cdot y)$$

Trong Ä‘Ã³:
- `y`: NhÃ£n thá»±c táº¿ (1 hoáº·c -1)
- `f(x)`: Äiá»ƒm sá»‘ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh
- Margin = 1 (cÃ³ thá»ƒ Ä‘iá»u chá»‰nh)

**VÃ­ dá»¥ cá»¥ thá»ƒ:**

| Máº«u | NhÃ£n thá»±c táº¿ (y) | Äiá»ƒm sá»‘ dá»± Ä‘oÃ¡n f(x) | f(x) Â· y | 1 - f(x)Â·y | Hinge Loss |
|-----|-----------------|---------------------|----------|------------|------------|
| 1   | 1               | 2.0                 | 2.0      | -1.0       | 0 (Ä‘Ãºng, margin > 1) |
| 2   | 1               | 1.0                 | 1.0      | 0.0        | 0 (Ä‘Ãºng, margin = 1) |
| 3   | 1               | 0.5                 | 0.5      | 0.5        | 0.5 (Ä‘Ãºng nhÆ°ng margin < 1) |
| 4   | 1               | -0.5                | -0.5     | 1.5        | 1.5 (sai) |
| 5   | -1              | 0.3                 | -0.3     | 1.3        | 1.3 (sai) |

**Giáº£i thÃ­ch:**
- Máº«u 1, 2: Dá»± Ä‘oÃ¡n Ä‘Ãºng vá»›i margin Ä‘á»§ lá»›n â†’ Loss = 0
- Máº«u 3: Dá»± Ä‘oÃ¡n Ä‘Ãºng nhÆ°ng margin nhá» â†’ Loss = 0.5 (váº«n bá»‹ pháº¡t nháº¹)
- Máº«u 4, 5: Dá»± Ä‘oÃ¡n sai â†’ Loss > 0

**VÃ­ dá»¥ vá» cháº¥p nháº­n sai sá»‘ trong vÃ¹ng margin:**

**TrÆ°á»ng há»£p 1: Dá»± Ä‘oÃ¡n Ä‘Ãºng vá»›i margin lá»›n**
- y = 1, f(x) = 3.0
- f(x)Â·y = 3.0
- Hinge Loss = max(0, 1 - 3.0) = **0**

**TrÆ°á»ng há»£p 2: Dá»± Ä‘oÃ¡n Ä‘Ãºng nhÆ°ng margin nhá»**
- y = 1, f(x) = 0.8
- f(x)Â·y = 0.8
- Hinge Loss = max(0, 1 - 0.8) = **0.2**

**TrÆ°á»ng há»£p 3: Dá»± Ä‘oÃ¡n sai**
- y = 1, f(x) = -0.5
- f(x)Â·y = -0.5
- Hinge Loss = max(0, 1 - (-0.5)) = **1.5**

Hinge Loss cháº¥p nháº­n má»™t pháº¡m vi sai sá»‘ trong vÃ¹ng margin (0 < margin < 1) mÃ  khÃ´ng pháº¡t náº·ng, táº­p trung vÃ o viá»‡c táº¡o khoáº£ng cÃ¡ch an toÃ n giá»¯a cÃ¡c lá»›p.

**So sÃ¡nh vá»›i Exponential Loss:**

| f(x)Â·y | Hinge Loss | Exponential Loss |
|--------|------------|------------------|
| 2.0    | 0          | 0.135            |
| 1.0    | 0          | 0.368            |
| 0.5    | 0.5        | 0.607            |
| 0.0    | 1.0        | 1.000            |
| -0.5   | 1.5        | 1.649            |
| -1.0   | 2.0        | 2.718            |
| -2.0   | 3.0        | 7.389            |

ÄÃ¢y lÃ  Ä‘iá»ƒm khÃ¡c biá»‡t quan trá»ng! Hinge Loss tÄƒng tuyáº¿n tÃ­nh (linear) khi sai lá»‡ch lá»›n, trong khi Exponential Loss tÄƒng theo hÃ m mÅ©. Äiá»u nÃ y lÃ m cho Hinge Loss Ã­t nháº¡y cáº£m vá»›i outliers hÆ¡n, nhÆ°ng nÃ³ cÅ©ng cÃ³ nhá»¯ng háº¡n cháº¿ riÃªng - Ä‘áº·c biá»‡t lÃ  nÃ³ khÃ´ng phÃ¹ há»£p vá»›i Deep Learning hiá»‡n Ä‘áº¡i.

**Báº£ng Æ¯u Ä‘iá»ƒm vÃ  NhÆ°á»£c Ä‘iá»ƒm:**

| Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|---------|------------|
| Ãt nháº¡y cáº£m vá»›i outliers hÆ¡n Exponential Loss | Cháº¥p nháº­n sai sá»‘ trong vÃ¹ng margin |
| TÄƒng tuyáº¿n tÃ­nh, khÃ´ng tÄƒng quÃ¡ nhanh | Táº­p trung vÃ o margin thay vÃ¬ xÃ¡c suáº¥t chÃ­nh xÃ¡c |
| PhÃ¹ há»£p vá»›i SVM | KhÃ´ng phÃ¹ há»£p vá»›i Deep Learning tiÃªu chuáº©n |
| GiÃºp tá»‘i Ä‘a hÃ³a margin giá»¯a cÃ¡c lá»›p | KhÃ´ng cung cáº¥p xÃ¡c suáº¥t Ä‘áº§u ra |
| Robust hÆ¡n Exponential Loss | Ãt Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i |

--------------------------------------------------------------------------------

## III. NhÃ³m HÃ m Loss Dá»±a trÃªn PhÃ¢n bá»‘ (Entropy & KL Divergence) - "TrÃ¡i tim" cá»§a Classification

ÄÃ¢y lÃ  nhÃ³m loss functions mÃ  mÃ¬nh nghÄ© lÃ  quan trá»ng nháº¥t cho classification. ChÃºng dá»±a trÃªn khÃ¡i niá»‡m entropy vÃ  khoáº£ng cÃ¡ch giá»¯a cÃ¡c phÃ¢n bá»‘ xÃ¡c suáº¥t. Khi mÃ¬nh hiá»ƒu Ä‘Æ°á»£c cÃ¡ch chÃºng hoáº¡t Ä‘á»™ng, má»i thá»© bá»—ng trá»Ÿ nÃªn rÃµ rÃ ng hÆ¡n ráº¥t nhiá»u!

### 1. KL Divergence (Kullback-Leibler Divergence)

**CÃ´ng thá»©c:**

$$D_{KL}(P \parallel Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)}$$

Trong Ä‘Ã³:
- `P`: PhÃ¢n bá»‘ thá»±c táº¿ (ground truth)
- `Q`: PhÃ¢n bá»‘ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh
- `P(i)`: XÃ¡c suáº¥t cá»§a sá»± kiá»‡n i trong phÃ¢n bá»‘ P

**VÃ­ dá»¥ cá»¥ thá»ƒ:**

So sÃ¡nh phÃ¢n bá»‘ doanh sá»‘ nÄƒm 2023 vÃ  2024:

| Sáº£n pháº©m | P (2023) | Q (2024) | P/Q | log(P/Q) | P Ã— log(P/Q) |
|----------|---------|---------|-----|----------|--------------|
| A        | 0.5     | 0.4     | 1.25| 0.223    | 0.112        |
| B        | 0.3     | 0.4     | 0.75| -0.288   | -0.086       |
| C        | 0.2     | 0.2     | 1.00| 0.000    | 0.000        |

KL Divergence = 0.112 + (-0.086) + 0.000 = **0.026**

**Giáº£i thÃ­ch:** KL Divergence = 0.026 cho tháº¥y hai phÃ¢n bá»‘ khÃ¡ giá»‘ng nhau.

**VÃ­ dá»¥ vá» tÃ­nh báº¥t Ä‘á»‘i xá»©ng:**

**TrÆ°á»ng há»£p 1: D_KL(P || Q)**
- P = [0.5, 0.3, 0.2]
- Q = [0.4, 0.4, 0.2]
- D_KL(P || Q) = 0.5Ã—log(0.5/0.4) + 0.3Ã—log(0.3/0.4) + 0.2Ã—log(0.2/0.2)
- D_KL(P || Q) â‰ˆ **0.026**

**TrÆ°á»ng há»£p 2: D_KL(Q || P)**
- Q = [0.4, 0.4, 0.2]
- P = [0.5, 0.3, 0.2]
- D_KL(Q || P) = 0.4Ã—log(0.4/0.5) + 0.4Ã—log(0.4/0.3) + 0.2Ã—log(0.2/0.2)
- D_KL(Q || P) â‰ˆ **0.031**

ÄÃ¢y lÃ  má»™t Ä‘iá»u thÃº vá»‹ mÃ  mÃ¬nh Ä‘Ã£ phÃ¡t hiá»‡n ra: D_KL(P || Q) â‰  D_KL(Q || P)! KL Divergence lÃ  báº¥t Ä‘á»‘i xá»©ng, nghÄ©a lÃ  khoáº£ng cÃ¡ch tá»« P Ä‘áº¿n Q khÃ¡c vá»›i khoáº£ng cÃ¡ch tá»« Q Ä‘áº¿n P. Äiá»u nÃ y cÃ³ thá»ƒ gÃ¢y nháº§m láº«n náº¿u báº¡n khÃ´ng biáº¿t, nhÆ°ng nÃ³ láº¡i cÃ³ Ã½ nghÄ©a thá»‘ng kÃª sÃ¢u sáº¯c.

**VÃ­ dá»¥ vá» sai lá»‡ch khi tÃ­nh trung bÃ¬nh:**

**Tá»· lá»‡ thay Ä‘á»•i P/Q:**
- Tá»· lá»‡ 1: P/Q = 1.0
- Tá»· lá»‡ 2: P/Q = 0.25
- Tá»· lá»‡ 3: P/Q = 4.0

**Trung bÃ¬nh cá»™ng:**
- Trung bÃ¬nh = (1.0 + 0.25 + 4.0) / 3 = **1.75**

**Trung bÃ¬nh log (sá»­ dá»¥ng log Ä‘á»ƒ kháº¯c phá»¥c):**
- log(1.0) = 0
- log(0.25) = -1.386
- log(4.0) = 1.386
- Trung bÃ¬nh log = (0 + (-1.386) + 1.386) / 3 = **0**

Trung bÃ¬nh cá»™ng bá»‹ kÃ©o lá»‡ch bá»Ÿi giÃ¡ trá»‹ 4.0, trong khi trung bÃ¬nh log pháº£n Ã¡nh chÃ­nh xÃ¡c hÆ¡n (tá»· lá»‡ trung bÃ¬nh thá»±c sá»± lÃ  1.0).

**Báº£ng Æ¯u Ä‘iá»ƒm vÃ  NhÆ°á»£c Ä‘iá»ƒm:**

| Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|---------|------------|
| Äo lÆ°á»ng khoáº£ng cÃ¡ch giá»¯a hai phÃ¢n bá»‘ | Báº¥t Ä‘á»‘i xá»©ng: D_KL(P\|\|Q) â‰  D_KL(Q\|\|P) |
| CÃ³ Ã½ nghÄ©a thá»‘ng kÃª rÃµ rÃ ng | KhÃ´ng pháº£i lÃ  metric (khÃ´ng thá»a báº¥t Ä‘áº³ng thá»©c tam giÃ¡c) |
| ÄÆ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong thá»‘ng kÃª | Cáº§n cáº£ hai phÃ¢n bá»‘ Ä‘á»u cÃ³ xÃ¡c suáº¥t > 0 |
| Pháº£n Ã¡nh sá»± khÃ¡c biá»‡t thÃ´ng tin | CÃ³ thá»ƒ bá»‹ sai lá»‡ch khi tÃ­nh trung bÃ¬nh Ä‘Æ¡n giáº£n |
| LiÃªn quan cháº·t cháº½ vá»›i Cross-Entropy | KhÃ³ diá»…n giáº£i trá»±c tiáº¿p |

---

### 2. Cross-Entropy Loss - "NgÃ´i sao" cá»§a Multi-class Classification

ÄÃ¢y lÃ  loss function mÃ  mÃ¬nh sá»­ dá»¥ng nhiá»u nháº¥t trong cÃ¡c dá»± Ã¡n classification cá»§a mÃ¬nh. Cross-Entropy Loss (hay cÃ²n gá»i lÃ  Log Loss) lÃ  "ngÃ´i sao" cá»§a multi-class classification. NhÆ°ng nhÆ° má»i "ngÃ´i sao", nÃ³ cÅ©ng cÃ³ nhá»¯ng háº¡n cháº¿ riÃªng...

**CÃ´ng thá»©c:**

$$L_{CE} = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)$$

Trong Ä‘Ã³:
- `C`: Sá»‘ lÆ°á»£ng lá»›p
- `y_i`: NhÃ£n thá»±c táº¿ (one-hot encoding)
- `Å·_i`: XÃ¡c suáº¥t dá»± Ä‘oÃ¡n cá»§a lá»›p i

**VÃ­ dá»¥ cá»¥ thá»ƒ - PhÃ¢n loáº¡i 3 lá»›p:**

**Máº«u 1: Dá»± Ä‘oÃ¡n Ä‘Ãºng vÃ  tá»± tin**
| Lá»›p | NhÃ£n thá»±c táº¿ (y) | XÃ¡c suáº¥t dá»± Ä‘oÃ¡n (Å·) | y Ã— log(Å·) |
|-----|-----------------|---------------------|------------|
| MÃ¨o | 1               | 0.9                 | 1 Ã— log(0.9) = -0.046 |
| ChÃ³ | 0               | 0.05                | 0 Ã— log(0.05) = 0 |
| Chim| 0               | 0.05                | 0 Ã— log(0.05) = 0 |

CE Loss = -(-0.046) = **0.046**

**Máº«u 2: Dá»± Ä‘oÃ¡n Ä‘Ãºng nhÆ°ng khÃ´ng tá»± tin**
| Lá»›p | NhÃ£n thá»±c táº¿ (y) | XÃ¡c suáº¥t dá»± Ä‘oÃ¡n (Å·) | y Ã— log(Å·) |
|-----|-----------------|---------------------|------------|
| MÃ¨o | 1               | 0.4                 | 1 Ã— log(0.4) = -0.916 |
| ChÃ³ | 0               | 0.3                 | 0 Ã— log(0.3) = 0 |
| Chim| 0               | 0.3                 | 0 Ã— log(0.3) = 0 |

CE Loss = -(-0.916) = **0.916**

**Máº«u 3: Dá»± Ä‘oÃ¡n sai**
| Lá»›p | NhÃ£n thá»±c táº¿ (y) | XÃ¡c suáº¥t dá»± Ä‘oÃ¡n (Å·) | y Ã— log(Å·) |
|-----|-----------------|---------------------|------------|
| MÃ¨o | 1               | 0.1                 | 1 Ã— log(0.1) = -2.303 |
| ChÃ³ | 0               | 0.7                 | 0 Ã— log(0.7) = 0 |
| Chim| 0               | 0.2                 | 0 Ã— log(0.2) = 0 |

CE Loss = -(-2.303) = **2.303**

**VÃ­ dá»¥ vá» khÃ´ng phÃ¹ há»£p vá»›i Multi-label:**

**BÃ i toÃ¡n Multi-label: GÃ¡n nhÃ£n cho phim**
- NhÃ£n cÃ³ thá»ƒ: "kinh dá»‹", "hÃ¬nh sá»±", "hÃ i"

**Vá»›i CE Loss + Softmax:**
| NhÃ£n | XÃ¡c suáº¥t (tá»•ng = 1) |
|------|---------------------|
| Kinh dá»‹ | 0.6 |
| HÃ¬nh sá»± | 0.3 |
| HÃ i    | 0.1 |

ÄÃ¢y chÃ­nh lÃ  váº¥n Ä‘á» mÃ  mÃ¬nh Ä‘Ã£ gáº·p pháº£i khi lÃ m bÃ i toÃ¡n multi-label! Náº¿u phim vá»«a lÃ  "kinh dá»‹" vá»«a lÃ  "hÃ¬nh sá»±", CE Loss khÃ´ng thá»ƒ xá»­ lÃ½ vÃ¬ Softmax buá»™c tá»•ng = 1, chá»‰ cho phÃ©p má»™t lá»›p cÃ³ xÃ¡c suáº¥t cao. MÃ¬nh Ä‘Ã£ tá»«ng "vá»¡ má»™ng" khi tháº¥y mÃ´ hÃ¬nh cá»§a mÃ¬nh khÃ´ng thá»ƒ há»c Ä‘Æ°á»£c ráº±ng má»™t bá»™ phim cÃ³ thá»ƒ cÃ³ nhiá»u thá»ƒ loáº¡i cÃ¹ng lÃºc. ÄÃ¢y lÃ  lÃºc mÃ¬nh nháº­n ra ráº±ng cáº§n pháº£i dÃ¹ng BCE Loss hoáº·c Pairwise Ranking Loss cho multi-label!

**Giáº£i phÃ¡p:** Sá»­ dá»¥ng BCE Loss hoáº·c Pairwise Ranking Loss cho Multi-label.

**Báº£ng Æ¯u Ä‘iá»ƒm vÃ  NhÆ°á»£c Ä‘iá»ƒm:**

| Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|---------|------------|
| PhÃ¹ há»£p vá»›i Multi-class Classification | KhÃ´ng phÃ¹ há»£p vá»›i Multi-label Classification |
| Phá»• biáº¿n nháº¥t trong phÃ¢n loáº¡i Ä‘a lá»›p | Cáº§n káº¿t há»£p vá»›i Softmax (tá»•ng xÃ¡c suáº¥t = 1) |
| Trá»«ng pháº¡t náº·ng cÃ¡c lá»—i tá»± tin | KhÃ´ng thá»ƒ xá»­ lÃ½ nhiá»u nhÃ£n cÃ¹ng lÃºc |
| CÃ³ gradient tá»‘t, dá»… tá»‘i Æ°u hÃ³a | CÃ³ thá»ƒ dáº«n Ä‘áº¿n overconfidence |
| LiÃªn quan cháº·t cháº½ vá»›i KL Divergence | KhÃ´ng tÃ­nh Ä‘áº¿n má»‘i quan há»‡ giá»¯a cÃ¡c lá»›p |

---

### 3. Binary Cross-Entropy Loss (BCE Loss)

**CÃ´ng thá»©c:**

$$L_{BCE} = -\frac{1}{N}\sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]$$

Trong Ä‘Ã³:
- `N`: Sá»‘ lÆ°á»£ng máº«u
- `y_i`: NhÃ£n thá»±c táº¿ (0 hoáº·c 1)
- `Å·_i`: XÃ¡c suáº¥t dá»± Ä‘oÃ¡n (0 Ä‘áº¿n 1)

**VÃ­ dá»¥ cá»¥ thá»ƒ - PhÃ¢n loáº¡i nhá»‹ phÃ¢n:**

| Máº«u | NhÃ£n thá»±c táº¿ (y) | XÃ¡c suáº¥t dá»± Ä‘oÃ¡n (Å·) | Loss |
|-----|-----------------|---------------------|------|
| 1   | 1               | 0.9                 | -log(0.9) = 0.105 |
| 2   | 1               | 0.7                 | -log(0.7) = 0.357 |
| 3   | 1               | 0.3                 | -log(0.3) = 1.204 |
| 4   | 0               | 0.2                 | -log(0.8) = 0.223 |
| 5   | 0               | 0.1                 | -log(0.9) = 0.105 |

Tá»•ng Loss = (0.105 + 0.357 + 1.204 + 0.223 + 0.105) / 5 = **0.399**

**VÃ­ dá»¥ vá» Multi-label Classification:**

**BÃ i toÃ¡n: GÃ¡n nhÃ£n cho áº£nh (cÃ³ thá»ƒ cÃ³ nhiá»u nhÃ£n)**
- NhÃ£n: "mÃ¨o", "chÃ³", "chim"

**Vá»›i BCE Loss (One vs. All):**
| áº¢nh | MÃ¨o (y) | MÃ¨o (Å·) | ChÃ³ (y) | ChÃ³ (Å·) | Chim (y) | Chim (Å·) |
|-----|---------|---------|---------|---------|----------|----------|
| 1   | 1       | 0.9     | 1       | 0.8     | 0        | 0.1      |
| 2   | 0       | 0.2     | 0       | 0.1     | 1        | 0.7      |

**TÃ­nh Loss cho tá»«ng nhÃ£n Ä‘á»™c láº­p:**
- Loss_mÃ¨o = -[1Ã—log(0.9) + 0Ã—log(0.2)] / 2 = 0.053
- Loss_chÃ³ = -[1Ã—log(0.8) + 0Ã—log(0.1)] / 2 = 0.112
- Loss_chim = -[0Ã—log(0.1) + 1Ã—log(0.7)] / 2 = 0.178

ÄÃ¢y lÃ  má»™t háº¡n cháº¿ lá»›n cá»§a BCE Loss mÃ  mÃ¬nh Ä‘Ã£ phÃ¡t hiá»‡n ra: BCE Loss khÃ´ng biáº¿t ráº±ng "mÃ¨o" vÃ  "chÃ³" thÆ°á»ng xuáº¥t hiá»‡n cÃ¹ng nhau trong áº£nh 1. NÃ³ xem má»—i nhÃ£n lÃ  Ä‘á»™c láº­p, nhÆ° thá»ƒ viá»‡c phÃ¢n loáº¡i "mÃ¨o" khÃ´ng liÃªn quan gÃ¬ Ä‘áº¿n viá»‡c phÃ¢n loáº¡i "chÃ³". Äiá»u nÃ y cÃ³ thá»ƒ khÃ´ng pháº£i lÃ  váº¥n Ä‘á» vá»›i cÃ¡c bÃ i toÃ¡n Ä‘Æ¡n giáº£n, nhÆ°ng vá»›i cÃ¡c bÃ i toÃ¡n phá»©c táº¡p hÆ¡n, báº¡n cÃ³ thá»ƒ cáº§n Ä‘áº¿n Pairwise Ranking Loss.

**So sÃ¡nh vá»›i CE Loss:**

| TÃ¬nh huá»‘ng | CE Loss | BCE Loss |
|------------|---------|----------|
| Binary Classification | CÃ³ thá»ƒ dÃ¹ng | PhÃ¹ há»£p nháº¥t |
| Multi-class (1 nhÃ£n) | PhÃ¹ há»£p nháº¥t | KhÃ´ng phÃ¹ há»£p |
| Multi-label (nhiá»u nhÃ£n) | KhÃ´ng phÃ¹ há»£p | CÃ³ thá»ƒ dÃ¹ng (One vs. All) |

**Báº£ng Æ¯u Ä‘iá»ƒm vÃ  NhÆ°á»£c Ä‘iá»ƒm:**

| Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|---------|------------|
| PhÃ¹ há»£p vá»›i Binary Classification | Bá» qua má»‘i quan há»‡ giá»¯a cÃ¡c nhÃ£n trong Multi-label |
| CÃ³ thá»ƒ Ã¡p dá»¥ng cho Multi-label (One vs. All) | Xem má»—i nhÃ£n lÃ  Ä‘á»™c láº­p |
| Trá»«ng pháº¡t náº·ng cÃ¡c lá»—i tá»± tin | KhÃ´ng tá»‘i Æ°u hÃ³a má»‘i quan há»‡ giá»¯a nhÃ£n |
| CÃ³ gradient tá»‘t | CÃ³ thá»ƒ khÃ´ng hiá»‡u quáº£ vá»›i Multi-label phá»©c táº¡p |
| ÄÆ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i | Cáº§n nhiá»u tham sá»‘ hÆ¡n CE Loss |

--------------------------------------------------------------------------------

## IV. CÃ¡c Ká»¹ thuáº­t Cáº£i tiáº¿n KhÃ¡c - Nhá»¯ng "bÃ­ kÃ­p" tá»« thá»±c táº¿

Sau khi lÃ m viá»‡c vá»›i cÃ¡c loss functions cÆ¡ báº£n, mÃ¬nh Ä‘Ã£ tÃ¬m hiá»ƒu vá» cÃ¡c ká»¹ thuáº­t cáº£i tiáº¿n Ä‘Æ°á»£c sá»­ dá»¥ng trong thá»±c táº¿. ÄÃ¢y lÃ  nhá»¯ng "bÃ­ kÃ­p" mÃ  mÃ¬nh Ä‘Ã£ há»c Ä‘Æ°á»£c tá»« cÃ¡c dá»± Ã¡n thá»±c táº¿ vÃ  cÃ¡c cuá»™c thi Machine Learning.

### 1. Sparse Categorical Cross-Entropy (SCCE)

**CÃ´ng thá»©c:**

SCCE cÃ³ cÃ¹ng cÃ´ng thá»©c vá»›i CE Loss, nhÆ°ng sá»­ dá»¥ng nhÃ£n sá»‘ nguyÃªn thay vÃ¬ one-hot encoding.

$$L_{SCCE} = -\log(\hat{y}_{y_{true}})$$

Trong Ä‘Ã³:
- `y_true`: NhÃ£n sá»‘ nguyÃªn (vÃ­ dá»¥: 0, 1, 2)
- `Å·_{y_true}`: XÃ¡c suáº¥t dá»± Ä‘oÃ¡n cá»§a lá»›p y_true

**VÃ­ dá»¥ cá»¥ thá»ƒ:**

**Vá»›i CE Loss (One-hot encoding):**
- NhÃ£n: [0, 1, 0] (lá»›p 1)
- Vector one-hot: 3 chiá»u

**Vá»›i SCCE (Integer label):**
- NhÃ£n: 1 (lá»›p 1)
- Chá»‰ cáº§n 1 sá»‘ nguyÃªn

**So sÃ¡nh bá»™ nhá»›:**

**BÃ i toÃ¡n 1000 lá»›p:**
- CE Loss: Má»—i máº«u cáº§n vector 1000 chiá»u â†’ 1000 sá»‘ float
- SCCE: Má»—i máº«u cáº§n 1 sá»‘ nguyÃªn â†’ 1 sá»‘ int

**Tiáº¿t kiá»‡m:** 
- Bá»™ nhá»›: Giáº£m ~1000 láº§n
- TÃ­nh toÃ¡n: Nhanh hÆ¡n Ä‘Ã¡ng ká»ƒ

**VÃ­ dá»¥ tÃ­nh toÃ¡n:**

| Máº«u | NhÃ£n thá»±c táº¿ (y) | XÃ¡c suáº¥t dá»± Ä‘oÃ¡n [Lá»›p 0, Lá»›p 1, Lá»›p 2] | SCCE Loss |
|-----|-----------------|----------------------------------------|-----------|
| 1   | 1               | [0.1, 0.8, 0.1]                        | -log(0.8) = 0.223 |
| 2   | 0               | [0.9, 0.05, 0.05]                      | -log(0.9) = 0.105 |
| 3   | 2               | [0.2, 0.1, 0.7]                        | -log(0.7) = 0.357 |

**Báº£ng Æ¯u Ä‘iá»ƒm vÃ  NhÆ°á»£c Ä‘iá»ƒm:**

| Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|---------|------------|
| Tiáº¿t kiá»‡m bá»™ nhá»› Ä‘Ã¡ng ká»ƒ | Chá»‰ khÃ¡c CE Loss vá» cÃ¡ch mÃ£ hÃ³a nhÃ£n |
| Nhanh hÆ¡n vá»›i sá»‘ lÆ°á»£ng lá»›p lá»›n | KhÃ´ng cÃ³ háº¡n cháº¿ cá»¥ thá»ƒ |
| PhÃ¹ há»£p vá»›i bÃ i toÃ¡n nhiá»u lá»›p (1000+) | Ãt Ä‘Æ°á»£c biáº¿t Ä‘áº¿n hÆ¡n CE Loss |
| Giáº£m Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n | Cáº§n Ä‘áº£m báº£o nhÃ£n lÃ  sá»‘ nguyÃªn |
| TÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i CE Loss vá» máº·t toÃ¡n há»c | |

---

### 2. Label Smoothing (LS)

**CÃ´ng thá»©c:**

Thay vÃ¬ nhÃ£n cá»©ng (hard label) [1, 0, 0], sá»­ dá»¥ng nhÃ£n má»m (soft label):

$$y_{smooth} = (1 - \alpha) \times y_{hard} + \alpha \times \frac{1}{C}$$

Trong Ä‘Ã³:
- `Î±`: Há»‡ sá»‘ smoothing (thÆ°á»ng 0.1)
- `C`: Sá»‘ lÆ°á»£ng lá»›p
- `y_hard`: NhÃ£n cá»©ng (one-hot)

**VÃ­ dá»¥ cá»¥ thá»ƒ:**

**BÃ i toÃ¡n 3 lá»›p: MÃ¨o, ChÃ³, Chim**

**KhÃ´ng cÃ³ Label Smoothing:**
| Lá»›p | NhÃ£n cá»©ng (y) |
|-----|--------------|
| MÃ¨o | 1.0          |
| ChÃ³ | 0.0          |
| Chim| 0.0          |

**Vá»›i Label Smoothing (Î± = 0.1):**
| Lá»›p | NhÃ£n má»m (y_smooth) |
|-----|-------------------|
| MÃ¨o | 0.9 + 0.1/3 = 0.933 |
| ChÃ³ | 0.0 + 0.1/3 = 0.033 |
| Chim| 0.0 + 0.1/3 = 0.033 |

**VÃ­ dá»¥ vá» giáº£m overconfidence:**

**MÃ´ hÃ¬nh khÃ´ng cÃ³ Label Smoothing:**
- Dá»± Ä‘oÃ¡n: [0.99, 0.005, 0.005] (quÃ¡ tá»± tin)
- MÃ´ hÃ¬nh nghÄ© cháº¯c cháº¯n lÃ  "MÃ¨o"

**MÃ´ hÃ¬nh cÃ³ Label Smoothing:**
- NhÃ£n má»m: [0.933, 0.033, 0.033]
- MÃ´ hÃ¬nh há»c Ä‘Æ°á»£c ráº±ng cÃ³ thá»ƒ cÃ³ má»™t chÃºt khÃ´ng cháº¯c cháº¯n
- Dá»± Ä‘oÃ¡n: [0.95, 0.03, 0.02] (tá»± tin nhÆ°ng khÃ´ng quÃ¡ má»©c)

**VÃ­ dá»¥ vá» máº¥t cÃ¢n báº±ng dá»¯ liá»‡u:**

**TrÆ°á»ng há»£p:** MÃ´ hÃ¬nh tháº¥y quÃ¡ nhiá»u "chuá»‘i vÃ ng" trong táº­p train
- KhÃ´ng cÃ³ LS: MÃ´ hÃ¬nh nghÄ© táº¥t cáº£ chuá»‘i Ä‘á»u vÃ ng â†’ Overfitting
- CÃ³ LS: MÃ´ hÃ¬nh há»c Ä‘Æ°á»£c ráº±ng cÃ³ thá»ƒ cÃ³ chuá»‘i xanh â†’ Tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n

**Báº£ng Æ¯u Ä‘iá»ƒm vÃ  NhÆ°á»£c Ä‘iá»ƒm:**

| Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|---------|------------|
| Giáº£m overfitting | Cáº§n Ä‘iá»u chá»‰nh há»‡ sá»‘ Î± |
| GiÃºp mÃ´ hÃ¬nh tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n | CÃ³ thá»ƒ lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c má»™t chÃºt |
| Giáº£m overconfidence | KhÃ´ng phÃ¹ há»£p vá»›i táº¥t cáº£ bÃ i toÃ¡n |
| PhÃ¹ há»£p vá»›i dá»¯ liá»‡u máº¥t cÃ¢n báº±ng | Cáº§n thá»­ nghiá»‡m Ä‘á»ƒ tÃ¬m Î± tá»‘i Æ°u |
| ÄÆ°á»£c sá»­ dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh hiá»‡n Ä‘áº¡i | CÃ³ thá»ƒ lÃ m cháº­m quÃ¡ trÃ¬nh há»™i tá»¥ |

---

### 3. Pairwise Ranking Loss

**CÃ´ng thá»©c:**

$$L_{ranking} = \sum_{i \in P} \sum_{j \in N} \max(0, \gamma - (s_i - s_j))$$

Trong Ä‘Ã³:
- `P`: Táº­p cÃ¡c nhÃ£n tÃ­ch cá»±c (positive labels)
- `N`: Táº­p cÃ¡c nhÃ£n tiÃªu cá»±c (negative labels)
- `s_i`: Äiá»ƒm sá»‘ cá»§a nhÃ£n tÃ­ch cá»±c i
- `s_j`: Äiá»ƒm sá»‘ cá»§a nhÃ£n tiÃªu cá»±c j
- `Î³`: Margin (thÆ°á»ng = 1)

**VÃ­ dá»¥ cá»¥ thá»ƒ - Multi-label Classification:**

**BÃ i toÃ¡n:** GÃ¡n nhÃ£n cho áº£nh
- NhÃ£n cÃ³ thá»ƒ: "mÃ¨o", "chÃ³", "chim", "cÃ¡"

**áº¢nh 1:** CÃ³ "mÃ¨o" vÃ  "chÃ³"
- NhÃ£n tÃ­ch cá»±c (P): ["mÃ¨o", "chÃ³"]
- NhÃ£n tiÃªu cá»±c (N): ["chim", "cÃ¡"]

**Äiá»ƒm sá»‘ dá»± Ä‘oÃ¡n:**
| NhÃ£n | Äiá»ƒm sá»‘ (s) |
|------|------------|
| MÃ¨o  | 0.9        |
| ChÃ³  | 0.8        |
| Chim | 0.3        |
| CÃ¡   | 0.2        |

**TÃ­nh Loss:**
- Cáº·p (MÃ¨o, Chim): max(0, 1 - (0.9 - 0.3)) = max(0, 0.4) = 0.4
- Cáº·p (MÃ¨o, CÃ¡): max(0, 1 - (0.9 - 0.2)) = max(0, 0.3) = 0.3
- Cáº·p (ChÃ³, Chim): max(0, 1 - (0.8 - 0.3)) = max(0, 0.5) = 0.5
- Cáº·p (ChÃ³, CÃ¡): max(0, 1 - (0.8 - 0.2)) = max(0, 0.4) = 0.4

Tá»•ng Loss = 0.4 + 0.3 + 0.5 + 0.4 = **1.6**

**VÃ­ dá»¥ vá» há»c má»‘i quan há»‡ giá»¯a nhÃ£n:**

**Vá»›i BCE Loss:**
- "MÃ¨o" vÃ  "ChÃ³" Ä‘Æ°á»£c xá»­ lÃ½ Ä‘á»™c láº­p
- KhÃ´ng biáº¿t ráº±ng chÃºng thÆ°á»ng xuáº¥t hiá»‡n cÃ¹ng nhau

**Vá»›i Pairwise Ranking Loss:**
- So sÃ¡nh Ä‘iá»ƒm sá»‘ giá»¯a nhÃ£n tÃ­ch cá»±c vÃ  tiÃªu cá»±c
- Há»c Ä‘Æ°á»£c thá»© tá»± Æ°u tiÃªn: "MÃ¨o" > "Chim", "ChÃ³" > "Chim"
- CÃ³ thá»ƒ há»c Ä‘Æ°á»£c má»‘i quan há»‡: náº¿u cÃ³ "MÃ¨o" thÃ¬ thÆ°á»ng cÃ³ "ChÃ³"

**So sÃ¡nh vá»›i BCE Loss:**

| Äáº·c Ä‘iá»ƒm | BCE Loss | Pairwise Ranking Loss |
|---------|----------|----------------------|
| Xá»­ lÃ½ Multi-label | CÃ³ (One vs. All) | CÃ³ (tá»‘i Æ°u hÃ³a margin) |
| Há»c má»‘i quan há»‡ nhÃ£n | KhÃ´ng | CÃ³ |
| Thá»© tá»± Æ°u tiÃªn | KhÃ´ng | CÃ³ |
| TÃ­nh toÃ¡n | ÄÆ¡n giáº£n | Phá»©c táº¡p hÆ¡n (O(PÃ—N)) |
| PhÃ¹ há»£p | Binary, Multi-label Ä‘Æ¡n giáº£n | Multi-label phá»©c táº¡p |

**Báº£ng Æ¯u Ä‘iá»ƒm vÃ  NhÆ°á»£c Ä‘iá»ƒm:**

| Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|---------|------------|
| Tá»‘i Æ°u hÃ³a dá»±a trÃªn margin | TÃ­nh toÃ¡n phá»©c táº¡p hÆ¡n BCE Loss |
| Há»c Ä‘Æ°á»£c thá»© tá»± Æ°u tiÃªn giá»¯a nhÃ£n | Cáº§n nhiá»u cáº·p (PÃ—N) Ä‘á»ƒ tÃ­nh toÃ¡n |
| CÃ³ thá»ƒ há»c má»‘i quan há»‡ giá»¯a nhÃ£n | KhÃ´ng phá»• biáº¿n báº±ng BCE Loss |
| PhÃ¹ há»£p vá»›i Multi-label phá»©c táº¡p | Cáº§n Ä‘iá»u chá»‰nh margin Î³ |
| Giáº£i quyáº¿t háº¡n cháº¿ cá»§a BCE Loss | CÃ³ thá»ƒ cháº­m vá»›i sá»‘ lÆ°á»£ng nhÃ£n lá»›n |

--------------------------------------------------------------------------------

## Káº¿t luáº­n: Nhá»¯ng bÃ i há»c Ä‘Ã£ há»c Ä‘Æ°á»£c

Sau hÃ nh trÃ¬nh tÃ¬m hiá»ƒu vá» cÃ¡c loss functions cho classification, mÃ¬nh Ä‘Ã£ rÃºt ra Ä‘Æ°á»£c nhiá»u bÃ i há»c quÃ½ giÃ¡. Viá»‡c lá»±a chá»n hÃ m Loss phÃ¹ há»£p khÃ´ng chá»‰ lÃ  má»™t quyáº¿t Ä‘á»‹nh ká»¹ thuáº­t, mÃ  cÃ²n áº£nh hÆ°á»Ÿng Ä‘áº¿n cÃ¡ch mÃ´ hÃ¬nh cá»§a báº¡n há»c vÃ  hoáº¡t Ä‘á»™ng.

### Báº£ng HÆ°á»›ng dáº«n Lá»±a chá»n

| TÃ¬nh huá»‘ng | Loss Function Ä‘Æ°á»£c khuyáº¿n nghá»‹ | LÃ½ do |
|------------|-------------------------------|-------|
| Binary Classification | BCE Loss | PhÃ¹ há»£p nháº¥t, trá»«ng pháº¡t náº·ng lá»—i tá»± tin |
| Multi-class (1 nhÃ£n) | Cross-Entropy Loss | Phá»• biáº¿n nháº¥t, káº¿t há»£p vá»›i Softmax |
| Multi-class (nhiá»u lá»›p, 1000+) | Sparse Categorical Cross-Entropy | Tiáº¿t kiá»‡m bá»™ nhá»› vÃ  tÃ­nh toÃ¡n |
| Multi-label (nhiá»u nhÃ£n) | BCE Loss hoáº·c Pairwise Ranking Loss | BCE cho Ä‘Æ¡n giáº£n, Pairwise cho phá»©c táº¡p |
| SVM, tá»‘i Ä‘a hÃ³a margin | Hinge Loss | PhÃ¹ há»£p vá»›i mÃ´ hÃ¬nh truyá»n thá»‘ng |
| Giáº£m overconfidence | Cross-Entropy + Label Smoothing | GiÃºp mÃ´ hÃ¬nh tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n |
| AdaBoost | Exponential Loss | ÄÆ°á»£c sá»­ dá»¥ng trong boosting algorithms |
| ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng | Zero-One Loss | ÄÆ¡n giáº£n, trá»±c tiáº¿p pháº£n Ã¡nh Ä‘á»™ chÃ­nh xÃ¡c |

### NguyÃªn táº¯c vÃ ng

1. **Hiá»ƒu rÃµ bÃ i toÃ¡n:** Binary, Multi-class, hay Multi-label sáº½ quyáº¿t Ä‘á»‹nh hÃ m Loss phÃ¹ há»£p.

2. **Xem xÃ©t Ä‘áº·c Ä‘iá»ƒm dá»¯ liá»‡u:** Outliers, máº¥t cÃ¢n báº±ng, noise sáº½ áº£nh hÆ°á»Ÿng Ä‘áº¿n lá»±a chá»n.

3. **CÃ¢n nháº¯c má»‘i quan há»‡ nhÃ£n:** Náº¿u cÃ¡c nhÃ£n cÃ³ má»‘i quan há»‡, cáº§n hÃ m Loss phÃ¹ há»£p.

4. **Tá»‘i Æ°u hÃ³a tÃ i nguyÃªn:** Vá»›i sá»‘ lÆ°á»£ng lá»›p lá»›n, Æ°u tiÃªn SCCE thay vÃ¬ CE Loss.

5. **Káº¿t há»£p ká»¹ thuáº­t:** Label Smoothing cÃ³ thá»ƒ káº¿t há»£p vá»›i CE Loss Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t.

Cuá»‘i cÃ¹ng, mÃ¬nh muá»‘n nháº¥n máº¡nh ráº±ng: viá»‡c lá»±a chá»n Ä‘Ãºng hÃ m Loss, giá»‘ng nhÆ° viá»‡c chá»n Ä‘Ãºng la bÃ n, sáº½ giÃºp mÃ´ hÃ¬nh cá»§a báº¡n Ä‘i Ä‘Ãºng hÆ°á»›ng vÃ  Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu mong muá»‘n. Má»—i hÃ m Loss lÃ  má»™t cÃ´ng cá»¥ khÃ¡c nhau, vÃ  viá»‡c hiá»ƒu rÃµ chÃºng sáº½ giÃºp báº¡n xÃ¢y dá»±ng mÃ´ hÃ¬nh hiá»‡u quáº£ hÆ¡n.

MÃ¬nh hy vá»ng bÃ i viáº¿t nÃ y sáº½ giÃºp cÃ¡c báº¡n trÃ¡nh Ä‘Æ°á»£c nhá»¯ng "cáº¡m báº«y" mÃ  mÃ¬nh Ä‘Ã£ tá»«ng váº¥p pháº£i. Náº¿u cÃ¡c báº¡n cÃ³ cÃ¢u há»i hoáº·c muá»‘n chia sáº» kinh nghiá»‡m cá»§a mÃ¬nh, Ä‘á»«ng ngáº¡i comment bÃªn dÆ°á»›i nhÃ©! ChÃºng ta cÃ¹ng há»c há»i láº«n nhau! ğŸš€

