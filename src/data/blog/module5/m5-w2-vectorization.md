---
title: "Vectorization trong Linear Regression"
description: "Há»c cÃ¡ch vectorize Linear Regression Ä‘á»ƒ tÃ­nh toÃ¡n hiá»‡u quáº£ vá»›i ma tráº­n vÃ  vector"
pubDatetime: 2025-01-27T20:00:00Z
heroImage: "/assets/images/vectorization-hero.jpg"
tags: ["linear-regression", "vectorization", "matrix", "optimization"]
---

# Vectorization trong Linear Regression

## **Tá»•ng quan**

Vectorization lÃ  ká»¹ thuáº­t chuyá»ƒn Ä‘á»•i cÃ¡c phÃ©p toÃ¡n tá»« vÃ²ng láº·p sang phÃ©p toÃ¡n ma tráº­n/vector, giÃºp:
- **TÄƒng tá»‘c tÃ­nh toÃ¡n** (10-100x nhanh hÆ¡n)
- **Táº­n dá»¥ng tá»‘i Ä‘a CPU/GPU**
- **Code ngáº¯n gá»n vÃ  dá»… Ä‘á»c**

## **Ná»™i dung chÃ­nh**

**Vá»›i theta Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trÆ°á»›c**

## XÃ©t tá»«ng sample tá»«ng 1 feature 

### 1 sample 1 feature 

ta cÃ³ 

$$
\theta = \begin{bmatrix}
b \\
w
\end{bmatrix}
$$

**vÃ  x lÃ  feature lÃ  má»™t cá»™t**

$$
x = \begin{bmatrix}
1 \\
x_1
\end{bmatrix}
$$
 
vá»›i cÃ¡i hÃ nh Ä‘á»™ng tá»± nhiÃªn lÃ  dotproduct(tÃ­ch vÃ´ hÆ°á»›ng)

$$
\theta^T \vec{x} = \begin{bmatrix}
b & w
\end{bmatrix} \begin{bmatrix}
1 \\
x_1
\end{bmatrix} = b \cdot 1 + w \cdot x_1 = \hat{y}
$$

vÃ  ta nháº­n xÃ©t nÃ³ ráº¥t giá»‘ng vá»›i phÆ°Æ¡ng trÃ¬nh predict sample cá»§a linear regression 

$$
\mathcal{L}(\theta) = (\hat{y} - y)^2 \quad \text{vá»›i } \hat{y} \text{ lÃ  má»™t scalar vÃ  } y \text{ cÅ©ng lÃ  má»™t scalar}
$$

nÃªn á»Ÿ bÆ°á»›c nÃ y ta khÃ´ng cáº§n vectorization gÃ¬ cáº£ 

compute gradient

$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial b} &= 2(\hat{y} - y) \cdot 1 \\
\frac{\partial \mathcal{L}}{\partial w} &= 2(\hat{y} - y) \cdot x_1
\end{aligned}
$$

vá»›i 

$$
\vec{x} \in \mathbb{R}^n \quad \text{(feature vector)} \\
\hat{y}, y \in \mathbb{R} \quad \Rightarrow \quad (\hat{y} - y) \in \mathbb{R}
$$



$$
\nabla_\theta \mathcal{L} = \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial b} \\
\frac{\partial \mathcal{L}}{\partial w}
\end{bmatrix} = 2(\hat{y} - y) \begin{bmatrix}
1 \\
x_1
\end{bmatrix}
$$

Update thÃ¬ chÃºng ta cáº§n $\text{lr},\ \mathbf{w},\ \mathcal{L}(\theta)$ nÃªn chÃºng ta cáº§n ...

$$
\theta = \theta - \eta \cdot \nabla_\theta \mathcal{L}
$$

```python
# ==============================
# Linear Regression:
# ==============================

import numpy as np

# --- Code ---
def predict(theta, x):
    # theta.T: shape (2,)
    # x: shape (N,)
    # X = stacked bias + feature
    X = np.vstack([np.ones_like(x), x])   # shape (2, N)
    return theta.T.dot(X)

def compute_loss(y_hat, y):
    # Loss = (y_hat - y)^2
    return (y_hat - y)**2

def compute_gradient(x, y_hat, y):
    # Gradient theo theta
    return 2*(y_hat - y) * np.vstack([1, x])

def update(theta, lr, grad):
    # Cáº­p nháº­t theta
    return theta - lr * grad

# --- Giáº£i thÃ­ch ---
# theta: shape (2,) -> [bias, weight]
# x: shape (N,) -> 1 feature duy nháº¥t
# predict(theta, x) = theta.T.dot(X) -> shape (N,)
# X Ä‘Æ°á»£c táº¡o báº±ng np.vstack([np.ones_like(x), x]) -> shape (2, N)
# gradient = 2*(y_hat - y) * [1, x]^T
```


## **Way1**
| Feature | Label|
|---------|------|
| 6.7     | 9.1  |
| 4.6     | 5.9  |
| 3.5     | 4.6  |
| 5.5     | 6.7  |

ta chá»n minibatch = 2, nhÆ°ng sample láº¥y theo row 
váº­y thÃ¬ X bÃ¢y giá» lÃ  má»™t ma tráº­n Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  X in hoa 
$$
X=
\begin{bmatrix}
6.7 & 4.6\\
1 & 1
\end{bmatrix}
\qquad
y=
\begin{bmatrix}
9.1\\
5.9
\end{bmatrix}
$$
------
$$
\text{Vá»›i } \theta =
\begin{bmatrix}
w\\
b
\end{bmatrix}
=
\begin{bmatrix}
-0.34 \\
0.049
\end{bmatrix}
$$
lÃ  random

-------
$$
\text{Vá»›i 2 sample: } 
y = 
\begin{bmatrix}
y_1 \\
y_2
\end{bmatrix},
\quad
\hat{y} = 
\begin{bmatrix}
\hat{y}_1 \\
\hat{y}_2
\end{bmatrix}
$$
-------
thÃ¬ ta cÃ³ 





$$
\hat{y} =
\begin{bmatrix}
\hat{y}^{(1)}\\
\hat{y}^{(2)}
\end{bmatrix}
=
\theta^\top \cdot X
\\[6pt]
=
\begin{bmatrix}
-0.34 & 0.049
\end{bmatrix}
\cdot
\begin{bmatrix}
6.7 & 4.6\\
1 & 1
\end{bmatrix}
\\[6pt]
=
\begin{bmatrix}
-0.34 \cdot 6.7 + 0.049 \cdot 1 & ,
-0.34 \cdot 4.6 + 0.049 \cdot 1
\end{bmatrix}
\\[6pt]
=
\begin{bmatrix}
-2.278 + 0.049 &,
-1.564 + 0.049
\end{bmatrix}
\\[6pt]
=
\begin{bmatrix}
-2.229 &
-1.515
\end{bmatrix}
$$

### **ğŸ”¢ TÃ­nh Loss (MSE)**

Vá»›i giÃ¡ trá»‹ vá»«a tÃ­nh Ä‘Æ°á»£c:

$$
\hat{y} = 
\begin{bmatrix}
-2.229 & -1.515
\end{bmatrix}
\quad \text{(dÃ²ng)}
$$

$$
y = 
\begin{bmatrix}
9.1 \\
5.9
\end{bmatrix}
\quad \text{(cá»™t)}
$$

**BÆ°á»›c 1: TÃ­nh sai sá»‘**

$$
\hat{y} - y^T = 
\begin{bmatrix}
-2.229 & -1.515
\end{bmatrix}
-
\begin{bmatrix}
9.1 & 5.9
\end{bmatrix}
=
\begin{bmatrix}
-11.329 & -7.415
\end{bmatrix}
$$

**BÆ°á»›c 2: TÃ­nh Loss (MSE)**

$$
\begin{aligned}
L &= \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 \\
&= \frac{1}{2} \left[ (-11.329)^2 + (-7.415)^2 \right] \\
&= \frac{1}{2} \left[ 128.35 + 54.98 \right] \\
&= \frac{1}{2} \times 183.33 \\
&= 91.665
\end{aligned}
$$

**Hoáº·c dÃ¹ng phÃ©p toÃ¡n vector:**

$$
L = \frac{1}{m} (\hat{y} - y^T) \cdot (\hat{y} - y^T)^T
$$

$$
= \frac{1}{2}
\begin{bmatrix}
-11.329 & -7.415
\end{bmatrix}
\begin{bmatrix}
-11.329 \\
-7.415
\end{bmatrix}
= \frac{1}{2} \times 183.33 = 91.665
$$

**ğŸ¯ Loss = 91.665** (khÃ¡ lá»›n vÃ¬ theta chÆ°a Ä‘Æ°á»£c train!)

-----------------

- $y$ lÃ  vector cá»™t: 
$$
y = 
\begin{bmatrix}
y^{(1)} \\
y^{(2)}
\end{bmatrix}
$$

- $\hat{y}$ hiá»‡n lÃ  vector dÃ²ng:
$$
\hat{y} = 
\begin{bmatrix}
\hat{y}^{(1)} & \hat{y}^{(2)}
\end{bmatrix}
$$

Do Ä‘Ã³, cáº§n transpose $\hat{y}$ thÃ nh vector cá»™t, hoáº·c transpose $y$ thÃ nh vector dÃ²ng.

$$
\text{lien tÆ°á»Ÿng trong} \quad x^2 =x \cdot x \quad 
\text{nhÆ°ng trong Ä‘áº¡i sá»‘} \quad \vec{x}= \vec{x}^{T} \cdot \vec{x}
$$

VÃ­ dá»¥, transpose $\hat{y}$:
$$

\hat{y}^T = 
\begin{bmatrix}
\hat{y}^{(1)} \\
\hat{y}^{(2)}
\end{bmatrix}
$$

Khi Ä‘Ã³, cÃ´ng thá»©c tÃ­nh loss (vÃ­ dá»¥ Mean Squared Error) sáº½ lÃ :
$$
L = \frac{1}{2} \sum_{i=1}^2 \left( \hat{y}^{(i)} - y^{(i)} \right)^2 = \frac{1}{2} \left\| \hat{y} - y^T\right\|^2
$$

Hoáº·c viáº¿t dÆ°á»›i dáº¡ng tá»•ng quÃ¡t vá»›i vectors:
$$
L = \frac{1}{2} (\hat{y}- y)^T \cdot (\hat{y}-y)
$$

vÃ­ dá»¥ 2 sample
$$
\frac{1}{2}
\begin{bmatrix}
\left( \hat{y}^{(0)} - y^{(0)} \right)^2 + \left( \hat{y}^{(1)} - y^{(1)} \right)^2
\end{bmatrix}
=

\\[6pt]
=\frac{1}{2}
\begin{bmatrix}
\left( \hat{y}^{(0)} - y^{(0)} \right)^2 + \left( \hat{y}^{(1)} - y^{(1)} \right)^2 = 
\begin{bmatrix}
\hat{y}^{(0)} - y^{(0)} , 
\hat{y}^{(1)} - y^{(1)}
\end{bmatrix}
\begin{bmatrix}
\hat{y}^{(0)} - y^{(0)} \\ 
\hat{y}^{(1)} - y^{(1)}
\end{bmatrix}
\end{bmatrix}
\\[6pt]
=\frac{1}{m}
(\hat{y}-y^{T})\cdot(\hat{y}-y^{T}) \quad 
\text{vÃ  nÃ³ ra con sá»‘ }
$$

TÃ³m láº¡i, *khi thao tÃ¡c vector hÃ³a, cáº§n chÃº Ã½ shape* Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘Ãºng!



**KhÃ³ khÄƒn khi vector hÃ³a vá»›i 2 chiá»u (features):**

- Äáº§u tiÃªn, khi má»›i tiáº¿p cáº­n vectorization, dá»… bá»‹ "láº«n lá»™n" chiá»u vectors/matrices. Äáº·c biá»‡t, báº¡n cáº§n nháº¥t quÃ¡n vá» cÃ¡ch tá»• chá»©c **input X** (vÃ­ dá»¥: feature lÃ  cá»™t, samples lÃ  hÃ ng) vÃ  vector **$\theta$** (bao gá»“m cáº£ bias).
- Khi láº¥y Ä‘áº¡o hÃ m (gradient), pháº£i Ä‘áº£m báº£o khi nhÃ¢n ma tráº­n/vectÆ¡ ra Ä‘Ãºng chiá»u. Dá»… gáº·p lá»—i náº¿u bá» sÃ³t dimension, nháº¥t lÃ  vá»›i bÃ i toÃ¡n nhiá»u chiá»u (nhiá»u feature), cÃ¡c phÃ©p nhÃ¢n pháº£i kiá»ƒm tra ráº¥t ká»¹.
- Thá»±c táº¿, pháº£i kiá»ƒm tra láº¡i phÃ©p nhÃ¢n: $k$ lÃ  shape $(m, 1)$, $X$ lÃ  $(m, n+1)$ (gá»“m bias), cáº§n broadcast hoáº·c dÃ¹ng `.reshape` Ä‘á»ƒ Ä‘áº£m báº£o nhÃ¢n Ä‘Ãºng cÃ¡ch.

-----


$$
X =
\begin{bmatrix}
x_1^{(1)} & x_1^{(2)} \\
x_0^{(1)} & x_0^{(2)}
\end{bmatrix}
=
\begin{bmatrix}
6.7 & 4.6 \\
1 & 1
\end{bmatrix}
$$

$$
k =
\begin{bmatrix}
-22.658 & -14.83
\end{bmatrix}
$$

* á» dÆ°á»›i Ä‘Ã¢y Ä‘ang lÃ m theo kiá»ƒu: láº¥y vector $k$ nhÃ¢n vá»›i tá»«ng cá»™t cá»§a $X$ rá»“i cá»™ng láº¡i luÃ´n Ä‘á»ƒ láº¥y tá»•ng cuá»‘i cÃ¹ng. Thá»±c ra, ta tÃ¡ch tá»«ng bÆ°á»›c sáº½ dá»… theo dÃµi hÆ¡n.

**Nháº¯c láº¡i:** Gradient tá»« backward pass theo chain rule:
$$
\frac{\partial L^{(i)}}{\partial w_j} = \underbrace{\frac{\partial L^{(i)}}{\partial \hat{y}^{(i)}}}_{\text{Ä‘áº¡o hÃ m TRÆ¯á»šC khi Ä‘áº¿n } w} \times \underbrace{\frac{\partial \hat{y}^{(i)}}{\partial w_j}}_{\text{local gradient}}
$$

$$
= \underbrace{(\hat{y}^{(i)} - y^{(i)})}_{k^{(i)} = \text{Ä‘áº¡o hÃ m tá»« output}} \cdot x_j^{(i)}
$$

**Trong Ä‘Ã³:**
- **$k^{(i)} = (\hat{y}^{(i)} - y^{(i)})$** = Äáº¡o hÃ m **TRÆ¯á»šC khi Ä‘áº¿n $w$** (gradient tá»« output backward vá»)
- **$x_j^{(i)}$** = Äáº¡o hÃ m local cá»§a $\hat{y}$ theo $w_j$
- NhÃ¢n 2 thá»© nÃ y láº¡i = chain rule = Ä‘áº¡o hÃ m Loss theo $w_j$

**BÆ°á»›c 1:** NhÃ¢n tá»«ng pháº§n tá»­ (chain rule)

$$
\underbrace{k^{(1)}}_{\substack{\text{Ä‘áº¡o hÃ m} \\ \text{trÆ°á»›c khi Ä‘áº¿n } w_1}} \times \underbrace{x_1^{(1)}}_{\text{local grad}} = (-22.658) \times 6.7 = -151.8086 \quad \text{â† } \frac{\partial L^{(1)}}{\partial w_1}
$$

$$
\underbrace{k^{(2)}}_{\substack{\text{Ä‘áº¡o hÃ m} \\ \text{trÆ°á»›c khi Ä‘áº¿n } w_1}} \times \underbrace{x_1^{(2)}}_{\text{local grad}} = (-14.83) \times 4.6 = -68.218 \quad \text{â† } \frac{\partial L^{(2)}}{\partial w_1}
$$

$$
\underbrace{k^{(1)}}_{\substack{\text{Ä‘áº¡o hÃ m} \\ \text{trÆ°á»›c khi Ä‘áº¿n } w_0}} \times \underbrace{x_0^{(1)}}_{\text{local grad}} = (-22.658) \times 1 = -22.658 \quad \text{â† } \frac{\partial L^{(1)}}{\partial w_0}
$$

$$
\underbrace{k^{(2)}}_{\substack{\text{Ä‘áº¡o hÃ m} \\ \text{trÆ°á»›c khi Ä‘áº¿n } w_0}} \times \underbrace{x_0^{(2)}}_{\text{local grad}} = (-14.83) \times 1 = -14.83 \quad \text{â† } \frac{\partial L^{(2)}}{\partial w_0}
$$

**BÆ°á»›c 2:** Cá»™ng láº¡i Ä‘á»ƒ láº¥y tá»•ng tá»«ng feature:

$$
\sum_i k^{(i)} x_1^{(i)} = -151.8086 + (-68.218) = -220.0266
$$

$$
\sum_i k^{(i)} x_0^{(i)} = -22.658 + (-14.83) = -37.488
$$

$$
\text{Káº¿t quáº£ vector tá»•ng:}
\quad
\begin{bmatrix}
-220.0266 \\
-37.488
\end{bmatrix}
$$

---

### **ğŸ¤” Táº¡i sao KHÃ”NG dÃ¹ng cÃ¡ch trÃªn mÃ  pháº£i vectorize?**

**Váº¥n Ä‘á» vá»›i cÃ¡ch 1 (tÃ­nh tá»«ng pháº§n tá»­):**

```python
# Code vá»›i 2 samples, 2 features nhÆ° trÃªn
grad_w1 = 0
grad_w0 = 0

for i in range(m):  # Loop qua tá»«ng sample
    grad_w1 += k[i] * X[0, i]  # Feature x1
    grad_w0 += k[i] * X[1, i]  # Feature x0 (bias)
```

âŒ **Háº¡n cháº¿ nghiÃªm trá»ng:**
1. **Cháº­m:** Python loop cháº­m hÆ¡n 10-100x so vá»›i NumPy/C
2. **KhÃ´ng scale:** Vá»›i 1 triá»‡u samples â†’ 1 triá»‡u láº§n láº·p
3. **KhÃ´ng táº­n dá»¥ng hardware:** CPU/GPU cÃ³ thá»ƒ tÃ­nh song song nhÆ°ng loop lÃ  tuáº§n tá»±
4. **Code dÃ i vÃ  dá»… lá»—i:** Pháº£i viáº¿t loop cho má»—i feature

**ğŸ’¡ Giáº£i phÃ¡p: Vectorization (cÃ¡ch 2)**

Thay vÃ¬ loop, ta dÃ¹ng **phÃ©p toÃ¡n ma tráº­n** Ä‘á»ƒ tÃ­nh Táº¤T Cáº¢ samples cÃ¹ng lÃºc:

**CÃ¡ch 2: Vectorization**

**BÆ°á»›c 1:** TÃ­nh $k$ = Ä‘áº¡o hÃ m **TRÆ¯á»šC khi Ä‘áº¿n W** (gradient tá»« output)

$$
k = 2(\hat{y} - y^T) = 
\begin{bmatrix}
k^{(1)} & k^{(2)}
\end{bmatrix}
=
\begin{bmatrix}
-22.658 & -14.83
\end{bmatrix}
\quad \text{(Ä‘áº¡o hÃ m táº¡i output má»—i sample)}
$$

**BÆ°á»›c 2:** Ma tráº­n input X (features Ã— samples)

$$
X = 
\begin{bmatrix}
x_1^{(1)} & x_1^{(2)} \\
x_0^{(1)} & x_0^{(2)}
\end{bmatrix}
=
\begin{bmatrix}
6.7 & 4.6 \\
1   & 1
\end{bmatrix}
$$

**BÆ°á»›c 3:** Stack $k$ thÃ nh ma tráº­n (repeat theo chiá»u features)

$$
\begin{bmatrix}
k \\
k
\end{bmatrix}
=
\begin{bmatrix}
k^{(1)} & k^{(2)} \\
k^{(1)} & k^{(2)}
\end{bmatrix}
=
\begin{bmatrix}
-22.658 & -14.83 \\
-22.658 & -14.83
\end{bmatrix}
\quad \text{(má»—i hÃ ng = gradient cho 1 feature)}
$$

**BÆ°á»›c 4:** Element-wise multiply (chain rule tá»«ng pháº§n tá»­)

$$
\underbrace{\begin{bmatrix}
k^{(1)} & k^{(2)} \\
k^{(1)} & k^{(2)}
\end{bmatrix}}_{\text{Ä‘áº¡o hÃ m trÆ°á»›c w}}
\odot
\underbrace{\begin{bmatrix}
x_1^{(1)} & x_1^{(2)} \\
x_0^{(1)} & x_0^{(2)}
\end{bmatrix}}_{\text{local gradient}}
=
\begin{bmatrix}
k^{(1)} x_1^{(1)} & k^{(2)} x_1^{(2)} \\
k^{(1)} x_0^{(1)} & k^{(2)} x_0^{(2)}
\end{bmatrix}
=
\begin{bmatrix}
-151.8086 & -68.218 \\
-22.658 & -14.83
\end{bmatrix}
$$

Má»—i pháº§n tá»­ = $k^{(i)} \times x_j^{(i)}$ = gradient cá»§a Loss theo $w_j$ tá»« sample $i$

**Kiá»ƒm tra tÃ­nh toÃ¡n:**
- $k^{(1)} \times x_1^{(1)} = (-22.658) \times 6.7 = -151.8086$ 
- $k^{(2)} \times x_1^{(2)} = (-14.83) \times 4.6 = -68.218$ 
**BÆ°á»›c 5:** Sum theo samples (nhÃ¢n vá»›i [1; 1] = cá»™ng cÃ¡c cá»™t)

$$
\text{Gradient: } \frac{\partial L}{\partial W} = 
\underbrace{\begin{bmatrix}
\frac{\partial L^{(1)}}{\partial w_1} & \frac{\partial L^{(2)}}{\partial w_1} \\
\frac{\partial L^{(1)}}{\partial w_0} & \frac{\partial L^{(2)}}{\partial w_0}
\end{bmatrix}}_{\text{gradient tá»« má»—i sample}}
\begin{bmatrix}
1 \\
1
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial L^{(1)}}{\partial w_1} + \frac{\partial L^{(2)}}{\partial w_1} \\
\frac{\partial L^{(1)}}{\partial w_0} + \frac{\partial L^{(2)}}{\partial w_0}
\end{bmatrix}
$$

$$
=
\begin{bmatrix}
-151.8086 + (-68.218) \\
-22.658 + (-14.83)
\end{bmatrix}
=
\begin{bmatrix}
-220.0266 \\
-37.488
\end{bmatrix}
$$

### **ğŸ“Œ Update Weight (Cáº­p nháº­t Trá»ng sá»‘)**

Sau khi tÃ­nh Ä‘Æ°á»£c gradient, ta cáº­p nháº­t trá»ng sá»‘ theo cÃ´ng thá»©c **Gradient Descent**:

$$
\theta_{\text{new}} = \theta_{\text{old}} - \eta \cdot \nabla_\theta \mathcal{L}
$$

**Trong Ä‘Ã³:**
- $\theta_{\text{old}}$: Trá»ng sá»‘ hiá»‡n táº¡i
- $\eta$ (learning rate): Tá»‘c Ä‘á»™ há»c (vÃ­ dá»¥: 0.01, 0.001)
- $\nabla_\theta \mathcal{L}$: Gradient cá»§a Loss theo $\theta$
- $\theta_{\text{new}}$: Trá»ng sá»‘ sau khi cáº­p nháº­t

**ğŸ¯ Ã nghÄ©a:**
- Gradient **chá»‰ hÆ°á»›ng tÄƒng nhanh nháº¥t** cá»§a Loss
- Ta **trá»« Ä‘i gradient** Ä‘á»ƒ Ä‘i theo hÆ°á»›ng **giáº£m Loss**
- Learning rate **kiá»ƒm soÃ¡t bÆ°á»›c nháº£y**: quÃ¡ lá»›n â†’ khÃ´ng há»™i tá»¥, quÃ¡ nhá» â†’ há»c cháº­m

---

**ğŸ“Š VÃ­ dá»¥ cá»¥ thá»ƒ vá»›i data trÃªn:**

**GiÃ¡ trá»‹ ban Ä‘áº§u:**
$$
\theta_{\text{old}} = 
\begin{bmatrix}
w \\
b
\end{bmatrix}
=
\begin{bmatrix}
-0.34 \\
0.049
\end{bmatrix}
$$

**Gradient vá»«a tÃ­nh Ä‘Æ°á»£c:**
$$
\nabla_\theta \mathcal{L} = 
\begin{bmatrix}
\frac{\partial L}{\partial w} \\
\frac{\partial L}{\partial b}
\end{bmatrix}
=
\begin{bmatrix}
-220.0266 \\
-37.488
\end{bmatrix}
$$

**Learning rate:**
$$
\eta = 0.01
$$

**Cáº­p nháº­t:**
$$
\begin{aligned}
\theta_{\text{new}} &= \theta_{\text{old}} - \eta \cdot \nabla_\theta \mathcal{L} \\
&= 
\begin{bmatrix}
-0.34 \\
0.049
\end{bmatrix}
- 0.01 \times
\begin{bmatrix}
-220.0266 \\
-37.488
\end{bmatrix}
\\
&=
\begin{bmatrix}
-0.34 \\
0.049
\end{bmatrix}
-
\begin{bmatrix}
-2.200266 \\
-0.37488
\end{bmatrix}
\\
&=
\begin{bmatrix}
-0.34 - (-2.200266) \\
0.049 - (-0.37488)
\end{bmatrix}
\\
&=
\begin{bmatrix}
-0.34 + 2.200266 \\
0.049 + 0.37488
\end{bmatrix}
\\
&=
\begin{bmatrix}
1.860266 \\
0.42388
\end{bmatrix}
\end{aligned}
$$

**âœ… Káº¿t quáº£:**
- $w$ thay Ä‘á»•i tá»« $-0.34$ â†’ $1.860266$ (tÄƒng máº¡nh)
- $b$ thay Ä‘á»•i tá»« $0.049$ â†’ $0.42388$ (tÄƒng máº¡nh)
- Loss sáº½ **giáº£m** á»Ÿ iteration tiáº¿p theo!

---

**ğŸ”„ QuÃ¡ trÃ¬nh Training Ä‘áº§y Ä‘á»§:**

```python
# Khá»Ÿi táº¡o
theta = np.array([[-0.34], [0.049]])  # [w, b]
lr = 0.01
m = 2  # mini-batch size

# Forward pass
y_hat = theta.T.dot(X)  # X shape: (2, m)

# Compute loss
loss = (1/m) * (y_hat - y.T).dot((y_hat - y.T).T)

# Compute gradient
k = 2 * (y_hat - y.T)  # shape: (1, m)
gradients = np.multiply(np.vstack((k, k)), X)  # element-wise
gradients = gradients.dot(np.ones((m, 1))) / m  # sum vÃ  average

# Update weights
theta = theta - lr * gradients 

print(f"Theta má»›i: {theta}")
```

**Output:**
```
Theta má»›i: [[1.860266]
            [0.42388]]
```

---

**ğŸ“ Vectorization cho Update:**

**KhÃ´ng vectorize (loop):**
```python
for i in range(len(theta)):
    theta[i] = theta[i] - lr * gradients[i]
```
âŒ Cháº­m vá»›i nhiá»u parameters

**CÃ³ vectorize:**
```python
theta = theta - lr * gradients  # 1 dÃ²ng, tÃ­nh Táº¤T Cáº¢ parameters cÃ¹ng lÃºc
```
âœ… Nhanh 10-100x, táº­n dá»¥ng CPU/GPU parallelism

---

** LÆ°u Ã½ quan trá»ng:**

1. **Shape consistency:**
   - `theta`: `(n_features, 1)` - vector cá»™t
   - `gradients`: `(n_features, 1)` - vector cá»™t
   - Pháº£i **cÃ¹ng shape** má»›i trá»« Ä‘Æ°á»£c!

2. **Learning rate:**
   - QuÃ¡ lá»›n (>0.1): cÃ³ thá»ƒ **diverge** (Loss tÄƒng)
   - QuÃ¡ nhá» (<0.0001): **há»™i tá»¥ cháº­m**
   - ThÆ°á»ng dÃ¹ng: 0.001 - 0.01

3. **Batch size áº£nh hÆ°á»Ÿng:**
   - Batch nhá»: gradient **noisy** nhÆ°ng **cáº­p nháº­t nhanh**
   - Batch lá»›n: gradient **stable** nhÆ°ng **tÃ­nh toÃ¡n cháº­m**


**TrÆ°á»›c khi lÃ m ta nÃªn váº½ computational graph ra Ä‘á»ƒ cÃ³ thá»ƒ hÃ¬nh dung vá» forward vÃ  backward**
--------------------------------------------------------


## **Way2: Samples theo ROW (Phá»• biáº¿n hÆ¡n)**

ÄÃ¢y lÃ  cÃ¡ch tá»• chá»©c data **phá»• biáº¿n nháº¥t** trong ML (giá»‘ng sklearn, PyTorch):
- **Má»—i ROW = 1 sample**
- **Má»—i COLUMN = 1 feature**

### **ğŸ“Š Setup Data**

$$
X = \begin{bmatrix}
6.7 & 1 \\
4.6 & 1
\end{bmatrix}
\quad 
\begin{matrix}
\leftarrow \text{sample 1} \\
\leftarrow \text{sample 2}
\end{matrix}
$$

$$
X \text{ shape: } (m, n+1) = (2, 2)
$$

$$
y = 
\begin{bmatrix}
9.1 \\
5.9
\end{bmatrix}
\quad \text{(cá»™t)}
$$

$$
\theta =
\begin{bmatrix}
w\\
b
\end{bmatrix}
=
\begin{bmatrix}
-0.34 \\
0.049
\end{bmatrix}
\quad \text{shape: } (2, 1)
$$

### **ğŸ”¹ Forward Pass**

**KhÃ¡c vá»›i Way1:** BÃ¢y giá» ta dÃ¹ng $X \cdot \theta$ (khÃ´ng pháº£i $\theta^T \cdot X$)

$$
\hat{y} = X \cdot \theta
$$

$$
= \begin{bmatrix}
6.7 & 1 \\
4.6 & 1
\end{bmatrix}
\begin{bmatrix}
-0.34 \\
0.049
\end{bmatrix}
$$

$$
= \begin{bmatrix}
6.7 \times (-0.34) + 1 \times 0.049 \\
4.6 \times (-0.34) + 1 \times 0.049
\end{bmatrix}
$$

$$
= \begin{bmatrix}
-2.278 + 0.049 \\
-1.564 + 0.049
\end{bmatrix}
= \begin{bmatrix}
-2.229 \\
-1.515
\end{bmatrix}
$$

**âœ… Káº¿t quáº£ giá»‘ng Way1!** (chá»‰ khÃ¡c shape: vector cá»™t thay vÃ¬ dÃ²ng)

### **ğŸ”¢ TÃ­nh Loss**

$$
\hat{y} - y = 
\begin{bmatrix}
-2.229 \\
-1.515
\end{bmatrix}
-
\begin{bmatrix}
9.1 \\
5.9
\end{bmatrix}
=
\begin{bmatrix}
-11.329 \\
-7.415
\end{bmatrix}
$$

$$
L = \frac{1}{m} (\hat{y} - y)^T (\hat{y} - y)
$$

$$
= \frac{1}{2}
\begin{bmatrix}
-11.329 & -7.415
\end{bmatrix}
\begin{bmatrix}
-11.329 \\
-7.415
\end{bmatrix}
$$

$$
= \frac{1}{2} \times 183.33 = 91.665
$$


### **ğŸ“ TÃ­nh Gradient**

**BÆ°á»›c 1: TÃ­nh k (gradient tá»« loss vá» $\hat{y}$)**

$$
k = \frac{\partial L}{\partial \hat{y}} = 2(\hat{y} - y)
$$

$$
= 2 \times
\begin{bmatrix}
-11.329 \\
-7.415
\end{bmatrix}
=
\begin{bmatrix}
-22.658 \\
-14.83
\end{bmatrix}
\quad \text{shape: } (m, 1) = (2, 1)
$$

**BÆ°á»›c 2: Gradient theo $\theta$ (Chain rule)**

Vá»›i $\hat{y} = X \cdot \theta$:

$$
\frac{\partial L}{\partial \theta} = \frac{\partial \hat{y}}{\partial \theta}^T \cdot \frac{\partial L}{\partial \hat{y}}
$$

$$
= X^T \cdot k
$$

**TÃ­nh cá»¥ thá»ƒ:**

$$
\nabla_\theta L = X^T \cdot k
$$

$$
= \begin{bmatrix}
6.7 & 4.6 \\
1 & 1
\end{bmatrix}
\begin{bmatrix}
-22.658 \\
-14.83
\end{bmatrix}
$$

$$
= \begin{bmatrix}
6.7 \times (-22.658) + 4.6 \times (-14.83) \\
1 \times (-22.658) + 1 \times (-14.83)
\end{bmatrix}
$$

$$
= \begin{bmatrix}
-151.8086 + (-68.218) \\
-22.658 + (-14.83)
\end{bmatrix}
= \begin{bmatrix}
-220.0266 \\
-37.488
\end{bmatrix}
$$



### **ğŸ”„ Update Weight**

$$
\theta_{\text{new}} = \theta_{\text{old}} - \eta \cdot \nabla_\theta L
$$

$$
= \begin{bmatrix}
-0.34 \\
0.049
\end{bmatrix}
- 0.01 \times
\begin{bmatrix}
-220.0266 \\
-37.488
\end{bmatrix}
$$

$$
= \begin{bmatrix}
-0.34 + 2.200266 \\
0.049 + 0.37488
\end{bmatrix}
= \begin{bmatrix}
1.860266 \\
0.42388
\end{bmatrix}
$$



### **ğŸ’» Code Python (Way2)**

```python
import numpy as np

# Data: samples theo ROW
X = np.array([[6.7, 1],
              [4.6, 1]])  # shape: (2, 2)
y = np.array([[9.1],
              [5.9]])     # shape: (2, 1)
theta = np.array([[-0.34],
                  [0.049]]) # shape: (2, 1)
lr = 0.01

# Forward pass
y_hat = X.dot(theta)  # shape: (2, 1)
print(f"y_hat:\n{y_hat}")

# Compute loss
loss = (1/len(X)) * (y_hat - y).T.dot(y_hat - y)
print(f"\nLoss: {loss[0,0]}")

# Compute gradient
k = 2 * (y_hat - y)  # shape: (2, 1)
gradients = X.T.dot(k)  # shape: (2, 1)
print(f"\nGradients:\n{gradients}")

# Update weights
theta_new = theta - lr * gradients
print(f"\nTheta new:\n{theta_new}")
```

**Output:**
```
y_hat:
[[-2.229]
 [-1.515]]

Loss: 91.66499999999999

Gradients:
[[-220.0266]
 [ -37.488 ]]

Theta new:
[[1.860266]
 [0.42388 ]]
```


---

## **Way3: Má»Ÿ rá»™ng lÃªn M Samples (Mini-batch)**

BÃ¢y giá» tÄƒng tá»« 2 samples lÃªn **4 samples** Ä‘á»ƒ tháº¥y rÃµ sá»©c máº¡nh cá»§a vectorization!

### **ğŸ“Š Setup Data - 4 Samples**

Sá»­ dá»¥ng cáº£ 4 dÃ²ng data:

| Feature (x) | Label (y) |
|-------------|-----------|
| 6.7         | 9.1       |
| 4.6         | 5.9       |
| 3.5         | 4.6       |
| 5.5         | 6.7       |

**Tá»• chá»©c theo Way1 (features theo cá»™t, samples theo cá»™t):**

$$
X = 
\begin{bmatrix}
x_1^{(1)} & x_1^{(2)} & x_1^{(3)} & x_1^{(4)} \\
x_0^{(1)} & x_0^{(2)} & x_0^{(3)} & x_0^{(4)}
\end{bmatrix}
=
\begin{bmatrix}
6.7 & 4.6 & 3.5 & 5.5 \\
1 & 1 & 1 & 1
\end{bmatrix}
$$

$$
X \text{ shape: } (n+1, m) = (2, 4)
$$

$$
y = 
\begin{bmatrix}
9.1 \\
5.9 \\
4.6 \\
6.7
\end{bmatrix}
\quad \text{(cá»™t)}
$$

$$
\theta =
\begin{bmatrix}
w\\
b
\end{bmatrix}
=
\begin{bmatrix}
-0.34 \\
0.049
\end{bmatrix}
\quad \text{shape: } (2, 1)
$$

### **ğŸ”¹ Forward Pass - TÃ­nh Táº¤T Cáº¢ 4 samples cÃ¹ng lÃºc**

$$
\hat{y} = \theta^T \cdot X
$$

$$
= 
\begin{bmatrix}
-0.34 & 0.049
\end{bmatrix}
\begin{bmatrix}
6.7 & 4.6 & 3.5 & 5.5 \\
1 & 1 & 1 & 1
\end{bmatrix}
$$

**TÃ­nh tá»«ng sample:**

$$
\begin{aligned}
\hat{y}^{(1)} &= -0.34 \times 6.7 + 0.049 \times 1 = -2.278 + 0.049 = -2.229 \\
\hat{y}^{(2)} &= -0.34 \times 4.6 + 0.049 \times 1 = -1.564 + 0.049 = -1.515 \\
\hat{y}^{(3)} &= -0.34 \times 3.5 + 0.049 \times 1 = -1.19 + 0.049 = -1.141 \\
\hat{y}^{(4)} &= -0.34 \times 5.5 + 0.049 \times 1 = -1.87 + 0.049 = -1.821
\end{aligned}
$$

$$
\hat{y} = 
\begin{bmatrix}
-2.229 & -1.515 & -1.141 & -1.821
\end{bmatrix}
\quad \text{(dÃ²ng)}
$$

**ğŸ’¡ Chá»‰ 1 phÃ©p nhÃ¢n ma tráº­n â†’ TÃ­nh 4 predictions cÃ¹ng lÃºc!**

### **ğŸ”¢ TÃ­nh Loss (MSE) cho 4 samples**

**BÆ°á»›c 1: TÃ­nh sai sá»‘ má»—i sample**

$$
\hat{y} - y^T = 
\begin{bmatrix}
-2.229 & -1.515 & -1.141 & -1.821
\end{bmatrix}
-
\begin{bmatrix}
9.1 & 5.9 & 4.6 & 6.7
\end{bmatrix}
$$

$$
=
\begin{bmatrix}
-11.329 & -7.415 & -5.741 & -8.521
\end{bmatrix}
$$

**BÆ°á»›c 2: TÃ­nh Loss trung bÃ¬nh**

$$
L = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2
$$

$$
= \frac{1}{4} \left[ (-11.329)^2 + (-7.415)^2 + (-5.741)^2 + (-8.521)^2 \right]
$$

$$
= \frac{1}{4} \left[ 128.35 + 54.98 + 32.96 + 72.61 \right]
$$

$$
= \frac{1}{4} \times 288.9 = 72.225
$$

**Hoáº·c dÃ¹ng vector:**

$$
L = \frac{1}{m} (\hat{y} - y^T) \cdot (\hat{y} - y^T)^T
$$

$$
= \frac{1}{4}
\begin{bmatrix}
-11.329 & -7.415 & -5.741 & -8.521
\end{bmatrix}
\begin{bmatrix}
-11.329 \\
-7.415 \\
-5.741 \\
-8.521
\end{bmatrix}
$$

$$
= \frac{1}{4} \times 288.9 = 72.225
$$

**ğŸ¯ Loss = 72.225** (tháº¥p hÆ¡n vá»›i 2 samples vÃ¬ trung bÃ¬nh trÃªn nhiá»u data hÆ¡n)

### **ğŸ“ TÃ­nh k (Gradient tá»« Loss vá» $\hat{y}$)**

$$
k = \frac{\partial L}{\partial \hat{y}} = \frac{2}{m}(\hat{y} - y^T)
$$

$$
= \frac{2}{4}
\begin{bmatrix}
-11.329 & -7.415 & -5.741 & -8.521
\end{bmatrix}
$$

$$
=
\begin{bmatrix}
-5.6645 & -3.7075 & -2.8705 & -4.2605
\end{bmatrix}
$$

**âš ï¸ ChÃº Ã½:** CÃ³ há»‡ sá»‘ $\frac{2}{m}$ vÃ¬ Loss cÃ³ $\frac{1}{m}$ á»Ÿ trÆ°á»›c!

### **ğŸ§® TÃ­nh Gradient theo $\theta$ - Chain Rule**

Giá»‘ng Way1, ta cáº§n nhÃ¢n **k** (gradient tá»« Loss) vá»›i **X** (local gradient):

**BÆ°á»›c 1: NhÃ¢n element-wise (chain rule)**

Má»—i $k^{(i)}$ cáº§n nhÃ¢n vá»›i tá»«ng feature cá»§a sample $i$:

$$
\text{Gradient tá»« sample } i: \quad
\begin{bmatrix}
k^{(i)} \times x_1^{(i)} \\
k^{(i)} \times x_0^{(i)}
\end{bmatrix}
$$

**Stack k thÃ nh ma tráº­n:**

$$
\begin{bmatrix}
k \\
k
\end{bmatrix}
=
\begin{bmatrix}
-5.6645 & -3.7075 & -2.8705 & -4.2605 \\
-5.6645 & -3.7075 & -2.8705 & -4.2605
\end{bmatrix}
$$

**Element-wise multiply vá»›i X:**

$$
\begin{bmatrix}
k^{(1)} & k^{(2)} & k^{(3)} & k^{(4)} \\
k^{(1)} & k^{(2)} & k^{(3)} & k^{(4)}
\end{bmatrix}
\odot
\begin{bmatrix}
6.7 & 4.6 & 3.5 & 5.5 \\
1 & 1 & 1 & 1
\end{bmatrix}
$$

$$
=
\begin{bmatrix}
-5.6645 \times 6.7 & -3.7075 \times 4.6 & -2.8705 \times 3.5 & -4.2605 \times 5.5 \\
-5.6645 \times 1 & -3.7075 \times 1 & -2.8705 \times 1 & -4.2605 \times 1
\end{bmatrix}
$$

**TÃ­nh cá»¥ thá»ƒ:**

$$
\begin{aligned}
\text{HÃ ng 1:} \quad &-37.95, \quad -17.05, \quad -10.05, \quad -23.43 \\
\text{HÃ ng 2:} \quad &-5.6645, \quad -3.7075, \quad -2.8705, \quad -4.2605
\end{aligned}
$$

$$
\text{Gradient matrix} = 
\begin{bmatrix}
-37.95 & -17.05 & -10.05 & -23.43 \\
-5.6645 & -3.7075 & -2.8705 & -4.2605
\end{bmatrix}
$$

**BÆ°á»›c 2: Sum theo samples (cá»™ng cÃ¡c cá»™t)**

$$
\nabla_\theta L = 
\begin{bmatrix}
\sum_{i=1}^{4} k^{(i)} x_1^{(i)} \\
\sum_{i=1}^{4} k^{(i)} x_0^{(i)}
\end{bmatrix}
$$

$$
=
\begin{bmatrix}
-37.95 + (-17.05) + (-10.05) + (-23.43) \\
-5.6645 + (-3.7075) + (-2.8705) + (-4.2605)
\end{bmatrix}
$$

$$
=
\begin{bmatrix}
-88.48 \\
-16.503
\end{bmatrix}
$$

**âœ… Gradient cuá»‘i cÃ¹ng:**

$$
\nabla_\theta L = 
\begin{bmatrix}
-88.48 \\
-16.503
\end{bmatrix}
$$

### **ğŸ”„ Update Weight**

$$
\theta_{\text{new}} = \theta_{\text{old}} - \eta \cdot \nabla_\theta L
$$

$$
= 
\begin{bmatrix}
-0.34 \\
0.049
\end{bmatrix}
- 0.01 \times
\begin{bmatrix}
-88.48 \\
-16.503
\end{bmatrix}
$$

$$
=
\begin{bmatrix}
-0.34 \\
0.049
\end{bmatrix}
-
\begin{bmatrix}
-0.8848 \\
-0.16503
\end{bmatrix}
$$

$$
=
\begin{bmatrix}
-0.34 + 0.8848 \\
0.049 + 0.16503
\end{bmatrix}
=
\begin{bmatrix}
0.5448 \\
0.21403
\end{bmatrix}
$$

**âœ… Káº¿t quáº£:**
- $w$ thay Ä‘á»•i tá»« $-0.34$ â†’ $0.5448$
- $b$ thay Ä‘á»•i tá»« $0.049$ â†’ $0.21403$
- Loss sáº½ giáº£m tá»« 72.225 xuá»‘ng!

### **ğŸ’» Code Python (Way3 - 4 samples)**

```python
import numpy as np

# Data: 4 samples, features theo cá»™t
X = np.array([[6.7, 4.6, 3.5, 5.5],
              [1,   1,   1,   1  ]])  # shape: (2, 4)
y = np.array([[9.1],
              [5.9],
              [4.6],
              [6.7]])  # shape: (4, 1)
theta = np.array([[-0.34],
                  [0.049]])  # shape: (2, 1)
lr = 0.01
m = 4

# Forward pass - Táº¤T Cáº¢ 4 samples cÃ¹ng lÃºc!
y_hat = theta.T.dot(X)  # shape: (1, 4)
print(f"y_hat:\n{y_hat}")

# Compute loss
loss = (1/m) * (y_hat - y.T).dot((y_hat - y.T).T)
print(f"\nLoss: {loss[0,0]}")

# Compute gradient
k = (2/m) * (y_hat - y.T)  # shape: (1, 4)
print(f"\nk (gradient tá»« loss):\n{k}")

# Element-wise multiply
grad_matrix = np.multiply(np.vstack([k, k]), X)  # shape: (2, 4)
print(f"\nGradient matrix:\n{grad_matrix}")

# Sum over samples
gradients = grad_matrix.sum(axis=1, keepdims=True)  # shape: (2, 1)
print(f"\nGradients tá»•ng:\n{gradients}")

# Update weights
theta_new = theta - lr * gradients
print(f"\nTheta new:\n{theta_new}")

# Kiá»ƒm tra loss má»›i
y_hat_new = theta_new.T.dot(X)
loss_new = (1/m) * (y_hat_new - y.T).dot((y_hat_new - y.T).T)
print(f"\nLoss má»›i: {loss_new[0,0]}")
print(f"Loss giáº£m: {loss[0,0] - loss_new[0,0]:.4f}")
```

**Output:**
```
y_hat:
[[-2.229 -1.515 -1.141 -1.821]]

Loss: 72.2252

k (gradient tá»« loss):
[[-5.6645 -3.7075 -2.8705 -4.2605]]

Gradient matrix:
[[-37.95215  -17.0545   -10.04675  -23.43275 ]
 [ -5.6645    -3.7075    -2.8705    -4.2605  ]]

Gradients tá»•ng:
[[-88.48615]
 [-16.5030 ]]

Theta new:
[[0.544486]
 [0.21403 ]]

Loss má»›i: 29.8165
Loss giáº£m: 42.4087
```

**ğŸ‰ Loss giáº£m tá»« 72.23 â†’ 29.82 chá»‰ sau 1 iteration!**

### **ğŸ“ˆ So sÃ¡nh 2 samples vs 4 samples**

| Metric | 2 Samples | 4 Samples |
|--------|-----------|-----------|
| **Loss ban Ä‘áº§u** | 91.665 | 72.225 |
| **Gradient w** | -220.03 | -88.49 |
| **Gradient b** | -37.49 | -16.50 |
| **w má»›i** | 1.860 | 0.545 |
| **b má»›i** | 0.424 | 0.214 |
| **Tá»‘c Ä‘á»™ tÃ­nh** | ğŸš€ Nhanh | ğŸš€ Nhanh (váº«n 1 phÃ©p nhÃ¢n ma tráº­n!) |

**ğŸ’¡ Äiá»ƒm quan trá»ng:**
1. **CÃ¹ng 1 dÃ²ng code** tÃ­nh Ä‘Æ°á»£c 2 samples hay 4 samples!
2. **KhÃ´ng cáº§n loop** â†’ tá»‘c Ä‘á»™ khÃ´ng Ä‘á»•i dÃ¹ tÄƒng gáº¥p Ä‘Ã´i data
3. Gradient **á»•n Ä‘á»‹nh hÆ¡n** vá»›i nhiá»u samples
4. Code **dá»… scale** lÃªn 100, 1000, 10000 samples!

### **ğŸ“ Tá»•ng káº¿t Vectorization**

**KhÃ´ng vectorize (Loop):**
```python
# Pháº£i loop qua Tá»ªNG sample â†’ Cháº­m!
for i in range(m):
    y_hat_i = theta.T.dot(X[:, i])
    grad_i = 2 * (y_hat_i - y[i]) * X[:, i]
    gradients += grad_i
```
âŒ Vá»›i m=1000: 1000 láº§n láº·p!

**CÃ³ vectorize:**
```python
# 1 dÃ²ng tÃ­nh Táº¤T Cáº¢ samples!
y_hat = theta.T.dot(X)
k = 2 * (y_hat - y.T)
gradients = np.multiply(np.vstack([k, k]), X).sum(axis=1)
```
âœ… Vá»›i m=1000: váº«n chá»‰ 1 phÃ©p tÃ­nh ma tráº­n!

**ğŸ† Vectorization = ChÃ¬a khÃ³a cá»§a Deep Learning!** 










































































