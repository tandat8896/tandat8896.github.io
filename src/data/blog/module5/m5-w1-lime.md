---
title: "LIME - Local Interpretable Model-agnostic Explanations (Cosine Distance)"
pubDatetime: 2025-10-05T15:00:00Z
featured: false
description: "TÃ¬m hiá»ƒu chi tiáº¿t vá» LIME vá»›i cosine distance Ä‘á»ƒ giáº£i thÃ­ch dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh Machine Learning"
tags: ["machine-learning", "lime", "explainable-ai", "interpretability", "xai", "cosine-distance"]
---

# LIME - Local Interpretable Model-agnostic Explanations (Cosine Distance)

## NguyÃªn lÃ½ cÆ¡ báº£n cá»§a LIME

### **Ã tÆ°á»Ÿng chÃ­nh - Táº¡i sao LIME hoáº¡t Ä‘á»™ng?**

Khi tÃ´i báº¯t Ä‘áº§u há»c vá» LIME, cÃ¢u há»i Ä‘áº§u tiÃªn tÃ´i Ä‘áº·t ra lÃ : "Táº¡i sao mÃ´ hÃ¬nh láº¡i dá»± Ä‘oÃ¡n nhÆ° váº­y?" LIME sinh ra Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y.

**1. Váº¥n Ä‘á» cáº§n giáº£i quyáº¿t:**
- MÃ´ hÃ¬nh Machine Learning (Ä‘áº·c biá»‡t lÃ  Deep Learning) thÆ°á»ng lÃ  "black box"
- ChÃºng ta khÃ´ng biáº¿t táº¡i sao mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n káº¿t quáº£ Ä‘Ã³
- Cáº§n giáº£i thÃ­ch dá»± Ä‘oÃ¡n cho tá»«ng trÆ°á»ng há»£p cá»¥ thá»ƒ (local explanation)

**2. Ã tÆ°á»Ÿng cá»§a LIME:**
- Thay vÃ¬ giáº£i thÃ­ch toÃ n bá»™ mÃ´ hÃ¬nh phá»©c táº¡p (global)
- LIME giáº£i thÃ­ch dá»± Ä‘oÃ¡n cho **má»™t Ä‘iá»ƒm dá»¯ liá»‡u cá»¥ thá»ƒ** (local)
- Sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n (linear model) Ä‘á»ƒ xáº¥p xá»‰ mÃ´ hÃ¬nh phá»©c táº¡p **xung quanh Ä‘iá»ƒm Ä‘Ã³**

**3. VÃ­ dá»¥ thá»±c táº¿:**
Giáº£ sá»­ báº¡n cÃ³ mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n giÃ¡ nhÃ , vÃ  nÃ³ dá»± Ä‘oÃ¡n má»™t cÄƒn nhÃ  cÃ³ giÃ¡ 500 triá»‡u. LIME sáº½ tráº£ lá»i:
- Diá»‡n tÃ­ch lá»›n â†’ tÄƒng giÃ¡ +150 triá»‡u
- Vá»‹ trÃ­ tá»‘t â†’ tÄƒng giÃ¡ +100 triá»‡u  
- NhÃ  cÅ© â†’ giáº£m giÃ¡ -50 triá»‡u
- Sá»‘ phÃ²ng nhiá»u â†’ tÄƒng giÃ¡ +80 triá»‡u

### **CÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n**

**1. Local vs Global Explanation:**
- **Global**: Giáº£i thÃ­ch toÃ n bá»™ mÃ´ hÃ¬nh (feature importance tá»•ng thá»ƒ)
- **Local**: Giáº£i thÃ­ch má»™t dá»± Ä‘oÃ¡n cá»¥ thá»ƒ (táº¡i sao Ä‘iá»ƒm nÃ y Ä‘Æ°á»£c dá»± Ä‘oÃ¡n nhÆ° váº­y)

**2. Model-agnostic:**
- LIME hoáº¡t Ä‘á»™ng vá»›i **báº¥t ká»³ mÃ´ hÃ¬nh nÃ o**: Random Forest, XGBoost, Neural Network, SVM...
- KhÃ´ng cáº§n biáº¿t cáº¥u trÃºc bÃªn trong cá»§a mÃ´ hÃ¬nh
- Chá»‰ cáº§n mÃ´ hÃ¬nh cÃ³ thá»ƒ dá»± Ä‘oÃ¡n (predict)

**3. Interpretable Representation:**
- Biáº¿n Ä‘á»•i dá»¯ liá»‡u phá»©c táº¡p thÃ nh dáº¡ng dá»… hiá»ƒu
- VÃ­ dá»¥: Text â†’ CÃ³/KhÃ´ng cÃ³ tá»« khÃ³a, Image â†’ CÃ³/KhÃ´ng cÃ³ vÃ¹ng áº£nh

**4. Perturbation (Nhiá»…u loáº¡n):**
- Táº¡o cÃ¡c máº«u dá»¯ liá»‡u má»›i báº±ng cÃ¡ch thay Ä‘á»•i nháº¹ dá»¯ liá»‡u gá»‘c
- Xem mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n nhÆ° tháº¿ nÃ o vá»›i cÃ¡c máº«u nÃ y
- Tá»« Ä‘Ã³ há»c Ä‘Æ°á»£c feature nÃ o quan trá»ng

### **CÃ´ng thá»©c toÃ¡n há»c**

**1. Má»¥c tiÃªu cá»§a LIME:**
$$\text{explanation}(x) = \arg\min_{g \in G} L(f, g, \pi_x) + \Omega(g)$$

Trong Ä‘Ã³:
- $x$: Äiá»ƒm dá»¯ liá»‡u cáº§n giáº£i thÃ­ch
- $f$: MÃ´ hÃ¬nh phá»©c táº¡p (black box)
- $g$: MÃ´ hÃ¬nh Ä‘Æ¡n giáº£n (interpretable model, thÆ°á»ng lÃ  linear)
- $G$: Táº­p cÃ¡c mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n
- $L$: Loss function (Ä‘o Ä‘á»™ khÃ¡c biá»‡t giá»¯a $f$ vÃ  $g$)
- $\pi_x$: Proximity measure (Ä‘á»™ gáº§n vá»›i $x$)
- $\Omega(g)$: Complexity cá»§a mÃ´ hÃ¬nh $g$

**2. Loss function:**
$$L(f, g, \pi_x) = \sum_{z \in Z} \pi_x(z) [f(z) - g(z)]^2$$

Trong Ä‘Ã³:
- $Z$: Táº­p cÃ¡c máº«u perturbed (nhiá»…u loáº¡n)
- $\pi_x(z)$: Trá»ng sá»‘ dá»±a trÃªn khoáº£ng cÃ¡ch tá»« $z$ Ä‘áº¿n $x$
- $f(z)$: Dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh phá»©c táº¡p
- $g(z)$: Dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n

**3. Proximity measure vá»›i Cosine Distance:**
$$\pi_x(z) = \exp\left(-\frac{D_{cosine}(x, z)^2}{\sigma^2}\right)$$

**CÃ´ng thá»©c Cosine Distance:**
$$D_{cosine}(x, z) = 1 - \frac{x \cdot z}{||x|| \times ||z||} = 1 - \cos(\theta)$$

Trong Ä‘Ã³:
- $x \cdot z$: Dot product cá»§a hai vector
- $||x||, ||z||$: Norm cá»§a vector
- $\theta$: GÃ³c giá»¯a hai vector
- $\sigma$: Kernel width (Ä‘á»™ rá»™ng cá»§a kernel)

### **Thuáº­t toÃ¡n LIME - Chi tiáº¿t tá»«ng bÆ°á»›c**

**Input:** 
- Äiá»ƒm dá»¯ liá»‡u $x$ cáº§n giáº£i thÃ­ch
- MÃ´ hÃ¬nh $f$ (black box)
- Sá»‘ lÆ°á»£ng máº«u perturbed $N$

**Output:** 
- Trá»ng sá»‘ cá»§a cÃ¡c features (giáº£i thÃ­ch local)

**BÆ°á»›c 1: Táº¡o perturbed samples**
- Táº¡o $N$ máº«u dá»¯ liá»‡u má»›i báº±ng cÃ¡ch thay Ä‘á»•i nháº¹ $x$
- VÃ­ dá»¥: Vá»›i tabular data, thay Ä‘á»•i giÃ¡ trá»‹ má»™t sá»‘ features

**BÆ°á»›c 2: Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh gá»‘c**
- Sá»­ dá»¥ng mÃ´ hÃ¬nh $f$ Ä‘á»ƒ dá»± Ä‘oÃ¡n cho táº¥t cáº£ perturbed samples
- LÆ°u láº¡i káº¿t quáº£ dá»± Ä‘oÃ¡n

**BÆ°á»›c 3: TÃ­nh trá»ng sá»‘ proximity (Cosine Distance)**
- TÃ­nh cosine distance tá»« má»—i perturbed sample Ä‘áº¿n $x$
- Máº«u cÃ ng gáº§n $x$ (cosine similarity cao) thÃ¬ trá»ng sá»‘ cÃ ng cao

**BÆ°á»›c 4: Fit mÃ´ hÃ¬nh linear**
- Sá»­ dá»¥ng perturbed samples lÃ m training data
- Fit mÃ´ hÃ¬nh linear vá»›i trá»ng sá»‘ proximity
- MÃ´ hÃ¬nh linear nÃ y giáº£i thÃ­ch mÃ´ hÃ¬nh phá»©c táº¡p **xung quanh $x$**

**BÆ°á»›c 5: TrÃ­ch xuáº¥t feature importance**
- Há»‡ sá»‘ cá»§a mÃ´ hÃ¬nh linear chÃ­nh lÃ  feature importance
- Há»‡ sá»‘ dÆ°Æ¡ng â†’ feature tÄƒng prediction
- Há»‡ sá»‘ Ã¢m â†’ feature giáº£m prediction

---

## VÃ­ Dá»¥ TÃ­nh Tay - LIME vá»›i Cosine Distance

### **Dataset vÃ  MÃ´ hÃ¬nh**

Giáº£ sá»­ tÃ´i cÃ³ má»™t mÃ´ hÃ¬nh XGBoost dá»± Ä‘oÃ¡n xem má»™t ngÆ°á»i cÃ³ mua sáº£n pháº©m hay khÃ´ng dá»±a trÃªn 4 features:

| Feature | MÃ´ táº£ | ÄÆ¡n vá»‹ |
|---------|-------|--------|
| Age | Tuá»•i | nÄƒm |
| Income | Thu nháº­p | triá»‡u/nÄƒm |
| TimeOnSite | Thá»i gian trÃªn website | phÃºt |
| PreviousPurchases | Sá»‘ láº§n mua trÆ°á»›c Ä‘Ã³ | láº§n |

**Äiá»ƒm dá»¯ liá»‡u cáº§n giáº£i thÃ­ch:**

| Age | Income | TimeOnSite | PreviousPurchases | Prediction |
|-----|--------|------------|-------------------|------------|
| 35 | 50 | 15 | 3 | 0.85 (85% mua) |

MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n ngÆ°á»i nÃ y cÃ³ **85% kháº£ nÄƒng mua sáº£n pháº©m**. NhÆ°ng táº¡i sao?

---

### **Step 1: Táº¡o Perturbed Samples**

TÃ´i sáº½ táº¡o 5 máº«u perturbed báº±ng cÃ¡ch thay Ä‘á»•i nháº¹ cÃ¡c giÃ¡ trá»‹:

| ID | Age | Income | TimeOnSite | PreviousPurchases | Cosine_Distance | Weight | Model_Prediction |
|----|-----|--------|------------|-------------------|-----------------|--------|------------------|
| Original | 35 | 50 | 15 | 3 | 0.000 | 1.000 | 0.85 |
| Sample_1 | 33 | 48 | 14 | 3 | 0.008 | 0.984 | 0.78 |
| Sample_2 | 37 | 52 | 16 | 4 | 0.012 | 0.976 | 0.92 |
| Sample_3 | 35 | 45 | 12 | 2 | 0.025 | 0.951 | 0.65 |
| Sample_4 | 32 | 50 | 18 | 3 | 0.015 | 0.970 | 0.82 |
| Sample_5 | 38 | 55 | 15 | 5 | 0.018 | 0.965 | 0.88 |

**Giáº£i thÃ­ch cÃ¡c cá»™t:**

**Cosine Distance:** Khoáº£ng cÃ¡ch cosine giá»¯a sample vÃ  original point

**TÃ­nh toÃ¡n chi tiáº¿t cho Sample_1:**
- Original: $x = [35, 50, 15, 3]$
- Sample_1: $z = [33, 48, 14, 3]$

**Dot product:**
$$x \cdot z = 35 \times 33 + 50 \times 48 + 15 \times 14 + 3 \times 3 = 1155 + 2400 + 210 + 9 = 3774$$

**Norms:**
$$||x|| = \sqrt{35^2 + 50^2 + 15^2 + 3^2} = \sqrt{1225 + 2500 + 225 + 9} = \sqrt{3959} = 62.92$$
$$||z|| = \sqrt{33^2 + 48^2 + 14^2 + 3^2} = \sqrt{1089 + 2304 + 196 + 9} = \sqrt{3598} = 59.98$$

**Cosine similarity:**
$$\cos(\theta) = \frac{x \cdot z}{||x|| \times ||z||} = \frac{3774}{62.92 \times 59.98} = \frac{3774}{3774.1} = 0.9997$$

**Cosine distance:**
$$D_{cosine}(x, z) = 1 - \cos(\theta) = 1 - 0.9997 = 0.0003$$

**Weight vá»›i $\sigma = 0.1$:**
$$\pi_x(z) = \exp\left(-\frac{0.0003^2}{0.1^2}\right) = \exp(-0.0009) = 0.9991$$

**TÃ­nh toÃ¡n tÆ°Æ¡ng tá»± cho cÃ¡c samples khÃ¡c:**

**Sample_2:**
- $z = [37, 52, 16, 4]$
- $x \cdot z = 35 \times 37 + 50 \times 52 + 15 \times 16 + 3 \times 4 = 1295 + 2600 + 240 + 12 = 4147$
- $||z|| = \sqrt{37^2 + 52^2 + 16^2 + 4^2} = \sqrt{1369 + 2704 + 256 + 16} = \sqrt{4345} = 65.92$
- $\cos(\theta) = \frac{4147}{62.92 \times 65.92} = \frac{4147}{4147.7} = 0.9998$
- $D_{cosine} = 1 - 0.9998 = 0.0002$
- Weight = $\exp(-0.0004) = 0.9996$

**Sample_3:**
- $z = [35, 45, 12, 2]$
- $x \cdot z = 35 \times 35 + 50 \times 45 + 15 \times 12 + 3 \times 2 = 1225 + 2250 + 180 + 6 = 3661$
- $||z|| = \sqrt{35^2 + 45^2 + 12^2 + 2^2} = \sqrt{1225 + 2025 + 144 + 4} = \sqrt{3398} = 58.29$
- $\cos(\theta) = \frac{3661}{62.92 \times 58.29} = \frac{3661}{3667.6} = 0.9982$
- $D_{cosine} = 1 - 0.9982 = 0.0018$
- Weight = $\exp(-0.0032) = 0.9968$

**Sample_4:**
- $z = [32, 50, 18, 3]$
- $x \cdot z = 35 \times 32 + 50 \times 50 + 15 \times 18 + 3 \times 3 = 1120 + 2500 + 270 + 9 = 3899$
- $||z|| = \sqrt{32^2 + 50^2 + 18^2 + 3^2} = \sqrt{1024 + 2500 + 324 + 9} = \sqrt{3857} = 62.10$
- $\cos(\theta) = \frac{3899}{62.92 \times 62.10} = \frac{3899}{3907.3} = 0.9979$
- $D_{cosine} = 1 - 0.9979 = 0.0021$
- Weight = $\exp(-0.0044) = 0.9956$

**Sample_5:**
- $z = [38, 55, 15, 5]$
- $x \cdot z = 35 \times 38 + 50 \times 55 + 15 \times 15 + 3 \times 5 = 1330 + 2750 + 225 + 15 = 4320$
- $||z|| = \sqrt{38^2 + 55^2 + 15^2 + 5^2} = \sqrt{1444 + 3025 + 225 + 25} = \sqrt{4719} = 68.70$
- $\cos(\theta) = \frac{4320}{62.92 \times 68.70} = \frac{4320}{4322.6} = 0.9994$
- $D_{cosine} = 1 - 0.9994 = 0.0006$
- Weight = $\exp(-0.0036) = 0.9964$

---

### **Step 2: Fit Linear Model**

BÃ¢y giá» tÃ´i sáº½ fit má»™t mÃ´ hÃ¬nh linear Ä‘Æ¡n giáº£n:

$$\text{Prediction} = \beta_0 + \beta_1 \times \text{Age} + \beta_2 \times \text{Income} + \beta_3 \times \text{TimeOnSite} + \beta_4 \times \text{PreviousPurchases}$$

Sá»­ dá»¥ng **Weighted Least Squares** vá»›i trá»ng sá»‘ lÃ  Weight column:

**TÃ­nh toÃ¡n thá»§ cÃ´ng (Ä‘Æ¡n giáº£n hÃ³a):**

TÃ´i sáº½ tÃ­nh há»‡ sá»‘ báº±ng cÃ¡ch xem sá»± thay Ä‘á»•i cá»§a prediction khi thay Ä‘á»•i tá»«ng feature:

**1. Age:**
- Sample_1: Age giáº£m 2 â†’ Prediction giáº£m 0.07 â†’ áº¢nh hÆ°á»Ÿng: +0.035/nÄƒm
- Sample_2: Age tÄƒng 2 â†’ Prediction tÄƒng 0.07 â†’ áº¢nh hÆ°á»Ÿng: +0.035/nÄƒm
- **Trung bÃ¬nh: +0.035**

**2. Income:**
- Sample_3: Income giáº£m 5 â†’ Prediction giáº£m 0.20 â†’ áº¢nh hÆ°á»Ÿng: +0.040/triá»‡u
- Sample_5: Income tÄƒng 5 â†’ Prediction tÄƒng 0.03 â†’ áº¢nh hÆ°á»Ÿng: +0.006/triá»‡u
- **Trung bÃ¬nh: +0.023**

**3. TimeOnSite:**
- Sample_3: Time giáº£m 3 â†’ Prediction giáº£m 0.20 â†’ áº¢nh hÆ°á»Ÿng: +0.067/phÃºt
- Sample_4: Time tÄƒng 3 â†’ Prediction giáº£m 0.03 â†’ áº¢nh hÆ°á»Ÿng: -0.010/phÃºt
- **Trung bÃ¬nh: +0.028**

**4. PreviousPurchases:**
- Sample_2: Purchases tÄƒng 1 â†’ Prediction tÄƒng 0.07 â†’ áº¢nh hÆ°á»Ÿng: +0.070/láº§n
- Sample_5: Purchases tÄƒng 2 â†’ Prediction tÄƒng 0.03 â†’ áº¢nh hÆ°á»Ÿng: +0.015/láº§n
- **Trung bÃ¬nh: +0.042**

**Káº¿t quáº£ Linear Model:**

$$\text{Prediction} = 0.10 + 0.035 \times \text{Age} + 0.023 \times \text{Income} + 0.028 \times \text{TimeOnSite} + 0.042 \times \text{PreviousPurchases}$$

**Kiá»ƒm tra vá»›i Ä‘iá»ƒm gá»‘c:**
$$\text{Prediction} = 0.10 + 0.035 \times 35 + 0.023 \times 50 + 0.028 \times 15 + 0.042 \times 3$$
$$= 0.10 + 1.225 + 1.150 + 0.420 + 0.126 = 3.021$$

**Normalize vá» 0.85:**
$$\text{Scale factor} = \frac{0.85}{3.021} = 0.281$$

---

### **Step 3: TÃ­nh Feature Contributions**

BÃ¢y giá» tÃ´i Ã¡p dá»¥ng mÃ´ hÃ¬nh linear cho Ä‘iá»ƒm gá»‘c:

| Feature | Value | Coefficient | Contribution | Percentage |
|---------|-------|-------------|--------------|------------|
| Intercept | 1 | 0.10 | +0.10 | 3.3% |
| Age | 35 | 0.035 | +1.225 | 40.6% |
| Income | 50 | 0.023 | +1.150 | 38.1% |
| TimeOnSite | 15 | 0.028 | +0.420 | 13.9% |
| PreviousPurchases | 3 | 0.042 | +0.126 | 4.1% |
| **Total** | | | **+3.021** | **100%** |

**CÃ¡ch tÃ­nh:**
- Contribution = Coefficient Ã— Value
- VÃ­ dá»¥ Age: 0.035 Ã— 35 = 1.225
- Percentage = (Contribution / Total) Ã— 100%

**Normalized Contributions (Ä‘á»ƒ tá»•ng = 0.85):**

| Feature | Normalized_Contribution | Impact |
|---------|------------------------|--------|
| Age | +0.345 | +40.6% |
| Income | +0.324 | +38.1% |
| TimeOnSite | +0.118 | +13.9% |
| PreviousPurchases | +0.035 | +4.1% |
| Intercept | +0.028 | +3.3% |
| **Total** | **0.85** | **100%** |

---

## Giáº£i ThÃ­ch TÃ¡c Äá»™ng Local

### **Positive Impact (TÃ¡c Ä‘á»™ng tÃ­ch cá»±c)**

ÄÃ¢y lÃ  nhá»¯ng features lÃ m **TÄ‚NG** kháº£ nÄƒng mua sáº£n pháº©m cho ngÆ°á»i nÃ y:

**1. Age = 35 (+0.345 hay +40.6%)**
- **Giáº£i thÃ­ch:** Tuá»•i 35 lÃ  Ä‘á»™ tuá»•i "vÃ ng" cho sáº£n pháº©m nÃ y
- **Táº¡i sao:** NgÆ°á»i á»Ÿ Ä‘á»™ tuá»•i nÃ y thÆ°á»ng cÃ³ thu nháº­p á»•n Ä‘á»‹nh vÃ  nhu cáº§u cao
- **TÃ¡c Ä‘á»™ng:** Náº¿u ngÆ°á»i nÃ y tráº» hÆ¡n (25 tuá»•i), kháº£ nÄƒng mua sáº½ giáº£m xuá»‘ng ~0.50

**2. Income = 50 triá»‡u (+0.324 hay +38.1%)**
- **Giáº£i thÃ­ch:** Thu nháº­p 50 triá»‡u/nÄƒm lÃ  Ä‘á»§ Ä‘á»ƒ mua sáº£n pháº©m
- **Táº¡i sao:** CÃ³ kháº£ nÄƒng chi tráº£ tá»‘t
- **TÃ¡c Ä‘á»™ng:** Náº¿u thu nháº­p giáº£m xuá»‘ng 30 triá»‡u, kháº£ nÄƒng mua giáº£m ~0.46

**3. TimeOnSite = 15 phÃºt (+0.118 hay +13.9%)**
- **Giáº£i thÃ­ch:** DÃ nh nhiá»u thá»i gian trÃªn website â†’ quan tÃ¢m sáº£n pháº©m
- **Táº¡i sao:** Thá»i gian dÃ i = nghiÃªn cá»©u ká»¹ = cÃ³ Ã½ Ä‘á»‹nh mua
- **TÃ¡c Ä‘á»™ng:** Náº¿u chá»‰ á»Ÿ 5 phÃºt, kháº£ nÄƒng mua giáº£m ~0.28

**4. PreviousPurchases = 3 láº§n (+0.035 hay +4.1%)**
- **Giáº£i thÃ­ch:** ÄÃ£ mua 3 láº§n trÆ°á»›c Ä‘Ã³ â†’ khÃ¡ch hÃ ng trung thÃ nh
- **Táº¡i sao:** Tin tÆ°á»Ÿng thÆ°Æ¡ng hiá»‡u
- **TÃ¡c Ä‘á»™ng:** Náº¿u chÆ°a mua láº§n nÃ o, kháº£ nÄƒng mua giáº£m ~0.13

### **Negative Impact (TÃ¡c Ä‘á»™ng tiÃªu cá»±c)**

Trong vÃ­ dá»¥ nÃ y, **KHÃ”NG CÃ“** feature nÃ o cÃ³ tÃ¡c Ä‘á»™ng tiÃªu cá»±c (há»‡ sá»‘ Ã¢m). NhÆ°ng Ä‘á»ƒ minh há»a, tÃ´i sáº½ táº¡o má»™t vÃ­ dá»¥ khÃ¡c:

**VÃ­ dá»¥: Má»™t ngÆ°á»i KHÃC vá»›i prediction tháº¥p**

| Feature | Value | Coefficient | Contribution | Impact |
|---------|-------|-------------|--------------|--------|
| Age | 22 | 0.035 | +0.770 | +77.0% |
| Income | 20 | 0.023 | +0.460 | +46.0% |
| TimeOnSite | 3 | 0.028 | +0.084 | +8.4% |
| PreviousPurchases | 0 | 0.042 | +0.000 | +0.0% |
| **Total** | | | **1.314** | |
| **Normalized** | | | **0.25** | **25% mua** |

**Negative Impacts (so vá»›i ngÆ°á»i Ä‘áº§u tiÃªn):**

**1. Age = 22 (thay vÃ¬ 35) â†’ Giáº£m -0.455**
- **Giáº£i thÃ­ch:** Tuá»•i tráº» â†’ thu nháº­p tháº¥p, chÆ°a cÃ³ nhu cáº§u
- **TÃ¡c Ä‘á»™ng:** LÃ m giáº£m 53.5% kháº£ nÄƒng mua

**2. Income = 20 (thay vÃ¬ 50) â†’ Giáº£m -0.690**
- **Giáº£i thÃ­ch:** Thu nháº­p tháº¥p â†’ khÃ´ng Ä‘á»§ kháº£ nÄƒng chi tráº£
- **TÃ¡c Ä‘á»™ng:** LÃ m giáº£m 81.2% kháº£ nÄƒng mua

**3. TimeOnSite = 3 (thay vÃ¬ 15) â†’ Giáº£m -0.336**
- **Giáº£i thÃ­ch:** Ãt thá»i gian â†’ khÃ´ng quan tÃ¢m sáº£n pháº©m
- **TÃ¡c Ä‘á»™ng:** LÃ m giáº£m 39.5% kháº£ nÄƒng mua

**4. PreviousPurchases = 0 (thay vÃ¬ 3) â†’ Giáº£m -0.126**
- **Giáº£i thÃ­ch:** ChÆ°a mua láº§n nÃ o â†’ chÆ°a tin tÆ°á»Ÿng
- **TÃ¡c Ä‘á»™ng:** LÃ m giáº£m 14.8% kháº£ nÄƒng mua

---

## Visualization - Biá»ƒu Äá»“ LIME

### **Feature Importance Plot**

```
PreviousPurchases (3) â–ˆâ–ˆâ–ˆâ–ˆ +0.035 (4.1%)

TimeOnSite (15)      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ +0.118 (13.9%)

Income (50)            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ +0.324 (38.1%)

Age (35)               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ +0.345 (40.6%)

                       0.0    0.1    0.2    0.3    0.4
                            Contribution to Prediction
```

### **Comparison Plot (Positive vs Negative Case)**

```
Feature          Negative Case (0.25)    Positive Case (0.85)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Age              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (22)           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (35)
Income           â–ˆâ–ˆâ–ˆâ–ˆ (20)               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (50)
TimeOnSite       â–ˆ (3)                   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (15)
PrevPurchases    (0)                     â–ˆâ–ˆ (3)
```

---

## Code Python - LIME vá»›i Cosine Distance

### **CÃ i Ä‘áº·t**

```python
pip install lime
pip install xgboost
pip install scikit-learn
```

### **VÃ­ dá»¥ hoÃ n chá»‰nh**

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from lime import lime_tabular
from sklearn.metrics.pairwise import cosine_distances

# Táº¡o dataset
np.random.seed(42)
n_samples = 1000

data = pd.DataFrame({
    'Age': np.random.randint(20, 60, n_samples),
    'Income': np.random.randint(20, 100, n_samples),
    'TimeOnSite': np.random.randint(1, 30, n_samples),
    'PreviousPurchases': np.random.randint(0, 10, n_samples)
})

# Táº¡o target (logic: ngÆ°á»i cÃ³ income cao, time_on_site cao â†’ mua nhiá»u)
data['Purchase'] = (
    (data['Income'] > 40) & 
    (data['TimeOnSite'] > 10) & 
    (data['PreviousPurchases'] > 1)
).astype(int)

print("Dataset:")
print(data.head(10))
print(f"\nPurchase rate: {data['Purchase'].mean():.2%}")

# Split data
X = data.drop('Purchase', axis=1)
y = data['Purchase']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train XGBoost model
model = XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42
)
model.fit(X_train, y_train)

# Evaluate
train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)
print(f"\nModel Performance:")
print(f"Train accuracy: {train_score:.2%}")
print(f"Test accuracy: {test_score:.2%}")

# Chá»n má»™t Ä‘iá»ƒm Ä‘á»ƒ giáº£i thÃ­ch
instance_idx = 0
instance = X_test.iloc[instance_idx].values
prediction = model.predict_proba([instance])[0]

print(f"\n=== Instance to Explain ===")
print(f"Features: {dict(zip(X.columns, instance))}")
print(f"Prediction: {prediction[1]:.2%} (class 1 - Purchase)")
print(f"True label: {y_test.iloc[instance_idx]}")

# Táº¡o LIME explainer vá»›i cosine distance
explainer = lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X.columns.tolist(),
    class_names=['No Purchase', 'Purchase'],
    mode='classification',
    random_state=42,
    distance_metric='cosine'  # Sá»­ dá»¥ng cosine distance
)

# Giáº£i thÃ­ch prediction
explanation = explainer.explain_instance(
    data_row=instance,
    predict_fn=model.predict_proba,
    num_features=4,
    num_samples=5000
)

# Hiá»ƒn thá»‹ káº¿t quáº£
print(f"\n=== LIME Explanation (Cosine Distance) ===")
print(f"Intercept: {explanation.intercept[1]:.4f}")
print(f"\nLocal prediction: {explanation.local_pred[1]:.4f}")
print(f"Model prediction: {prediction[1]:.4f}")

print(f"\nFeature Contributions:")
for feature, weight in explanation.as_list(label=1):
    impact = "POSITIVE" if weight > 0 else "NEGATIVE"
    print(f"  {feature:30s} â†’ {weight:+.4f} ({impact})")

# TÃ­nh contribution cho tá»«ng feature
print(f"\n=== Detailed Feature Analysis ===")
feature_values = dict(zip(X.columns, instance))
for feature, weight in explanation.as_list(label=1):
    feature_name = feature.split()[0]
    if feature_name in feature_values:
        value = feature_values[feature_name]
        contribution = weight * value
        print(f"{feature_name:20s} = {value:6.2f} Ã— {weight:+.4f} = {contribution:+.4f}")

# So sÃ¡nh vá»›i Euclidean distance
print(f"\n=== Comparison: Cosine vs Euclidean Distance ===")
explainer_euclidean = lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X.columns.tolist(),
    class_names=['No Purchase', 'Purchase'],
    mode='classification',
    random_state=42,
    distance_metric='euclidean'
)

explanation_euclidean = explainer_euclidean.explain_instance(
    data_row=instance,
    predict_fn=model.predict_proba,
    num_features=4,
    num_samples=5000
)

print("Cosine Distance Results:")
for feature, weight in explanation.as_list(label=1):
    print(f"  {feature:30s} â†’ {weight:+.4f}")

print("\nEuclidean Distance Results:")
for feature, weight in explanation_euclidean.as_list(label=1):
    print(f"  {feature:30s} â†’ {weight:+.4f}")
```

---

## Káº¿t luáº­n

LIME lÃ  cÃ´ng cá»¥ máº¡nh máº½ Ä‘á»ƒ giáº£i thÃ­ch dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh Machine Learning:

**âœ… Æ¯u Ä‘iá»ƒm:**
- **Model-agnostic**: Hoáº¡t Ä‘á»™ng vá»›i má»i mÃ´ hÃ¬nh
- **Local explanation**: Giáº£i thÃ­ch tá»«ng dá»± Ä‘oÃ¡n cá»¥ thá»ƒ
- **Intuitive**: Dá»… hiá»ƒu vá»›i linear model
- **Flexible**: Ãp dá»¥ng cho nhiá»u loáº¡i dá»¯ liá»‡u

**âŒ NhÆ°á»£c Ä‘iá»ƒm:**
- **Instability**: Káº¿t quáº£ cÃ³ thá»ƒ thay Ä‘á»•i
- **Slow**: Cáº§n nhiá»u perturbed samples
- **Local only**: KhÃ´ng giáº£i thÃ­ch toÃ n bá»™ mÃ´ hÃ¬nh
- **Approximation**: Chá»‰ lÃ  xáº¥p xá»‰

**ğŸ¯ Khi nÃ o sá»­ dá»¥ng:**
- Cáº§n giáº£i thÃ­ch dá»± Ä‘oÃ¡n cá»¥ thá»ƒ
- LÃ m viá»‡c vá»›i black-box models
- Cáº§n interpretability cao
- Debugging model predictions

LIME giÃºp lÃ m cho Machine Learning trá»Ÿ nÃªn **transparent** vÃ  **trustworthy** hÆ¡n!