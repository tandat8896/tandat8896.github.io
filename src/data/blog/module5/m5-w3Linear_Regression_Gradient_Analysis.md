---
title: "Há»“i Quy Tuyáº¿n TÃ­nh: Row-wise vs Column-wise Gradient"
description: "PhÃ¢n tÃ­ch chi tiáº¿t vá» hai phÆ°Æ¡ng phÃ¡p tÃ­nh gradient trong linear regression: row-wise vÃ  column-wise gradient"
pubDatetime: 2025-01-28T10:00:00Z
tags: ["Linear Regression", "Gradient Descent", "Machine Learning", "Vectorization"]
heroImage: "/assets/images/linear-regression-gradient.png"
---

# Há»“i Quy Tuyáº¿n TÃ­nh: Row-wise vs Column-wise Gradient

## ğŸ¯ Tá»•ng Quan

Trong linear regression, cÃ³ hai cÃ¡ch tiáº¿p cáº­n chÃ­nh Ä‘á»ƒ tÃ­nh gradient:
- **Row-wise gradient**: TÃ­nh gradient cho tá»«ng sample má»™t cÃ¡ch tuáº§n tá»±
- **Column-wise gradient**: TÃ­nh gradient vector hÃ³a cho táº¥t cáº£ samples cÃ¹ng lÃºc

## ğŸ“Š Row-wise Gradient - TÃNH THEO Tá»ªNG SAMPLE

### **CÃ¡ch hoáº¡t Ä‘á»™ng:**
```python
# Dá»¯ liá»‡u máº«u: 3 samples, 3 features
X = np.array([[1, 2, 3],    # Sample 1
              [4, 5, 6],    # Sample 2  
              [7, 8, 9]])   # Sample 3
y = np.array([[10], [20], [30]])  # Target values
theta = np.array([[0.1], [0.2], [0.3]])  # Parameters
```

### **Shapes Analysis:**
| Variable | Shape | **Táº I SAO?** |
|----------|-------|---------------|
| `x_i` | (3,) hoáº·c (3,1) | Features cá»§a sample thá»© i |
| `y_i` | (1,) hoáº·c (1,1) | Target cá»§a sample thá»© i |
| `theta` | (3,1) | Parameters (3 features + bias) |
| `grad` | (3,1) | Gradient cho tá»«ng parameter |

### **Code Implementation:**
```python
def row_wise_gradient(X, y, theta):
    m = X.shape[0]  # Sá»‘ samples
    gradients = np.zeros_like(theta)
    
    for i in range(m):
        # Láº¥y sample thá»© i
        x_i = X[i]  # Shape: (3,)
        y_i = y[i]  # Shape: (1,)
        
        # TÃ­nh prediction
        y_hat = x_i.dot(theta)  # Shape: scalar
        
        # TÃ­nh gradient cho sample nÃ y
        error = y_hat - y_i  # Shape: scalar
        grad_i = 2 * error * x_i  # Shape: (3,)
        
        # Cá»™ng dá»“n gradient
        gradients += grad_i.reshape(-1, 1)  # Reshape Ä‘á»ƒ cá»™ng
    
    return gradients / m  # Trung bÃ¬nh gradient
```

### **VÃ­ dá»¥ sá»‘ minh há»a (3 samples, 3 features):**

#### **Sample 1: xâ‚ = [1, 2, 3], yâ‚ = 10**
```python
x_1 = [1, 2, 3]
y_1 = 10
theta = [0.1, 0.2, 0.3]

# TÃ­nh prediction
y_hat_1 = 1*0.1 + 2*0.2 + 3*0.3 = 0.1 + 0.4 + 0.9 = 1.4

# TÃ­nh error
error_1 = 1.4 - 10 = -8.6

# TÃ­nh gradient
grad_1 = 2 * (-8.6) * [1, 2, 3] = [-17.2, -34.4, -51.6]
```

#### **Sample 2: xâ‚‚ = [4, 5, 6], yâ‚‚ = 20**
```python
x_2 = [4, 5, 6]
y_2 = 20

# TÃ­nh prediction
y_hat_2 = 4*0.1 + 5*0.2 + 6*0.3 = 0.4 + 1.0 + 1.8 = 3.2

# TÃ­nh error
error_2 = 3.2 - 20 = -16.8

# TÃ­nh gradient
grad_2 = 2 * (-16.8) * [4, 5, 6] = [-134.4, -168.0, -201.6]
```

#### **Sample 3: xâ‚ƒ = [7, 8, 9], yâ‚ƒ = 30**
```python
x_3 = [7, 8, 9]
y_3 = 30

# TÃ­nh prediction
y_hat_3 = 7*0.1 + 8*0.2 + 9*0.3 = 0.7 + 1.6 + 2.7 = 5.0

# TÃ­nh error
error_3 = 5.0 - 30 = -25.0

# TÃ­nh gradient
grad_3 = 2 * (-25.0) * [7, 8, 9] = [-350.0, -400.0, -450.0]
```

#### **Gradient cuá»‘i cÃ¹ng:**
```python
# Tá»•ng gradient
total_grad = grad_1 + grad_2 + grad_3
total_grad = [-17.2, -34.4, -51.6] + [-134.4, -168.0, -201.6] + [-350.0, -400.0, -450.0]
total_grad = [-501.6, -602.4, -703.2]

# Gradient trung bÃ¬nh
avg_grad = total_grad / 3 = [-167.2, -200.8, -234.4]
```

### **Æ¯u Ä‘iá»ƒm Row-wise:**
- âœ… **LuÃ´n an toÃ n**: KhÃ´ng cáº§n reshape y
- âœ… **Dá»… hiá»ƒu**: Logic rÃµ rÃ ng tá»«ng bÆ°á»›c
- âœ… **Debug dá»…**: CÃ³ thá»ƒ trace tá»«ng sample
- âœ… **Memory efficient**: Chá»‰ load 1 sample táº¡i má»™t thá»i Ä‘iá»ƒm

---

## ğŸš€ Column-wise Gradient - VECTOR HÃ“A

### **CÃ¡ch hoáº¡t Ä‘á»™ng:**
```python
# Dá»¯ liá»‡u máº«u: 3 samples, 3 features
X = np.array([[1, 2, 3],    # Sample 1
              [4, 5, 6],    # Sample 2  
              [7, 8, 9]])   # Sample 3
y = np.array([[10], [20], [30]])  # Target values
theta = np.array([[0.1], [0.2], [0.3]])  # Parameters
```

### **Shapes Analysis:**
| Variable | Shape | **Táº I SAO?** |
|----------|-------|---------------|
| `X` | (3, 3) | Táº¥t cáº£ features cá»§a táº¥t cáº£ samples |
| `y` | (3, 1) | Táº¥t cáº£ targets |
| `y_hat` | (3, 1) | Táº¥t cáº£ predictions |
| `grad` | (3, 1) | Gradient cho tá»«ng parameter |

### **Code Implementation - ÄÃšNG:**
```python
def column_wise_gradient_correct(X, y, theta):
    m = X.shape[0]  # Sá»‘ samples
    
    # TÃ­nh táº¥t cáº£ predictions cÃ¹ng lÃºc
    y_hat = X.dot(theta)  # Shape: (3, 3) Ã— (3, 1) = (3, 1)
    
    # TÃ­nh error cho táº¥t cáº£ samples
    error = y_hat - y  # Shape: (3, 1) - (3, 1) = (3, 1) âœ…
    
    # TÃ­nh gradient vector hÃ³a
    gradients = 2 * X.T.dot(error) / m  # Shape: (3, 3)áµ€ Ã— (3, 1) = (3, 1)
    
    return gradients
```

### **Code Implementation - SAI (Broadcasting Error):**
```python
def column_wise_gradient_wrong(X, y, theta):
    m = X.shape[0]
    
    # TÃ­nh predictions
    y_hat = X.dot(theta)  # Shape: (3, 1)
    
    # SAI: y khÃ´ng Ä‘Æ°á»£c reshape Ä‘Ãºng
    y_flat = y.flatten()  # Shape: (3,) - 1D array
    error = y_hat - y_flat  # Shape: (3, 1) - (3,) â†’ Broadcasting issues! âŒ
    
    # Gradient sáº½ sai do broadcasting
    gradients = 2 * X.T.dot(error) / m  # Káº¿t quáº£ sai! âŒ
    
    return gradients
```

### **VÃ­ dá»¥ sá»‘ minh há»a - GRADIENT ÄÃšNG:**

#### **TÃ­nh predictions:**
```python
X = [[1, 2, 3],
     [4, 5, 6], 
     [7, 8, 9]]
theta = [[0.1], [0.2], [0.3]]

# Matrix multiplication
y_hat = X.dot(theta) = [[1, 2, 3],    [[0.1],    [[1.4],
                        [4, 5, 6],  Ã—  [0.2],  =  [3.2],
                        [7, 8, 9]]     [0.3]]     [5.0]]
```

#### **TÃ­nh error:**
```python
y = [[10], [20], [30]]

# Error calculation
error = y_hat - y = [[1.4],   [[10],   [[-8.6],
                     [3.2], - [20], =  [-16.8],
                     [5.0]]    [30]]    [-25.0]]
```

#### **TÃ­nh gradient:**
```python
# X.T.dot(error)
X.T = [[1, 4, 7],
       [2, 5, 8],
       [3, 6, 9]]

gradients = 2 * X.T.dot(error) / 3
         = 2 * [[1, 4, 7],    [[-8.6],    / 3
               [2, 5, 8],  Ã—  [-16.8],
               [3, 6, 9]]     [-25.0]]

# TÃ­nh tá»«ng element:
# grad[0] = 2 * (1*(-8.6) + 4*(-16.8) + 7*(-25.0)) / 3
#         = 2 * (-8.6 - 67.2 - 175.0) / 3
#         = 2 * (-250.8) / 3 = -167.2

# grad[1] = 2 * (2*(-8.6) + 5*(-16.8) + 8*(-25.0)) / 3
#         = 2 * (-17.2 - 84.0 - 200.0) / 3
#         = 2 * (-301.2) / 3 = -200.8

# grad[2] = 2 * (3*(-8.6) + 6*(-16.8) + 9*(-25.0)) / 3
#         = 2 * (-25.8 - 100.8 - 225.0) / 3
#         = 2 * (-351.6) / 3 = -234.4

gradients = [[-167.2], [-200.8], [-234.4]]
```

### **VÃ­ dá»¥ sá»‘ minh há»a - GRADIENT SAI (Broadcasting Error):**

#### **Khi y khÃ´ng Ä‘Æ°á»£c reshape Ä‘Ãºng:**
```python
# SAI: y lÃ  1D array
y_flat = [10, 20, 30]  # Shape: (3,)
y_hat = [[1.4], [3.2], [5.0]]  # Shape: (3, 1)

# Broadcasting sáº½ gÃ¢y lá»—i
error = y_hat - y_flat  # (3, 1) - (3,) â†’ Broadcasting issues! âŒ
```

#### **Káº¿t quáº£ gradient sai:**
```python
# Gradient sáº½ bá»‹ sai do broadcasting
gradients_wrong = 2 * X.T.dot(error_wrong) / 3
# Káº¿t quáº£: [[-83.6], [-100.4], [-117.2]]  # SAI! âŒ

# So vá»›i gradient Ä‘Ãºng:
gradients_correct = [[-167.2], [-200.8], [-234.4]]  # ÄÃšNG! âœ…
```

---

## ğŸ“Š Báº£ng So SÃ¡nh Trá»±c Quan

| Aspect | **Row-wise Gradient** | **Column-wise Gradient** |
|--------|----------------------|-------------------------|
| **Shape x_i** | (3,) - 1D array | (3, 3) - 2D matrix |
| **Shape y_i** | (1,) - scalar | (3, 1) - 2D array |
| **Shape gradient** | (3,) - 1D array | (3, 1) - 2D array |
| **Cáº§n reshape y?** | âŒ KhÃ´ng cáº§n | âœ… Cáº§n reshape (3, 1) |
| **Nguy cÆ¡ gradient sai** | ğŸŸ¢ Tháº¥p | ğŸŸ¡ Cao (broadcasting) |
| **Performance** | ğŸŸ¡ Cháº­m (loop) | ğŸŸ¢ Nhanh (vectorized) |
| **Memory usage** | ğŸŸ¢ Tháº¥p | ğŸŸ¡ Cao |
| **Debug difficulty** | ğŸŸ¢ Dá»… | ğŸŸ¡ KhÃ³ |
| **Code complexity** | ğŸŸ¢ ÄÆ¡n giáº£n | ğŸŸ¡ Phá»©c táº¡p |

---

## ğŸ”„ Flow Diagram - Gradient Calculation

```
Row-wise Gradient Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sample 1â”‚â”€â”€â”€â–¶â”‚ Sample 2â”‚â”€â”€â”€â–¶â”‚ Sample 3â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚              â”‚
     â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ grad_1  â”‚    â”‚ grad_2  â”‚    â”‚ grad_3  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Total Grad  â”‚
            â”‚ (Average)   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Column-wise Gradient Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sample 1â”‚    â”‚ Sample 2â”‚    â”‚ Sample 3â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     X       â”‚
            â”‚ (3Ã—3 matrix)â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   y_hat     â”‚
            â”‚ (3Ã—1 vector)â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚    error    â”‚
            â”‚ (3Ã—1 vector)â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  gradients  â”‚
            â”‚ (3Ã—1 vector) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Káº¿t Luáº­n

### **Khi nÃ o dÃ¹ng Row-wise:**
- âœ… **Há»c táº­p**: Dá»… hiá»ƒu vÃ  debug
- âœ… **Data nhá»**: KhÃ´ng cáº§n performance cao
- âœ… **An toÃ n**: Ãt lá»—i broadcasting

### **Khi nÃ o dÃ¹ng Column-wise:**
- âœ… **Production**: Performance cao
- âœ… **Data lá»›n**: Cáº§n vectorization
- âš ï¸ **Cáº©n tháº­n**: Pháº£i reshape y Ä‘Ãºng shape

### **LÆ°u Ã½ quan trá»ng:**
```python
# ÄÃšNG: Column-wise gradient
y = y.reshape(-1, 1)  # Äáº£m báº£o shape (m, 1)
error = y_hat - y     # (m, 1) - (m, 1) = (m, 1) âœ…

# SAI: Broadcasting error
y_flat = y.flatten()  # Shape (m,)
error = y_hat - y_flat  # (m, 1) - (m,) â†’ Broadcasting issues! âŒ
```

**Nhá»›**: Shape consistency lÃ  chÃ¬a khÃ³a Ä‘á»ƒ trÃ¡nh gradient sai! ğŸ¯
