---
title: "M5-W1: Bridge to Linear Regression - Tá»« Gradient Descent Ä‘áº¿n Deep Learning"
pubDatetime: 2025-01-27T10:00:00Z
featured: false
description: "KhÃ¡m phÃ¡ Linear Regression nhÆ° cáº§u ná»‘i giá»¯a Machine Learning cÆ¡ báº£n vÃ  Deep Learning, vá»›i focus vÃ o Gradient Descent vÃ  optimization"
tags: ["machine-learning", "linear-regression", "gradient-descent", "optimization", "deep-learning"]
---
# Bridge to Linear Regression - Tá»« Gradient Descent Ä‘áº¿n Deep Learning

Linear Regression khÃ´ng chá»‰ lÃ  thuáº­t toÃ¡n cÆ¡ báº£n mÃ  cÃ²n lÃ  **cáº§u ná»‘i quan trá»ng** giá»¯a Machine Learning truyá»n thá»‘ng vÃ  Deep Learning hiá»‡n Ä‘áº¡i. Trong bÃ i nÃ y, chÃºng ta sáº½ khÃ¡m phÃ¡ cÃ¡ch Linear Regression hoáº¡t Ä‘á»™ng thÃ´ng qua **Gradient Descent** vÃ  cÃ¡ch nÃ³ chuáº©n bá»‹ ná»n táº£ng cho cÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p hÆ¡n.

---

## 1. HÃ m Loss Function - Square Function
 
![Image](https://github.com/user-attachments/assets/1e0fcb16-5d80-4634-9fbe-a3c90c5678ab)

### **Má»¥c tiÃªu cá»§a Linear Regression:**
TÃ¬m Ä‘Æ°á»ng tháº³ng tá»‘t nháº¥t Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ y tá»« x:

$$y = wx + b$$

Trong Ä‘Ã³:
- **w**: weight (há»‡ sá»‘ gÃ³c)
- **b**: bias (há»‡ sá»‘ cháº·n)

### **Loss Function - Mean Squared Error (MSE):**
$$L(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(h_w(x^{(i)}) - y^{(i)})^2$$

$$L(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(wx^{(i)} + b - y^{(i)})^2$$

**Äáº·c Ä‘iá»ƒm cá»§a Square Function:**
- **Convex function**: CÃ³ má»™t Ä‘iá»ƒm minimum duy nháº¥t
- **Smooth**: CÃ³ thá»ƒ tÃ­nh Ä‘áº¡o hÃ m táº¡i má»i Ä‘iá»ƒm
- **Quadratic**: TÄƒng nhanh khi xa khá»i minimum

---

## 2. Gradient Descent vá»›i Learning Rate Nhá»

![Image](https://github.com/user-attachments/assets/866b520e-69c9-47e1-b5fd-6d7df52a8fef)

### **Gradient Descent Algorithm:**
$$w := w - \alpha \frac{\partial L}{\partial w}$$
$$b := b - \alpha \frac{\partial L}{\partial b}$$

### **Vá»›i Learning Rate Nhá» (Î± = 0.01):**

**Æ¯u Ä‘iá»ƒm:**
- **á»”n Ä‘á»‹nh**: KhÃ´ng bá»‹ overshoot
- **ChÃ­nh xÃ¡c**: Tiáº¿n dáº§n Ä‘áº¿n minimum
- **An toÃ n**: KhÃ´ng bá»‹ diverge

**NhÆ°á»£c Ä‘iá»ƒm:**
- **Cháº­m**: Cáº§n nhiá»u iterations
- **CÃ³ thá»ƒ bá»‹ stuck**: á» local minimum (náº¿u cÃ³)

**Quan sÃ¡t tá»« hÃ¬nh:**
- CÃ¡c bÆ°á»›c nhá», Ä‘á»u Ä‘áº·n
- Tiáº¿n dáº§n vá» phÃ­a minimum
- Convergence cháº­m nhÆ°ng á»•n Ä‘á»‹nh

---

## 3. Gradient Descent vá»›i Learning Rate Lá»›n

![Image](https://github.com/user-attachments/assets/7a600214-c8c1-4472-b550-a757188be8e9)

### **Vá»›i Learning Rate Lá»›n (Î± = 0.1):**

**Æ¯u Ä‘iá»ƒm:**
- **Nhanh**: Tiáº¿n nhanh vá» phÃ­a minimum
- **Hiá»‡u quáº£**: Ãt iterations hÆ¡n

**NhÆ°á»£c Ä‘iá»ƒm:**
- **Oscillation**: Dao Ä‘á»™ng quanh minimum
- **KhÃ³ converge**: CÃ³ thá»ƒ khÃ´ng bao giá» Ä‘áº¡t Ä‘Æ°á»£c chÃ­nh xÃ¡c minimum

**Quan sÃ¡t tá»« hÃ¬nh:**
- CÃ¡c bÆ°á»›c lá»›n hÆ¡n
- CÃ³ hiá»‡n tÆ°á»£ng "zigzag" quanh minimum
- Convergence nhanh nhÆ°ng khÃ´ng á»•n Ä‘á»‹nh

---

## 4. Gradient Descent vá»›i Learning Rate QuÃ¡ Lá»›n

![Image](https://github.com/user-attachments/assets/d4e2a51f-bdc0-4445-9924-c315fc07ed38)

### **Vá»›i Learning Rate QuÃ¡ Lá»›n (Î± = 0.5):**

**Váº¥n Ä‘á» nghiÃªm trá»ng:**
- **Divergence**: KhÃ´ng thá»ƒ converge
- **Overshooting**: VÆ°á»£t quÃ¡ minimum
- **Instability**: Máº¥t á»•n Ä‘á»‹nh hoÃ n toÃ n

**Quan sÃ¡t tá»« hÃ¬nh:**
- CÃ¡c bÆ°á»›c ráº¥t lá»›n
- Bouncing qua láº¡i minimum
- Loss function tÄƒng lÃªn thay vÃ¬ giáº£m
- **KhÃ´ng thá»ƒ tÃ¬m Ä‘Æ°á»£c solution**

---

## 5. Partial Derivatives - Fix w, Differentiate b

![Image](https://github.com/user-attachments/assets/4c935f58-a4e7-42c6-867f-01667efc8d5b)

### **Mathematical Foundation:**

Khi **fix w**, ta chá»‰ optimize b:

$$\frac{\partial L}{\partial b} = \frac{1}{m}\sum_{i=1}^{m}(wx^{(i)} + b - y^{(i)})$$

$$\frac{\partial L}{\partial b} = \frac{1}{m}\sum_{i=1}^{m}(h_w(x^{(i)}) - y^{(i)})$$

**Ã nghÄ©a:**
- **Gradient theo b**: Äo Ä‘á»™ lá»‡ch trung bÃ¬nh cá»§a predictions
- **Direction**: HÆ°á»›ng Ä‘á»ƒ Ä‘iá»u chá»‰nh bias
- **Magnitude**: Äá»™ lá»›n cá»§a adjustment cáº§n thiáº¿t

---

## 6. Partial Derivatives - Fix b, Differentiate w

![Image](https://github.com/user-attachments/assets/f3b7c8f8-a804-433e-a175-a95541c3e544)

### **Khi fix b, optimize w:**

$$\frac{\partial L}{\partial w} = \frac{1}{m}\sum_{i=1}^{m}(wx^{(i)} + b - y^{(i)}) \cdot x^{(i)}$$

$$\frac{\partial L}{\partial w} = \frac{1}{m}\sum_{i=1}^{m}(h_w(x^{(i)}) - y^{(i)}) \cdot x^{(i)}$$

**Ã nghÄ©a:**
- **Gradient theo w**: Äo Ä‘á»™ nháº¡y cáº£m cá»§a loss vá»›i weight
- **x^{(i)} factor**: Features cÃ³ giÃ¡ trá»‹ lá»›n sáº½ áº£nh hÆ°á»Ÿng nhiá»u hÆ¡n
- **Weighted error**: Lá»—i Ä‘Æ°á»£c weighted bá»Ÿi feature values

---

## 7. Simultaneous Optimization - Cáº£ w vÃ  b

![Image](https://github.com/user-attachments/assets/1e9e7204-7855-4239-8b67-4e1466cf628e)

### **Complete Gradient Descent:**

**Update Rules:**
$$w := w - \alpha \frac{\partial L}{\partial w}$$
$$b := b - \alpha \frac{\partial L}{\partial b}$$

**Vectorized Form:**
$$\theta := \theta - \alpha \nabla_\theta L(\theta)$$

Trong Ä‘Ã³ $\theta = [w, b]^T$

### **Táº¡i sao Ä‘Ã¢y lÃ  "Bridge to Deep Learning"?**

1. **Gradient Descent**: Ná»n táº£ng cá»§a táº¥t cáº£ neural networks
2. **Backpropagation**: Má»Ÿ rá»™ng cá»§a gradient descent cho multiple layers
3. **Optimization**: CÃ¹ng cÃ¡c ká»¹ thuáº­t (Adam, RMSprop, etc.)
4. **Loss Functions**: MSE â†’ Cross-entropy, etc.

---

## 8. Káº¿t Luáº­n - Tá»« Linear Regression Ä‘áº¿n Deep Learning

### **Linear Regression dáº¡y chÃºng ta:**

1. **Loss Function Design**: CÃ¡ch thiáº¿t káº¿ objective function
2. **Gradient Computation**: TÃ­nh toÃ¡n derivatives
3. **Optimization**: TÃ¬m minimum cá»§a complex functions
4. **Learning Rate**: Táº§m quan trá»ng cá»§a hyperparameter tuning

### **Chuyá»ƒn tiáº¿p sang Deep Learning:**

- **Neural Networks**: Multiple linear layers + activation functions
- **Backpropagation**: Chain rule cho multiple layers
- **Advanced Optimizers**: Adam, RMSprop, AdaGrad
- **Regularization**: Dropout, BatchNorm, L1/L2

### **Key Takeaways:**

âœ… **Linear Regression** lÃ  foundation cá»§a Deep Learning
âœ… **Gradient Descent** lÃ  core algorithm
âœ… **Learning Rate** lÃ  hyperparameter quan trá»ng nháº¥t
âœ… **Optimization** lÃ  skill cáº§n thiáº¿t cho AI/ML

---

## 9. Next Steps

Sau khi hiá»ƒu rÃµ Linear Regression vÃ  Gradient Descent, chÃºng ta sáº½ tiáº¿n tá»›i:

1. **Multiple Linear Regression**
2. **Logistic Regression** (Classification)
3. **Neural Networks** (Single Layer)
4. **Deep Neural Networks** (Multiple Layers)
5. **Advanced Architectures** (CNN, RNN, Transformer)

**Linear Regression** khÃ´ng chá»‰ lÃ  thuáº­t toÃ¡n Ä‘Æ¡n giáº£n - nÃ³ lÃ  **cáº§u ná»‘i vá»¯ng cháº¯c** dáº«n chÃºng ta vÃ o tháº¿ giá»›i phá»©c táº¡p vÃ  thÃº vá»‹ cá»§a Deep Learning! ğŸš€
