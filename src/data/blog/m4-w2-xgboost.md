
---
title: "Behind The Scene c·ªßa XGBoost"
pubDatetime: 2025-01-16T10:00:00Z
featured: false
description: "T√¨m hi·ªÉu chi ti·∫øt v·ªÅ thu·∫≠t to√°n XGBoost, t·ª´ h√†m m·ª•c ti√™u ƒë·∫øn qu√° tr√¨nh x√¢y d·ª±ng c√¢y quy·∫øt ƒë·ªãnh"
tags: ["machine-learning", "xgboost", "gradient-boosting", "algorithm"]
---

# Behind The Scene c·ªßa XGBoost

> **üìö Repo tham kh·∫£o:** [https://github.com/tandat8896/ml-from-the-scartch/tree/master/xgboost](https://github.com/tandat8896/ml-from-the-scartch/tree/master/xgboost)

## H√†m m·ª•c ti√™u (Objective) trong XGBoost
* **H√†m m·∫•t m√°t cho Regression:**

$$
\sum_{i=1}^{N} \mathcal{L}(y_i, \bar{y}_i) \quad \text{Where} \quad \mathcal{L}(y_i, \bar{y}_i) = \frac{1}{2}(y_i - \bar{y}_i)^2
$$

* **H√†m m·∫•t m√°t cho Classification:**

$$
\sum_{i=1}^{N} \mathcal{L}(y_i, \bar{y}_i) \quad \text{Where} \quad \mathcal{L}(y_i, \bar{y}_i) = - \left[ y_i\log(\bar{y}_i) + (1-y_i)\log(1-\bar{y}_i) \right]
$$

---

$$
\mathrm{Objective}^{(t)} = \sum_{i=1}^{N} \mathcal{L}(y_i, \hat{y}_i^{(t)}) + \sum_{j=1}^{t} \Omega(f_j)
$$





Trong ƒë√≥:

- **Loss function**:  
  $\mathcal{L}(y_i, \hat{y}_i^{(t)})$ ƒëo s·ª± kh√°c bi·ªát gi·ªØa gi√° tr·ªã th·ª±c $y_i$ v√† d·ª± ƒëo√°n $\hat{y}_i^{(t)}$.

- **Penalty term (th√†nh ph·∫ßn ph·∫°t)**:  
  $\Omega(f_j)$ l√† ph·∫ßn ph·∫°t, nh·∫±m gi·ªõi h·∫°n ƒë·ªô ph·ª©c t·∫°p c·ªßa c√¢y ƒë·ªÉ tr√°nh overfitting.

### Bi·ªÉu th·ª©c penalty

$$
\Omega(f) = \gamma T + \frac{\lambda}{2} \sum_{j=1}^{T} w_j^2
$$

Trong ƒë√≥:
- $T$: s·ªë l√° c·ªßa c√¢y.
- $w_j$: tr·ªçng s·ªë c·ªßa l√° $j$.
- $\gamma, \lambda$: si√™u tham s·ªë ƒëi·ªÅu ch·ªânh ƒë·ªô ph·ª©c t·∫°p.

* v√¨ XGBoost c√≥ th·ªÉ th·ª±c hi·ªán t·ªâa c√¢y (pruning) ngay c·∫£ khi $\gamma = 0$.
* T·ªâa c√¢y ƒë∆∞·ª£c th·ª±c hi·ªán sau khi c√¢y ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng ho√†n ch·ªânh. Do ƒë√≥, qu√° tr√¨nh n√†y kh√¥ng ƒë√≥ng vai tr√≤ trong vi·ªác x√°c ƒë·ªãnh c√°c gi√° tr·ªã ƒë·∫ßu ra t·ªëi ∆∞u ban ƒë·∫ßu c·ªßa c√°c l√°.


**XGBoost x√¢y d·ª±ng c√¢y m·ªõi d·ª±a tr√™n h√†m m·∫•t m√°t.**

$$
\sum_{i=1}^{n} \mathcal{L}(y_i, \bar{y}_i^0 + P) + \frac{1}{2}\lambda P^2
$$

* **M·ª•c ti√™u:** t√¨m gi√° tr·ªã d·ª± ƒëo√°n cho m·ªói leaf (P) c·ªßa c√¢y m·ªõi nh·∫±m minimize h√†m loss.
* **Rigde Regression Regularization term**

*Gi√° tr·ªã P c·∫ßn t√¨m l√† gi√° tr·ªã ·ª©ng v·ªõi ƒë·∫°o h√†m c·ªßa loss theo P b·∫±ng 0.*

v√¨ ph·∫ßn n√†y $\sum_{i=1}^{n} \mathcal{L}(y_i, \bar{y}_i^0 + P)$ r·∫•t kh√≥ ƒë·ªÉ ƒë·∫°o h√†m (H√†m loss c√≥ th·ªÉ kh√¥ng m∆∞·ª£t (non-smooth) v√† ph·ª©c t·∫°p, Kh√¥ng c√≥ c√¥ng th·ª©c ƒë√≥ng (closed-form) t·ªïng qu√°t, H√†m c√≥ th·ªÉ nhi·ªÅu c·ª±c tr·ªã)

**V√¨ v·∫≠y ch√∫ng ta s·∫Ω s·∫•p xƒ© n√≥ b·∫±ng Taylor Second Approximate( V√¨ c√¥ng th·ª©c n√†y cho ch√∫ng ta bi·∫øt x·∫•p xƒ© h√†m quanh 1 ƒëi·ªÉm).**

**T√¨m gi√° tr·ªã P ƒë·ªÉ t·ªëi ∆∞u h√≥a, ta x·∫•p x·ªâ h√†m loss b·∫±ng Taylor Approximation b·∫≠c hai:**

$$
\mathcal{L}(y_i, \bar{y}_i^0 + P) \approx \mathcal{L}(y_i, \bar{y}_i^0) + \left[ \frac{d\mathcal{L}}{d\bar{y}_i} \right] P + \frac{1}{2}\left[ \frac{d^2\mathcal{L}}{d\bar{y}_i^2} \right] P^2
$$

**S·ª≠ d·ª•ng k√Ω hi·ªáu g (gradient) v√† h (hessian) cho ƒë·∫°o h√†m:**

* $g$: ƒë·∫°o h√†m b·∫≠c nh·∫•t c·ªßa h√†m loss.
* $h$: ƒë·∫°o h√†m b·∫≠c hai c·ªßa h√†m loss.

$$
\mathcal{L}(y_i, \bar{y}_i^0 + P) \approx \mathcal{L}(y_i, \bar{y}_i^0) + gP + \frac{1}{2}hP^2
$$

**Thay v√†o h√†m m·∫•t m√°t c·ªßa XGBoost ta c√≥ :**

$$
\sum_{i=1}^{n} \mathcal{L}(y_i, \bar{y}_i^0) + \sum_{i=1}^{n} \left( g_i P + \frac{1}{2}h_i P^2 \right) + \frac{1}{2}\lambda P^2
$$

**T√¨m gi√° tr·ªã P sao cho ƒë·∫°o h√†m c·ªßa bi·ªÉu th·ª©c theo P b·∫±ng 0:**

$$
\frac{d}{dP} \left[ \sum_{i=1}^{n} (g_i + h_i P) + \lambda P \right] = 0
$$

**Gi·∫£i ph∆∞∆°ng tr√¨nh ƒë·ªÉ t√¨m gi√° tr·ªã t·ªëi ∆∞u c·ªßa P:**

$$
\frac{d}{dP} \left[ (g_1 + g_2 + \dots + g_n)P + \frac{1}{2}(h_1 + h_2 + \dots + h_n + \lambda)P^2 \right] = 0
$$

**Sau khi ƒë·∫°o h√†m, ta c√≥:**

$$
(g_1 + g_2 + \dots + g_n) + (h_1 + h_2 + \dots + h_n + \lambda)P = 0
$$

* **1:** $g_i = \frac{d}{d\bar{y}_i}\frac{1}{2}(y_i - \bar{y}_i)^2 = (y_i - \bar{y}_i)$
* **2:** $h_i = \frac{d^2}{d\bar{y}_i^2}\frac{1}{2}(y_i - \bar{y}_i)^2 = 1$

**Gi√° tr·ªã P t·ªëi ∆∞u (Output value of the leaf or terminal node):**

$$
P = -\frac{g_1 + g_2 + \dots + g_n}{h_1 + h_2 + \dots + h_n + \lambda} = -\frac{(y_1 - \bar{y}_1) + (y_2 - \bar{y}_2) + \dots + (y_n - \bar{y}_n)}{1 + 1 + \dots + 1 + \lambda}
$$

$$P = \frac{\sum_{i=1}^{n} (y_i - \bar{y}_i)}{n + \lambda}$$



### **Classification**

**H√†m m·∫•t m√°t (Loss Function):**

$$\mathcal{L}(Y_i, \bar{Y_i}) = -Y_i\log(\bar{Y_i}) + (1-Y_i)\log(1-\bar{Y_i})$$

**Chuy·ªÉn ƒë·ªïi x√°c su·∫•t (probability) th√†nh log(odds):**

$$\mathcal{L}(Y_i, \log(odds)) = -Y_i\log(odds) + \log(1 + e^{\log(odds)})$$

**ƒê·∫°o h√†m b·∫≠c nh·∫•t ($g_i$):**

$$
g_i = \frac{d}{d\log(odds)}\mathcal{L}(Y_i, \log(odds)) = -Y_i + \frac{e^{\log(odds)}}{1 + e^{\log(odds)}} = -(Y_i - \bar{Y_i})
$$

**ƒê·∫°o h√†m b·∫≠c hai ($h_i$):**

$$
h_i = \frac{d^2}{d\log(odds)^2}\mathcal{L}(Y_i, \log(odds)) = \frac{e^{\log(odds)}}{1 + e^{\log(odds)}} \times \frac{1}{1 + e^{\log(odds)}} = \bar{Y_i}(1-\bar{Y_i})
$$

**Gi√° tr·ªã P t·ªëi ∆∞u (Output value of the leaf or terminal node):**

$$
P = -\frac{-(g_1 + g_2 + \dots + g_n)}{h_1 + h_2 + \dots + h_n + \lambda} = \frac{\text{sum of residual}}{\text{sum of } \bar{Y_i}(1 - \bar{Y_i}) + \lambda} = \frac{\sum(\text{Residual})}{\sum\bar{Y_i}(1-\bar{Y_i}) + \lambda}
$$

**M·ªëi quan h·ªá gi·ªØa Loss Function v√† P**

* **Kh√≥ khƒÉn:** R·∫•t kh√≥ ƒë·ªÉ t√¨m gi√° tr·ªã P t·ªëi ∆∞u (optimization) tr·ª±c ti·∫øp t·ª´ h√†m Loss ban ƒë·∫ßu.
* **Gi·∫£i ph√°p:** S·ª≠ d·ª•ng x·∫•p x·ªâ b·∫≠c hai (Second Order Taylor Approximation) ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a h√†m Loss.

$$
\sum_{i=1}^{n} \mathcal{L}(y_i, \bar{y}_i^0 + P) + \frac{1}{2}\lambda P^2
$$

* **1.** H√†m Loss ban ƒë·∫ßu.
* **2.** H√†m Loss sau khi x·∫•p x·ªâ b·∫±ng Taylor.
$$
(g_1 + g_2 + \dots + g_n)P + \frac{1}{2}(h_1 + h_2 + \dots + h_n + \lambda)P^2
$$
* **ƒêi·ªÉm chung:** C·∫£ (1) v√† (2) ƒë·ªÅu c√≥ c√πng ƒëi·ªÉm t·ªëi ∆∞u (optimization point) P.
* **C√¥ng th·ª©c t√¨m P:**
$$
P = -\frac{-(g_1 + g_2 + \dots + g_n)}{h_1 + h_2 + \dots + h_n + \lambda}
$$


### **C√¥ng th·ª©c t√≠nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng (Similarity Score)**

* **M·ª•c ƒë√≠ch:** T√¨m ƒëi·ªÉm t·ªëi ∆∞u c·ªßa h√†m m·ª•c ti√™u b·∫±ng c√°ch x·∫•p x·ªâ h√†m loss ban ƒë·∫ßu.

H√†m loss ban ƒë·∫ßu:

$$
\sum_{i=1}^{n} \mathcal{L}(y_i, \bar{y}_i^0 + P) + \frac{1}{2}\lambda P^2
$$

H√†m loss sau khi x·∫•p x·ªâ Taylor b·∫≠c hai:

$$
\sum_{i=1}^{n} (g_i P + \frac{1}{2}h_i P^2) + \frac{1}{2}\lambda P^2
$$

* **L∆∞u √Ω:** C·∫£ hai h√†m tr√™n ƒë·ªÅu c√≥ c√πng m·ªôt ƒëi·ªÉm t·ªëi ∆∞u (P).

Khi ƒë√≥, h√†m m·ª•c ti√™u ƒë∆∞·ª£c vi·∫øt l·∫°i:


* **H√†m m·∫•t m√°t ban ƒë·∫ßu:**

$$
\sum_{i=1}^{n} \mathcal{L}(y_i, \bar{y}_i^0 + P) + \frac{1}{2}\lambda P^2
$$

* **X·∫•p x·ªâ Taylor b·∫≠c hai:**

$$
\mathcal{L}(y_i, \bar{y}_i^0 + P) \approx \mathcal{L}(y_i, \bar{y}_i^0) + \left[ \frac{d\mathcal{L}}{d\bar{y}_i} \right] P + \frac{1}{2}\left[ \frac{d^2\mathcal{L}}{d\bar{y}_i^2} \right] P^2
$$

$$
\mathcal{L}(y_i, \bar{y}_i^0 + P) \approx \mathcal{L}(y_i, \bar{y}_i^0) + gP + \frac{1}{2}hP^2
$$

* **Thay v√¨ t√¨m c·ª±c ti·ªÉu cho h√†m loss th√¨ ta s·∫Ω nh√¢n -1 ƒë·ªÉ ƒë·∫£o c·ª±c tr·ªã l·∫°i t√¨m c·ª±c ƒë·∫°i cho Similarity Score m√† t·∫°i ƒë√≥ ƒëi·ªÉm c·ª±c ƒë·∫°i s·∫Ω tr√πng v·ªõi c·ª±c ti·ªÉu c·ªßa h√†m loss (T∆∞∆°ng t·ª± √Ω nghƒ©a c·ªßa Infomation Gain trong Decision Tree Regression)**

$$
\text{Similarity Score} =-1\left(\sum_{i=1}^{n} g_i\right)P + \frac{1}{2}\left(\sum_{i=1}^{n} h_i + \lambda\right)P^2
$$

* **Gi√° tr·ªã P t·ªëi ∆∞u:**

$$
P = -\frac{\left(g_1 + g_2 + \dots + g_n\right)}{h_1 + h_2 + \dots + h_n + \lambda}
$$

**Thay P v√†o c√¥ng th·ª©c SimilarityScore**


$$
\text{Similarity Score} = \frac{1}{2} \frac{(g_1 + g_2 + \dots + g_n)^2}{(h_1 + h_2 + \dots + h_n + \lambda)}
$$


**V√≠ D·ª• T√≠nh Tay**

* **Loss Function Regression:**

$$
\sum_{i=1}^{3} \mathcal{L}(y_i, \bar{y}_i) \text{ Where } \mathcal{L}(y_i, \bar{y}_i) = \frac{1}{2}(y_i - \bar{y}_i)^2
$$

* **Loss Function Classification:**

$$
\sum_{i=1}^{3} \mathcal{L}(y_i, \bar{y}_i) \text{ Where } \mathcal{L}(y_i, \bar{y}_i) = - \left[ y_i\log(\bar{y}_i) + (1-y_i)\log(1-\bar{y}_i) \right]
$$

---

### **Step 1: Initialization**

| Age (X) | Chol (y) |
|:---:|:---:|
| 29 | 204 |
| 48 | 234 |
| 39 | 203 |
| 67 | 269 |
| 45 | 250 |
| 59 | 260 |


* **F0(x) l√† gi√° tr·ªã t·ªëi ∆∞u b·∫±ng c√°ch t√¨m gi√° tr·ªã nh·ªè nh·∫•t c·ªßa h√†m Loss:**
    $$
    F_0(x) = \arg \min_{\theta} \sum_{i=1}^{N} l(y_i, \theta)
    $$

$F_0 = \frac{1}{N} \sum_{i=1}^{N} y_i$

$F_0 = \frac{204 + 234 + 203 + 269 + 250 + 260}{6}$

$F_0 = 236.67$



#### **B∆∞·ªõc 2A: X√¢y d·ª±ng c√¢y ‚Äì S·∫Øp x·∫øp**

S·∫Øp x·∫øp c√°c m·∫´u trong node theo gi√° tr·ªã c·ªßa feature:

$$
X_{sorted} = [29, 39, 45, 48, 59, 67]
$$

| Age (X) | Chol (y) |
| :---: | :---: |
| 29 | 204 |
| 48 | 234 |
| 39 | 203 |
| 67 | 269 |
| 45 | 250 |
| 59 | 260 |
| $F_0$ = 236.67 | |

| Age (X) | Chol (y) |
| :---: | :---: |
| 29 | 204 |
| 39 | 203 |
| 45 | 250 |
| 48 | 234 |
| 59 | 260 |
| 67 | 269 |
| $F_0$ = 236.67 | |

---
### **ƒê·∫°o h√†m c·ªßa Loss Function Regression (MSE)**

* **H√†m m·∫•t m√°t (Loss Function):**
    $$
    l(\hat{y}_i, y_i) = \frac{1}{2}(\hat{y}_i - y_i)^2
    $$

* **ƒê·∫°o h√†m b·∫≠c nh·∫•t ($g_i$):**
    $$
    g_i = \frac{\partial l}{\partial \hat{y}_i} = \frac{\partial}{\partial \hat{y}_i} \left(\frac{1}{2}(\hat{y}_i - y_i)^2 \right) = \frac{1}{2} \cdot 2(\hat{y}_i - y_i) \cdot \frac{\partial (\hat{y}_i - y_i)}{\partial \hat{y}_i} = \hat{y}_i - y_i
    $$

* **ƒê·∫°o h√†m b·∫≠c hai ($h_i$):**
    $$
    h_i = \frac{\partial^2 l}{\partial \hat{y}_i^2} = \frac{\partial}{\partial \hat{y}_i} (\hat{y}_i - y_i) = 1
    $$

---
### **T√≠nh Gradients ($g_i$) v√† Hessians ($h_i$)**

* **Gi√° tr·ªã ban ƒë·∫ßu:** $F_0 = 236.67$

| Age (X) | Chol (y) | Gradients ($g_i = F_0 - y_i$) | Hessians ($h_i$) |
| :---: | :---: | :---: | :---: |
| 29 | 204 | $236.67 - 204 = 32.67$ | 1 |
| 39 | 203 | $236.67 - 203 = 33.67$ | 1 |
| 45 | 250 | $236.67 - 250 = -13.33$ | 1 |
| 48 | 234 | $236.67 - 234 = 2.67$ | 1 |
| 59 | 260 | $236.67 - 260 = -23.33$ | 1 |
| 67 | 269 | $236.67 - 269 = -32.33$ | 1 |

### **4. X√¢y d·ª±ng c√¢y v√† c√°c ng∆∞·ª°ng**


  ### **B∆∞·ªõc 2B: X√¢y d·ª±ng c√¢y - T√¨m ng∆∞·ª°ng ·ª©ng c·ª≠ vi√™n**


| Thresh |              
| :---: |
| 34.0 |
| 42.0 |
| 46.5 |
| 53.5 |
| 63.0 |

### **B∆∞·ªõc 2B: X√¢y d·ª±ng c√¢y - Gradients & Hessians cho c√°c node con**

| Age (X) | Chol (y) | Gradients | Hessians |
| :---: | :---: | :---: | :---: |
| 29 | 204 | 32.67 | 1 |
| 39 | 203 | 33.67 | 1 |
| 45 | 250 | -13.33 | 1 |
| 48 | 234 | 2.67 | 1 |
| 59 | 260 | -23.33 | 1 |
| 67 | 269 | -32.33 | 1 |

| Thresh |
| :---: |
| 34.0 |
| 42.0 |
| 46.5 |
| 53.5 |
| 63.0 |

---


### **B∆∞·ªõc 2B: X√¢y d·ª±ng c√¢y - Gradients & Hessians cho c√°c node con**

| Age (X) | Chol (y) | Gradients | Hessians |
| :---: | :---: | :---: | :---: |
| 29 | 204 | 32.67 | 1 |
| 39 | 203 | 33.67 | 1 |
| 45 | 250 | -13.33 | 1 |
| 48 | 234 | 2.67 | 1 |
| 59 | 260 | -23.33 | 1 |
| 67 | 269 | -32.33 | 1 |

* **Ng∆∞·ª°ng ph√¢n chia:** Thresh = 34.0
* **Node con b√™n tr√°i (Age <= 34.0):**

| Left Id | $G_L$ | $H_L$ |
| :---: | :---: | :---: |
| 29 | 32.67 | 1 |

* **Node con b√™n ph·∫£i (Age > 34.0):**

| Right Id | $G_R$ | $H_R$ |
| :---: | :---: | :---: |
| 39 | 33.67 | 1 |
| 45 | -13.33 | 1 |
| 48 | 2.67 | 1 |
| 59 | -23.33 | 1 |
| 67 | -32.33 | 1 |

* **T·ªïng h·ª£p cho c√°c node:**

| Node | $\sum G$ | $\sum H$ |
| :---: | :---: | :---: |
| **B√™n tr√°i** | 32.67 | 1 |
| **B√™n ph·∫£i** | $33.67 - 13.33 + 2.67 - 23.33 - 32.33 = -32.65$ | $1+1+1+1+1 = 5$ |

---

### **Ti·∫øp t·ª•c ch·ªçn ng∆∞·ª°ng**

| Age (X) | Chol (y) | Gradients | Hessians |
| :---: | :---: | :---: | :---: |
| 29 | 204 | 32.67 | 1 |
| 39 | 203 | 33.67 | 1 |
| 45 | 250 | -13.33 | 1 |
| 48 | 234 | 2.67 | 1 |
| 59 | 260 | -23.33 | 1 |
| 67 | 269 | -32.33 | 1 |
| $F_0 = 236.67$ | | | |

| Thresh |
| :---: |
| 34.0 |
| 42.0 |
| 46.5 |
| 53.5 |
| 63.0 |

* **Left Node (Age <= 34.0):**

| Left Id | $G_L$ | $H_L$ |
| :---: | :---: | :---: |
| 29 | 32.67 | 1 |
| 39 | 33.67 | 1 |

* **Right Node (Age > 34.0):**

| Right Id | $G_R$ | $H_R$ |
| :---: | :---: | :---: |
| 45 | -13.33 | 1 |
| 48 | 2.67 | 1 |
| 59 | -23.33 | 1 |
| 67 | -32.33 | 1 |

* **T·ªïng h·ª£p cho c√°c node:**

| Node | $\sum G$ | $\sum H$ |
| :---: | :---: | :---: |
| **B√™n tr√°i** | $32.67 + 33.67 = 66.34$ | $1 + 1 = 2$ |
| **B√™n ph·∫£i** | $-13.33 + 2.67 - 23.33 - 32.33 = -66.32$ | $1 + 1 + 1 + 1 = 4$ |

* **Formulas:**

$$G_L = \sum_{i \in I_L} g_i \quad H_L = \sum_{i \in I_L} h_i$$
$$G_R = \sum_{i \in I_R} g_i \quad H_R = \sum_{i \in I_R} h_i$$

| Age (X) | Chol (y) | Gradients | Hessians |
| :---: | :---: | :---: | :---: |
| 29 | 204 | 32.67 | 1 |
| 39 | 203 | 33.67 | 1 |
| 45 | 250 | -13.33 | 1 |
| 48 | 234 | 2.67 | 1 |
| 59 | 260 | -23.33 | 1 |
| 67 | 269 | -32.33 | 1 |
| $F_0 = 236.67$ | | | |

| Thresh |
| :---: |
| 34.0 |
| 42.0 |
| 46.5 |
| 53.5 |
| 63.0 |

* **Left Node (Age <= 46.5):**

| Left Id | $G_L$ | $H_L$ |
| :---: | :---: | :---: |
| 29 | 53.01 | 3 |
| 39 | | |
| 45 | | |

* **Right Node (Age > 46.5):**

| Right Id | $G_R$ | $H_R$ |
| :---: | :---: | :---: |
| 48 | -52.99 | 3 |
| 59 | | |
| 67 | | |

* **T·ªïng h·ª£p cho c√°c node:**

| Node | $\sum G$ | $\sum H$ |
| :---: | :---: | :---: |
| **B√™n tr√°i** | $32.67 + 33.67 - 13.33 = 53.01$ | $1 + 1 + 1 = 3$ |
| **B√™n ph·∫£i** | $2.67 - 23.33 - 32.33 = -52.99$ | $1 + 1 + 1 = 3$ |

| Age (X) | Chol (y) | Gradients | Hessians |
| :---: | :---: | :---: | :---: |
| 29 | 204 | 32.67 | 1 |
| 39 | 203 | 33.67 | 1 |
| 45 | 250 | -13.33 | 1 |
| 48 | 234 | 2.67 | 1 |
| 59 | 260 | -23.33 | 1 |
| 67 | 269 | -32.33 | 1 |
| $F_0 = 236.67$ | | | |

| Thresh |
| :---: |
| 34.0 |
| 42.0 |
| 46.5 |
| 53.5 |
| 63.0 |

* **Left Node (Age <= 46.5):**

| Left Id | $G_L$ | $H_L$ |
| :---: | :---: | :---: |
| 29 | 55.68 | 4 |
| 39 | | |
| 45 | | |
| 48 | | |

* **Right Node (Age > 46.5):**

| Right Id | $G_R$ | $H_R$ |
| :---: | :---: | :---: |
| 59 | -55.66 | 2 |
| 67 | | |

* **T·ªïng h·ª£p cho c√°c node:**

| Node | $\sum G$ | $\sum H$ |
| :---: | :---: | :---: |
| **B√™n tr√°i** | $32.67 + 33.67 + (-13.33) + 2.67 = 55.68$ | $1 + 1 + 1 + 1 = 4$ |
| **B√™n ph·∫£i** | $-23.33 + (-32.33) = -55.66$ | $1 + 1 = 2$ |



| Age (X) | Chol (y) | Gradients | Hessians |
| :---: | :---: | :---: | :---: |
| 29 | 204 | 32.67 | 1 |
| 39 | 203 | 33.67 | 1 |
| 45 | 250 | -13.33 | 1 |
| 48 | 234 | 2.67 | 1 |
| 59 | 260 | -23.33 | 1 |
| 67 | 269 | -32.33 | 1 |
| $F_0 = 236.67$ | | | |

| Thresh |
| :---: |
| 34.0 |
| 42.0 |
| 46.5 |
| 53.5 |
| 63.0 |

* **Left Node (Age <= 53.5):**

| Left Id | $G_L$ | $H_L$ |
| :---: | :---: | :---: |
| 29 | 32.35 | 5 |
| 39 | | |
| 45 | | |
| 48 | | |
| 59 | | |

* **Right Node (Age > 53.5):**

| Right Id | $G_R$ | $H_R$ |
| :---: | :---: | :---: |
| 67 | -32.33 | 1 |

* **T·ªïng h·ª£p cho c√°c node:**

| Node | $\sum G$ | $\sum H$ |
| :---: | :---: | :---: |
| **B√™n tr√°i** | $32.67 + 33.67 - 13.33 + 2.67 - 23.33 = 32.35$ | $1 + 1 + 1 + 1 + 1 = 5$ |
| **B√™n ph·∫£i** | $-32.33$ | $1$ |


### **Step 2B: Build Tree ‚Äì Gain**

| Age (X) | Chol (y) | Gradients | Hessians |
| :---: | :---: | :---: | :---: |
| 29 | 204 | 32.67 | 1 |
| 39 | 203 | 33.67 | 1 |
| 45 | 250 | -13.33 | 1 |
| 48 | 234 | 2.67 | 1 |
| 59 | 260 | -23.33 | 1 |
| 67 | 269 | -32.33 | 1 |
| $F_0 = 236.67$ | | | |

| Thresh | $G_L$ | $H_L$ | $G_R$ | $H_R$ | Gain |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 34.0 | 32.67 | 1 | -32.65 | 5 | |
| 42.0 | 66.34 | 2 | -66.32 | 4 | |
| 46.5 | 53.01 | 3 | -52.99 | 3 | |
| 53.5 | 55.68 | 4 | -55.66 | 2 | |
| 63.0 | 32.35 | 5 | -32.33 | 1 | |

$$
\text{Gain} = \frac{1}{2}\left( \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} \right) - \gamma
$$

$$
= \frac{1}{2}\left( \frac{32.67^2}{1+1} + \frac{(-32.65)^2}{5+1} - \frac{(32.67-32.65)^2}{1+5+1} \right) - 0
$$

$$
= \frac{1}{2}\left( \frac{1067.3289}{2} + \frac{1065.0225}{6} - \frac{0.0004}{7} \right) \approx 355.67
$$

* **Hyperparameters**

| | |
| :---: | :---: |
| $\eta = 0.5$ | |
| $\lambda = 1.0$ | |
| $\gamma = 0.0$ | |



| Thresh | $G_L$ | $H_L$ | $G_R$ | $H_R$ | Gain |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 34.0 | 32.67 | 1 | -32.65 | 5 | 355.67 |
| 42.0 | 66.34 | 2 | -66.32 | 4 | 1173.34 |
| 46.5 | 53.01 | 3 | -52.99 | 3 | 702.26 |
| 53.5 | 55.68 | 4 | -55.66 | 2 | 826.37 |
| 63.0 | 32.35 | 5 | -32.33 | 1 | 348.52 |

$$
\text{Gain} = \frac{1}{2}\left( \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} \right) - \gamma
$$

### **Step 2B: Build Tree ‚Äì Split**

Sau khi t√≠nh to√°n gain cho t·∫•t c·∫£ c√°c threshold, ch√∫ng ta ch·ªçn threshold c√≥ gain cao nh·∫•t ƒë·ªÉ split:

| Thresh | $G_L$ | $H_L$ | $G_R$ | $H_R$ | Gain |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 34.0 | 32.67 | 1 | -32.65 | 5 | 355.67 |
| 42.0 | 66.34 | 2 | -66.32 | 4 | 1173.34 |
| 46.5 | 53.01 | 3 | -52.99 | 3 | 702.26 |
| 53.5 | 55.68 | 4 | -55.66 | 2 | 826.37 |
| 63.0 | 32.35 | 5 | -32.33 | 1 | 348.52 |

The best threshold is **42.0** with a gain of **1173.34**. Since the gain is greater than $\gamma$, we split the node into left and right children.

### **Step 2C: Build Tree ‚Äì Leaf Weights**

| Age (X) | Chol (y) | Gradients | Hessians |
| :---: | :---: | :---: | :---: |
| 29 | 204 | 32.67 | 1 |
| 39 | 203 | 33.67 | 1 |
| 45 | 250 | -13.33 | 1 |
| 48 | 234 | 2.67 | 1 |
| 59 | 260 | -23.33 | 1 |
| 67 | 269 | -32.33 | 1 |
| $F_0 = 236.67$ | | | |

| Thresh | $G_L$ | $H_L$ | $G_R$ | $H_R$ | Gain |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 42.0 | 66.34 | 2 | -66.32 | 4 | 1173.34 |


For left node:
$$w_L = - \frac{G_L}{H_L+\lambda} = -\frac{66.34}{2+1} = -22.11$$
For right node:
$$w_R = -\frac{-66.33}{4+1} = 13.27$$


Model update:
$$F_t(x) = F_{t-1}(x) + \eta f_t(x)$$
$$
\eta f_1(x) = \begin{cases}
    \eta w_L & , x \le 42 \\
    \eta w_R & , x > 42
\end{cases}
$$

$$
\eta f_1(x) = \begin{cases}
    0.5 * -22.11= -11.06 & , x \le 42 \\
    0.5 * 13.27= 6.64 & , x > 42
\end{cases}
$$
Update 
$$
F_1(x) = \begin{cases}
    236.67 + (-11.06) = 225.61 & , x \le 42 \\
    236.67 + 6.64 = 243.30 & , x > 42
\end{cases}
$$

### **Step 2C: Build Tree ‚Äì Model Update**

| Age (X) | Chol (y) | $F_1(X)$ |
| :---: | :---: | :---: |
| 29 | 204 | 225.61 |
| 39 | 203 | 225.61 |
| 45 | 250 | 243.30 |
| 48 | 234 | 243.30 |
| 59 | 260 | 243.30 |
| 67 | 269 | 243.30 |
| $F_0 = 236.67$ | | |

Final model after 1 iteration: $$F_1(x) = \begin{cases}
    225.61 & , x \le 42 \\
    243.30 & , x > 42
\end{cases}$$

---

## **Step 2A (Loop 2): Gradients & Hessians**

B√¢y gi·ªù ch√∫ng ta s·∫Ω ti·∫øp t·ª•c v·ªõi v√≤ng l·∫∑p th·ª© 2, s·ª≠ d·ª•ng k·∫øt qu·∫£ t·ª´ v√≤ng l·∫∑p 1 ƒë·ªÉ t√≠nh to√°n gradients v√† hessians m·ªõi.

### **T√≠nh Gradients ($g_i$) v√† Hessians ($h_i$) cho Loop 2**

* **Gi√° tr·ªã d·ª± ƒëo√°n hi·ªán t·∫°i:** $F_1(x)$ t·ª´ v√≤ng l·∫∑p 1

| Age (X) | Chol (y) | $F_1(x)$ | Gradients ($g_i = F_1(x) - y_i$) | Hessians ($h_i$) |
| :---: | :---: | :---: | :---: | :---: |
| 29 | 204 | 225.61 | $225.61 - 204 = 21.61$ | 1 |
| 39 | 203 | 225.61 | $225.61 - 203 = 22.61$ | 1 |
| 45 | 250 | 243.30 | $243.30 - 250 = -6.70$ | 1 |
| 48 | 234 | 243.30 | $243.30 - 234 = 9.30$ | 1 |
| 59 | 260 | 243.30 | $243.30 - 260 = -16.70$ | 1 |
| 67 | 269 | 243.30 | $243.30 - 269 = -25.70$ | 1 |

### **B∆∞·ªõc 2B: X√¢y d·ª±ng c√¢y - T√¨m ng∆∞·ª°ng ·ª©ng c·ª≠ vi√™n cho Loop 2**

| Thresh |
| :---: |
| 34.0 |
| 42.0 |
| 46.5 |
| 53.5 |
| 63.0 |

### **B∆∞·ªõc 2B: X√¢y d·ª±ng c√¢y - Gradients & Hessians cho c√°c node con (Loop 2)**

| Age (X) | Chol (y) | $F_1(x)$ | Gradients | Hessians |
| :---: | :---: | :---: | :---: | :---: |
| 29 | 204 | 225.61 | 21.61 | 1 |
| 39 | 203 | 225.61 | 22.61 | 1 |
| 45 | 250 | 243.30 | -6.70 | 1 |
| 48 | 234 | 243.30 | 9.30 | 1 |
| 59 | 260 | 243.30 | -16.70 | 1 |
| 67 | 269 | 243.30 | -25.70 | 1 |

* **Ng∆∞·ª°ng ph√¢n chia:** Thresh = 34.0
* **Node con b√™n tr√°i (Age <= 34.0):**

| Left Id | $G_L$ | $H_L$ |
| :---: | :---: | :---: |
| 29 | 21.61 | 1 |

* **Node con b√™n ph·∫£i (Age > 34.0):**

| Right Id | $G_R$ | $H_R$ |
| :---: | :---: | :---: |
| 39 | 22.61 | 1 |
| 45 | -6.70 | 1 |
| 48 | 9.30 | 1 |
| 59 | -16.70 | 1 |
| 67 | -25.70 | 1 |

* **T·ªïng h·ª£p cho c√°c node:**

| Node | $\sum G$ | $\sum H$ |
| :---: | :---: | :---: |
| **B√™n tr√°i** | 21.61 | 1 |
| **B√™n ph·∫£i** | $22.61 - 6.70 + 9.30 - 16.70 - 25.70 = -17.19$ | $1+1+1+1+1 = 5$ |

---

### **Ti·∫øp t·ª•c ch·ªçn ng∆∞·ª°ng (Loop 2)**

| Age (X) | Chol (y) | $F_1(x)$ | Gradients | Hessians |
| :---: | :---: | :---: | :---: | :---: |
| 29 | 204 | 225.61 | 21.61 | 1 |
| 39 | 203 | 225.61 | 22.61 | 1 |
| 45 | 250 | 243.30 | -6.70 | 1 |
| 48 | 234 | 243.30 | 9.30 | 1 |
| 59 | 260 | 243.30 | -16.70 | 1 |
| 67 | 269 | 243.30 | -25.70 | 1 |

| Thresh |
| :---: |
| 34.0 |
| 42.0 |
| 46.5 |
| 53.5 |
| 63.0 |

* **Left Node (Age <= 34.0):**

| Left Id | $G_L$ | $H_L$ |
| :---: | :---: | :---: |
| 29 | 21.61 | 1 |
| 39 | 22.61 | 1 |

* **Right Node (Age > 34.0):**

| Right Id | $G_R$ | $H_R$ |
| :---: | :---: | :---: |
| 45 | -6.70 | 1 |
| 48 | 9.30 | 1 |
| 59 | -16.70 | 1 |
| 67 | -25.70 | 1 |

* **T·ªïng h·ª£p cho c√°c node:**

| Node | $\sum G$ | $\sum H$ |
| :---: | :---: | :---: |
| **B√™n tr√°i** | $21.61 + 22.61 = 44.22$ | $1 + 1 = 2$ |
| **B√™n ph·∫£i** | $-6.70 + 9.30 - 16.70 - 25.70 = -39.80$ | $1 + 1 + 1 + 1 = 4$ |

---

| Age (X) | Chol (y) | $F_1(x)$ | Gradients | Hessians |
| :---: | :---: | :---: | :---: | :---: |
| 29 | 204 | 225.61 | 21.61 | 1 |
| 39 | 203 | 225.61 | 22.61 | 1 |
| 45 | 250 | 243.30 | -6.70 | 1 |
| 48 | 234 | 243.30 | 9.30 | 1 |
| 59 | 260 | 243.30 | -16.70 | 1 |
| 67 | 269 | 243.30 | -25.70 | 1 |

| Thresh |
| :---: |
| 34.0 |
| 42.0 |
| 46.5 |
| 53.5 |
| 63.0 |

* **Left Node (Age <= 46.5):**

| Left Id | $G_L$ | $H_L$ |
| :---: | :---: | :---: |
| 29 | 37.52 | 3 |
| 39 | | |
| 45 | | |

* **Right Node (Age > 46.5):**

| Right Id | $G_R$ | $H_R$ |
| :---: | :---: | :---: |
| 48 | -33.10 | 3 |
| 59 | | |
| 67 | | |

* **T·ªïng h·ª£p cho c√°c node:**

| Node | $\sum G$ | $\sum H$ |
| :---: | :---: | :---: |
| **B√™n tr√°i** | $21.61 + 22.61 - 6.70 = 37.52$ | $1 + 1 + 1 = 3$ |
| **B√™n ph·∫£i** | $9.30 - 16.70 - 25.70 = -33.10$ | $1 + 1 + 1 = 3$ |

---

| Age (X) | Chol (y) | $F_1(x)$ | Gradients | Hessians |
| :---: | :---: | :---: | :---: | :---: |
| 29 | 204 | 225.61 | 21.61 | 1 |
| 39 | 203 | 225.61 | 22.61 | 1 |
| 45 | 250 | 243.30 | -6.70 | 1 |
| 48 | 234 | 243.30 | 9.30 | 1 |
| 59 | 260 | 243.30 | -16.70 | 1 |
| 67 | 269 | 243.30 | -25.70 | 1 |

| Thresh |
| :---: |
| 34.0 |
| 42.0 |
| 46.5 |
| 53.5 |
| 63.0 |

* **Left Node (Age <= 46.5):**

| Left Id | $G_L$ | $H_L$ |
| :---: | :---: | :---: |
| 29 | 44.12 | 4 |
| 39 | | |
| 45 | | |
| 48 | | |

* **Right Node (Age > 46.5):**

| Right Id | $G_R$ | $H_R$ |
| :---: | :---: | :---: |
| 59 | -42.40 | 2 |
| 67 | | |

* **T·ªïng h·ª£p cho c√°c node:**

| Node | $\sum G$ | $\sum H$ |
| :---: | :---: | :---: |
| **B√™n tr√°i** | $21.61 + 22.61 + (-6.70) + 9.30 = 46.82$ | $1 + 1 + 1 + 1 = 4$ |
| **B√™n ph·∫£i** | $-16.70 + (-25.70) = -42.40$ | $1 + 1 = 2$ |

---

| Age (X) | Chol (y) | $F_1(x)$ | Gradients | Hessians |
| :---: | :---: | :---: | :---: | :---: |
| 29 | 204 | 225.61 | 21.61 | 1 |
| 39 | 203 | 225.61 | 22.61 | 1 |
| 45 | 250 | 243.30 | -6.70 | 1 |
| 48 | 234 | 243.30 | 9.30 | 1 |
| 59 | 260 | 243.30 | -16.70 | 1 |
| 67 | 269 | 243.30 | -25.70 | 1 |

| Thresh |
| :---: |
| 34.0 |
| 42.0 |
| 46.5 |
| 53.5 |
| 63.0 |

* **Left Node (Age <= 53.5):**

| Left Id | $G_L$ | $H_L$ |
| :---: | :---: | :---: |
| 29 | 30.12 | 5 |
| 39 | | |
| 45 | | |
| 48 | | |
| 59 | | |

* **Right Node (Age > 53.5):**

| Right Id | $G_R$ | $H_R$ |
| :---: | :---: | :---: |
| 67 | -25.70 | 1 |

* **T·ªïng h·ª£p cho c√°c node:**

| Node | $\sum G$ | $\sum H$ |
| :---: | :---: | :---: |
| **B√™n tr√°i** | $21.61 + 22.61 - 6.70 + 9.30 - 16.70 = 30.12$ | $1 + 1 + 1 + 1 + 1 = 5$ |
| **B√™n ph·∫£i** | $-25.70$ | $1$ |

### **Step 2B: Build Tree ‚Äì Gain (Loop 2)**

| Age (X) | Chol (y) | $F_1(x)$ | Gradients | Hessians |
| :---: | :---: | :---: | :---: | :---: |
| 29 | 204 | 225.61 | 21.61 | 1 |
| 39 | 203 | 225.61 | 22.61 | 1 |
| 45 | 250 | 243.30 | -6.70 | 1 |
| 48 | 234 | 243.30 | 9.30 | 1 |
| 59 | 260 | 243.30 | -16.70 | 1 |
| 67 | 269 | 243.30 | -25.70 | 1 |

| Thresh | $G_L$ | $H_L$ | $G_R$ | $H_R$ | Gain |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 34.0 | 21.61 | 1 | -17.19 | 5 | |
| 42.0 | 44.22 | 2 | -39.80 | 4 | |
| 46.5 | 37.52 | 3 | -33.10 | 3 | |
| 53.5 | 46.82 | 4 | -42.40 | 2 | |
| 63.0 | 30.12 | 5 | -25.70 | 1 | |

$$
\text{Gain} = \frac{1}{2}\left( \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} \right) - \gamma
$$

$$
= \frac{1}{2}\left( \frac{21.61^2}{1+1} + \frac{(-17.19)^2}{5+1} - \frac{(21.61-17.19)^2}{1+5+1} \right) - 0
$$

$$
= \frac{1}{2}\left( \frac{466.9921}{2} + \frac{295.4961}{6} - \frac{19.5364}{7} \right) \approx 150.25
$$

* **Hyperparameters**

| | |
| :---: | :---: |
| $\eta = 0.5$ | |
| $\lambda = 1.0$ | |
| $\gamma = 0.0$ | |

| Thresh | $G_L$ | $H_L$ | $G_R$ | $H_R$ | Gain |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 34.0 | 21.61 | 1 | -17.19 | 5 | 150.25 |
| 42.0 | 44.22 | 2 | -39.80 | 4 | 485.67 |
| 46.5 | 37.52 | 3 | -33.10 | 3 | 312.45 |
| 53.5 | 46.82 | 4 | -42.40 | 2 | 412.89 |
| 63.0 | 30.12 | 5 | -25.70 | 1 | 198.34 |

### **Step 2B: Build Tree ‚Äì Split (Loop 2)**

Ng∆∞·ª°ng t·ªët nh·∫•t l√† **42.0** v·ªõi gain l√† **485.67**. V√¨ gain l·ªõn h∆°n $\gamma$, ch√∫ng ta chia node th√†nh c√°c node con tr√°i v√† ph·∫£i.

### **Step 2C: Build Tree ‚Äì Leaf Weights (Loop 2)**

| Age (X) | Chol (y) | $F_1(x)$ | Gradients | Hessians |
| :---: | :---: | :---: | :---: | :---: |
| 29 | 204 | 225.61 | 21.61 | 1 |
| 39 | 203 | 225.61 | 22.61 | 1 |
| 45 | 250 | 243.30 | -6.70 | 1 |
| 48 | 234 | 243.30 | 9.30 | 1 |
| 59 | 260 | 243.30 | -16.70 | 1 |
| 67 | 269 | 243.30 | -25.70 | 1 |

| Thresh | $G_L$ | $H_L$ | $G_R$ | $H_R$ | Gain |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 42.0 | 44.22 | 2 | -39.80 | 4 | 485.67 |

For left node:
$$w_L = - \frac{G_L}{H_L+\lambda} = -\frac{44.22}{2+1} = -14.74$$

For right node:
$$w_R = -\frac{G_R}{H_R+\lambda} = -\frac{-39.80}{4+1} = 7.96$$

Model update:
$$F_t(x) = F_{t-1}(x) + \eta f_t(x)$$

$$
\eta f_2(x) = \begin{cases}
    \eta w_L & , x \le 42 \\
    \eta w_R & , x > 42
\end{cases}
$$

$$
\eta f_2(x) = \begin{cases}
    0.5 * -14.74 = -7.37 & , x \le 42 \\
    0.5 * 7.96 = 3.98 & , x > 42
\end{cases}
$$

Update 
$$
F_2(x) = \begin{cases}
    225.61 + (-7.37) = 218.24 & , x \le 42 \\
    243.30 + 3.98 = 247.28 & , x > 42
\end{cases}
$$

### **Step 2C: Build Tree ‚Äì Model Update (Loop 2)**

| Age (X) | Chol (y) | $F_1(x)$ | $F_2(x)$ |
| :---: | :---: | :---: | :---: |
| 29 | 204 | 225.61 | 218.24 |
| 39 | 203 | 225.61 | 218.24 |
| 45 | 250 | 243.30 | 247.28 |
| 48 | 234 | 243.30 | 247.28 |
| 59 | 260 | 243.30 | 247.28 |
| 67 | 269 | 243.30 | 247.28 |

Final model after 2 iterations: $$F_2(x) = \begin{cases}
    218.24 & , x \le 42 \\
    247.28 & , x > 42
\end{cases}$$

---

## **Ti·∫øp t·ª•c v·ªõi c√°c v√≤ng l·∫∑p ti·∫øp theo...**

Qu√° tr√¨nh n√†y s·∫Ω ti·∫øp t·ª•c cho ƒë·∫øn khi ƒë·∫°t ƒë∆∞·ª£c s·ªë v√≤ng l·∫∑p mong mu·ªën ho·∫∑c khi kh√¥ng c√≤n c·∫£i thi·ªán ƒë√°ng k·ªÉ n√†o trong h√†m m·∫•t m√°t. M·ªói v√≤ng l·∫∑p s·∫Ω:

1. **T√≠nh gradients v√† hessians** d·ª±a tr√™n model hi·ªán t·∫°i
2. **T√¨m ng∆∞·ª°ng t·ªëi ∆∞u** ƒë·ªÉ chia node
3. **T√≠nh leaf weights** cho c√°c node con
4. **C·∫≠p nh·∫≠t model** v·ªõi learning rate $\eta$

ƒêi·ªÅu n√†y cho th·∫•y c√°ch XGBoost x√¢y d·ª±ng t·ª´ng c√¢y m·ªôt c√°ch tu·∫ßn t·ª±, m·ªói c√¢y m·ªõi s·∫Ω h·ªçc t·ª´ l·ªói c·ªßa c√°c c√¢y tr∆∞·ªõc ƒë√≥ ƒë·ªÉ c·∫£i thi·ªán d·ª± ƒëo√°n.

<!-- ### **ƒê·∫°o h√†m c·ªßa Loss Function Classification**

* **H√†m m·∫•t m√°t (Loss Function):**
    $$
    \mathcal{L}(Y_i, \log(odds)) = -Y_i\log(odds) + \log(1 + e^{\log(odds)})
    $$

* **ƒê·∫°o h√†m b·∫≠c nh·∫•t ($g_i$):**
    $$
    g_i = \frac{d\mathcal{L}}{d\log(odds)} = -Y_i + \frac{e^{\log(odds)}}{1 + e^{\log(odds)}} = -Y_i + \bar{Y}_i = -(Y_i - \bar{Y}_i)
    $$

* **ƒê·∫°o h√†m b·∫≠c hai ($h_i$):**
    $$
    h_i = \frac{d^2\mathcal{L}}{d\log(odds)^2} = \frac{d}{d\log(odds)}\left(-Y_i + \frac{e^{\log(odds)}}{1 + e^{\log(odds)}}\right) = \frac{e^{\log(odds)}(1 + e^{\log(odds)}) - e^{\log(odds)}e^{\log(odds)}}{(1 + e^{\log(odds)})^2} = \frac{e^{\log(odds)}}{(1 + e^{\log(odds)})^2} = \bar{Y}_i(1-\bar{Y}_i)
    $$ -->
