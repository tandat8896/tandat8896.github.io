---
title: "Multilayer Perceptron: Gradient Vanishing v√† Dying ReLU"
pubDatetime: 2025-01-20T12:00:00Z
featured: false
description: "T√¨m hi·ªÉu chi ti·∫øt v·ªÅ hai v·∫•n ƒë·ªÅ kinh ƒëi·ªÉn trong Deep Learning: Gradient Vanishing v√† Dying ReLU, v·ªõi ph√¢n t√≠ch to√°n h·ªçc c·ª• th·ªÉ v√† c√°c gi·∫£i ph√°p th·ª±c t·∫ø"
tags: ["Deep Learning", "Neural Networks", "Gradient Vanishing", "ReLU", "Backpropagation"]
---

B·∫°n c√≥ bao gi·ªù t·ª± h·ªèi t·∫°i sao m·∫°ng neural network c·ªßa m√¨nh kh√¥ng h·ªçc ƒë∆∞·ª£c khi c√≥ qu√° nhi·ªÅu l·ªõp? Hay t·∫°i sao m·ªôt s·ªë neuron l·∫°i "ch·∫øt" v√† kh√¥ng bao gi·ªù ho·∫°t ƒë·ªông tr·ªü l·∫°i? M√¨nh ƒë√£ t·ª´ng g·∫∑p nh·ªØng v·∫•n ƒë·ªÅ n√†y v√† ph·∫£i m·∫•t nhi·ªÅu th·ªùi gian m·ªõi hi·ªÉu ƒë∆∞·ª£c c∆° ch·∫ø ƒë·∫±ng sau ch√∫ng. Trong b√†i vi·∫øt n√†y, m√¨nh s·∫Ω c√πng c√°c b·∫°n kh√°m ph√° hai v·∫•n ƒë·ªÅ "kinh ƒëi·ªÉn" trong deep learning: **Gradient Vanishing** v√† **Dying ReLU**, th√¥ng qua c√°c v√≠ d·ª• t√≠nh to√°n c·ª• th·ªÉ v·ªõi s·ªë li·ªáu th·ª±c t·∫ø.

## 1. V·∫•n ƒë·ªÅ Gradient Vanishing (Ti√™u bi·∫øn Gradient) - Khi Gradient "Bi·∫øn m·∫•t"

Gradient Vanishing x·∫£y ra khi ƒë·ªô d·ªëc (gradient) c·ªßa h√†m m·∫•t m√°t tr·ªü n√™n c·ª±c k·ª≥ nh·ªè khi lan truy·ªÅn ng∆∞·ª£c (backpropagation) v·ªÅ c√°c l·ªõp ƒë·∫ßu ti√™n. ƒêi·ªÅu n√†y khi·∫øn vi·ªác c·∫≠p nh·∫≠t tr·ªçng s·ªë ·ªü c√°c l·ªõp s√¢u (g·∫ßn ƒë·∫ßu v√†o) di·ªÖn ra r·∫•t ch·∫≠m ho·∫∑c g·∫ßn nh∆∞ kh√¥ng x·∫£y ra, l√†m cho m·∫°ng kh√¥ng h·ªçc ƒë∆∞·ª£c.

M√¨nh ƒë√£ t·ª´ng "ƒëau ƒë·∫ßu" khi th·∫•y m·∫°ng neural network c·ªßa m√¨nh kh√¥ng h·ªçc ƒë∆∞·ª£c g√¨ sau nhi·ªÅu epoch, v√† sau ƒë√≥ m·ªõi ph√°t hi·ªán ra r·∫±ng gradient ƒë√£ "bi·∫øn m·∫•t" ho√†n to√†n ·ªü c√°c l·ªõp ƒë·∫ßu!

### 1.1. Ph√¢n t√≠ch h√†m Sigmoid - "Th·ªß ph·∫°m" ch√≠nh

H√†m Sigmoid ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√†:

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

ƒê·∫°o h√†m c·ªßa n√≥ l√†:

$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

**T√≠nh to√°n c·ª• th·ªÉ:**

1. **ƒê·∫°o h√†m t·ªëi ƒëa**: Gi√° tr·ªã l·ªõn nh·∫•t c·ªßa $\sigma'(x)$ x·∫£y ra khi $x = 0$, l√∫c n√†y $\sigma(0) = 0.5$.

$$\sigma'(0) = 0.5 \times (1 - 0.5) = 0.25$$

ƒê√¢y l√† gi√° tr·ªã l·ªõn nh·∫•t m√† ƒë·∫°o h√†m Sigmoid c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c!

2. **Tr∆∞·ªùng h·ª£p b√£o h√≤a (Saturation)**: Khi $x$ r·∫•t l·ªõn ho·∫∑c r·∫•t nh·ªè, h√†m Sigmoid b·ªã "b√£o h√≤a," t·ª©c l√† gi√° tr·ªã ƒë·∫ßu ra g·∫ßn 0 ho·∫∑c g·∫ßn 1.

- N·∫øu $x = 5$: $\sigma(5) \approx 0.993$, $\sigma'(5) \approx 0.993 \times (1 - 0.993) \approx 0.0069$
- N·∫øu $x = -5$: $\sigma(-5) \approx 0.0067$, $\sigma'(-5) \approx 0.0067 \times (1 - 0.0067) \approx 0.0066$

Nh∆∞ b·∫°n th·∫•y, khi ƒë·∫ßu v√†o n·∫±m ngo√†i v√πng trung t√¢m, ƒë·∫°o h√†m tr·ªü n√™n r·∫•t nh·ªè!

<center>
<img src="/static/uploads/20251129_122923_df963282.png" alt="Sigmoid Function" style="max-width:60%;">
<br>
<strong>H√¨nh 1:</strong> H√†m Sigmoid
</center>

#### V√≠ d·ª• T√≠nh tay: Gradient Vanishing qua Nhi·ªÅu L·ªõp ·∫®n

B√¢y gi·ªù, h√£y xem ƒëi·ªÅu g√¨ x·∫£y ra khi gradient lan truy·ªÅn qua m·ªôt m·∫°ng c√≥ nhi·ªÅu l·ªõp ·∫©n s·ª≠ d·ª•ng Sigmoid.

**Ki·∫øn tr√∫c m·∫°ng cho v√≠ d·ª• Gradient Vanishing:**

- **L·ªõp ƒë·∫ßu v√†o (Input Layer)**: 4 features + 1 bias
- **L·ªõp ·∫©n 1 (Hidden Layer 1)**: Nhi·ªÅu neurons (v√≠ d·ª•: 3+ neurons) v·ªõi h√†m k√≠ch ho·∫°t Sigmoid
- **L·ªõp ·∫©n 2, 3, ... (Hidden Layers)**: Nhi·ªÅu l·ªõp ·∫©n ti·∫øp theo, m·ªói l·ªõp c√≥ nhi·ªÅu neurons, t·∫•t c·∫£ ƒë·ªÅu s·ª≠ d·ª•ng h√†m k√≠ch ho·∫°t Sigmoid (fully connected)
- **L·ªõp ƒë·∫ßu ra (Output Layer)**: 3 neurons ($z_1$, $z_2$, $z_3$) v·ªõi h√†m k√≠ch ho·∫°t Softmax

<center>
<img src="/static/uploads/20251129_123240_edf52196.png" alt="Gradient Vanishing Sigmoid" style="max-width:60%;">
<br>
<strong>H√¨nh 2:</strong> MLP Architecture
</center>

*Gi·∫£ s·ª≠:**
- M·∫°ng c√≥ 5 l·ªõp ·∫©n, t·∫•t c·∫£ ƒë·ªÅu s·ª≠ d·ª•ng h√†m k√≠ch ho·∫°t Sigmoid
- Trong qu√° tr√¨nh lan truy·ªÅn ng∆∞·ª£c, gradient ban ƒë·∫ßu t·ª´ l·ªõp ƒë·∫ßu ra l√†: $\nabla_{output} L = 1.0$
- T·∫•t c·∫£ c√°c ƒë·∫ßu v√†o pre-activation ƒë·ªÅu n·∫±m trong v√πng b√£o h√≤a, v·ªõi ƒë·∫°o h√†m trung b√¨nh l√† $0.1$ (nh·ªè h∆°n gi√° tr·ªã t·ªëi ƒëa $0.25$)
<
**T√≠nh to√°n gradient qua t·ª´ng l·ªõp:**

**L·ªõp 5 (g·∫ßn ƒë·∫ßu ra nh·∫•t):**
$$\nabla_{layer5} L = \nabla_{output} L \times \sigma'(h_5) = 1.0 \times 0.1 = 0.1$$

**L·ªõp 4:**
$$\nabla_{layer4} L = \nabla_{layer5} L \times \sigma'(h_4) = 0.1 \times 0.1 = 0.01$$

**L·ªõp 3:**
$$\nabla_{layer3} L = \nabla_{layer4} L \times \sigma'(h_3) = 0.01 \times 0.1 = 0.001$$

**L·ªõp 2:**
$$\nabla_{layer2} L = \nabla_{layer3} L \times \sigma'(h_2) = 0.001 \times 0.1 = 0.0001$$

**L·ªõp 1 (g·∫ßn ƒë·∫ßu v√†o nh·∫•t):**
$$\nabla_{layer1} L = \nabla_{layer2} L \times \sigma'(h_1) = 0.0001 \times 0.1 = 0.00001$$

**K·∫øt qu·∫£:**


Nh∆∞ b·∫°n th·∫•y, sau 5 l·ªõp, gradient ƒë√£ gi·∫£m xu·ªëng c√≤n **0.00001**, t·ª©c l√† ch·ªâ c√≤n **0.001%** so v·ªõi gradient ban ƒë·∫ßu! ƒêi·ªÅu n√†y c√≥ nghƒ©a l√†:

- C√°c tr·ªçng s·ªë ·ªü l·ªõp 1 s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t v·ªõi learning rate c·ª±c k·ª≥ nh·ªè
- N·∫øu learning rate l√† $0.01$, th√¨ c·∫≠p nh·∫≠t th·ª±c t·∫ø s·∫Ω l√†: $\Delta w_1 = 0.00001 \times 0.01 = 0.0000001$ (g·∫ßn nh∆∞ b·∫±ng 0!)
- M·∫°ng s·∫Ω kh√¥ng h·ªçc ƒë∆∞·ª£c g√¨ ·ªü c√°c l·ªõp ƒë·∫ßu, d·∫´n ƒë·∫øn hi·ªán t∆∞·ª£ng **Gradient Vanishing**


### 1.3. Ph√¢n t√≠ch h√†m Tanh - T·ªët h∆°n nh∆∞ng v·∫´n c√≥ v·∫•n ƒë·ªÅ

H√†m Tanh (Hyperbolic Tangent) c√≥ ph·∫°m vi gi√° tr·ªã t·ª´ $[-1, 1]$, l·ªõn h∆°n Sigmoid $[0, 1]$. ƒê·∫°o h√†m c·ªßa n√≥ l√†:

<center>
<img src="/static/uploads/20251129_123559_8838d557.png" alt="Tanh Function" style="max-width:60%;">
<br>
<strong>H√¨nh 3:</strong> H√†m Tanh
</center>



$$\tanh'(x) = 1 - \tanh^2(x)$$

**So s√°nh v·ªõi Sigmoid:**

- **ƒê·∫°o h√†m t·ªëi ƒëa**: $\tanh'(0) = 1.0$ (l·ªõn h∆°n $0.25$ c·ªßa Sigmoid)
- **V·∫•n ƒë·ªÅ b√£o h√≤a**: T∆∞∆°ng t·ª± nh∆∞ Sigmoid, Tanh v·∫´n b·ªã b√£o h√≤a khi gi√° tr·ªã ƒë·∫ßu v√†o $x$ ti·∫øn v·ªÅ $\pm 2$

**V√≠ d·ª•:**
- $x = 2$: $\tanh(2) \approx 0.964$, $\tanh'(2) \approx 1 - 0.964^2 \approx 0.071$
- $x = 3$: $\tanh(3) \approx 0.995$, $\tanh'(3) \approx 1 - 0.995^2 \approx 0.010$

### 1.4. C√°c h√†m Activation kh√°c - Gi·∫£i ph√°p cho Gradient Vanishing

ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ Gradient Vanishing, nhi·ªÅu h√†m activation m·ªõi ƒë√£ ƒë∆∞·ª£c ph√°t tri·ªÉn qua c√°c nƒÉm. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë h√†m activation ph·ªï bi·∫øn:

**ReLU (Rectified Linear Unit) **

<center>
<img src="/static/uploads/20251129_155410_62a6699b.png" alt="ReLU Function" style="max-width:60%;">
<br>
<strong>H√¨nh 4:</strong> H√†m ReLU
</center>

$$\text{ReLU}(x) = \max(0, x) = \begin{cases} 0 & \text{n·∫øu } x < 0 \\ x & \text{n·∫øu } x \geq 0 \end{cases}$$

ReLU gi·∫£i quy·∫øt ƒë∆∞·ª£c v·∫•n ƒë·ªÅ gradient vanishing khi $x > 0$ (ƒë·∫°o h√†m = 1), nh∆∞ng l·∫°i g·∫∑p v·∫•n ƒë·ªÅ Dying ReLU khi $x \leq 0$ (ƒë·∫°o h√†m = 0). Ch√∫ng ta s·∫Ω ph√¢n t√≠ch chi ti·∫øt v·∫•n ƒë·ªÅ n√†y trong ph·∫ßn 2.

**Softplus:**

<center>
<img src="/static/uploads/20251129_154920_f43d0fa1.png" alt="Softplus Function" style="max-width:60%;">
<br>
<strong>H√¨nh 5:</strong> H√†m Softplus
</center>


$$\text{Softplus}(x) = \log(1 + e^x)$$

Softplus l√† phi√™n b·∫£n "m·ªÅm" c·ªßa ReLU, c√≥ ƒë·∫°o h√†m lu√¥n d∆∞∆°ng nh∆∞ng kh√¥ng b·∫±ng 0, gi√∫p tr√°nh ƒë∆∞·ª£c Dying ReLU.

**ELU (Exponential Linear Unit) **
<center>
<img src="/static/uploads/20251129_155512_91eefeff.png" alt="ELU Function" style="max-width:60%;">
<br>
<strong>H√¨nh 6:</strong> H√†m ELU
</center>


$$\text{ELU}(x) = \begin{cases} \alpha(e^x - 1) & \text{n·∫øu } x < 0 \\ x & \text{n·∫øu } x \geq 0 \end{cases}$$

ELU c√≥ ƒë·∫°o h√†m √¢m nh·ªè khi $x < 0$ (thay v√¨ b·∫±ng 0 nh∆∞ ReLU), gi√∫p tr√°nh Dying ReLU v√† cho ph√©p gradient lan truy·ªÅn ng∆∞·ª£c ngay c·∫£ khi ƒë·∫ßu v√†o √¢m.

**PReLU (Parametric ReLU)**
<center>
<img src="/static/uploads/20251129_155606_b61c66c8.png" alt="PReLU Function" style="max-width:60%;">
<br>
<strong>H√¨nh 7:</strong> H√†m PReLU
</center>


$$\text{PReLU}(x) = \begin{cases} \alpha x & \text{n·∫øu } x < 0 \\ x & \text{n·∫øu } x \geq 0 \end{cases}$$

PReLU t∆∞∆°ng t·ª± ELU nh∆∞ng h·ªá s·ªë $\alpha$ l√† tham s·ªë c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c, cho ph√©p m√¥ h√¨nh t·ª± ƒëi·ªÅu ch·ªânh ƒë·ªô d·ªëc khi $x < 0$.


**Swish**

<center>
<img src="/static/uploads/20251129_155742_481509b6.png" alt="Swish Function" style="max-width:60%;">
<br>
<strong>H√¨nh 8:</strong> H√†m Swish
</center>


$$\text{Swish}(x) = x \cdot \sigma(x) = x \cdot \frac{1}{1 + e^{-x}}$$

Swish l√† t√≠ch c·ªßa $x$ v√† Sigmoid, c√≥ ƒë·∫°o h√†m lu√¥n d∆∞∆°ng v√† kh√¥ng b·ªã b√£o h√≤a nh∆∞ Sigmoid thu·∫ßn t√∫y.

**GELU (Gaussian Error Linear Unit)**
$$\text{GELU}(x) = x \Phi(x) \approx x \cdot \sigma(1.702x)$$

GELU s·ª≠ d·ª•ng ph√¢n ph·ªëi chu·∫©n t√≠ch l≈©y $\Phi(x)$, ƒë∆∞·ª£c x·∫•p x·ªâ b·∫±ng $x$ nh√¢n v·ªõi Sigmoid. GELU ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i trong c√°c m√¥ h√¨nh transformer hi·ªán ƒë·∫°i.




## 2. V·∫•n ƒë·ªÅ Dying ReLU (ReLU Ch·∫øt) - Khi Neuron "Ch·∫øt"

ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ Gradient Vanishing, h√†m ReLU (Rectified Linear Unit) ƒë∆∞·ª£c ph√°t minh:
Tr∆∞·ªõc khi ƒëi s√¢u v√†o c√°c v·∫•n ƒë·ªÅ, h√£y c√πng xem c√°c ki·∫øn tr√∫c MLP c·ª• th·ªÉ m√† ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng trong c√°c v√≠ d·ª•. M·ªói v·∫•n ƒë·ªÅ s·∫Ω s·ª≠ d·ª•ng m·ªôt ki·∫øn tr√∫c ph√π h·ª£p ƒë·ªÉ minh h·ªça r√µ r√†ng nh·∫•t:

### Ki·∫øn tr√∫c cho v√≠ d·ª• Dying ReLU

**Ki·∫øn tr√∫c m·∫°ng:**
- **L·ªõp ƒë·∫ßu v√†o (Input Layer)**: 2 features ($x_1$, $x_2$) + 1 bias = 3 nodes
- **L·ªõp ·∫©n (Hidden Layer)**: 2 neurons ($h_1$, $h_2$) v·ªõi h√†m k√≠ch ho·∫°t ReLU
- **L·ªõp ƒë·∫ßu ra (Output Layer)**: 3 neurons ($z_1$, $z_2$, $z_3$) v·ªõi h√†m k√≠ch ho·∫°t Softmax
- **H√†m m·∫•t m√°t (Loss Function)**: Cross-entropy loss

<center>
<img src="/static/uploads/20251129_121954_a8cc7aa7.png" alt="MLP Architecture" style="max-width:80%;">
<br>
<strong>H√¨nh 10:</strong> MLP Architecture
</center>

$$\text{ReLU}(x) = \max(0, x)$$


ƒê·∫°o h√†m c·ªßa ReLU l√†:
- $\text{ReLU}'(x) = 1$ n·∫øu $x > 0$
- $\text{ReLU}'(x) = 0$ n·∫øu $x \leq 0$

ƒêi·ªÅu n√†y gi√∫p duy tr√¨ gradient l·ªõn khi $x > 0$, gi·∫£i quy·∫øt ƒë∆∞·ª£c v·∫•n ƒë·ªÅ gradient vanishing. Tuy nhi√™n, ReLU l·∫°i sinh ra v·∫•n ƒë·ªÅ **Dying ReLU**, x·∫£y ra khi m·ªôt neuron lu√¥n ƒë∆∞a ra k·∫øt qu·∫£ b·∫±ng 0 cho t·∫•t c·∫£ c√°c m·∫´u d·ªØ li·ªáu, khi·∫øn gradient c·ªßa n√≥ lu√¥n b·∫±ng 0 v√† neuron ƒë√≥ ng·ª´ng h·ªçc vƒ©nh vi·ªÖn.

M√¨nh ƒë√£ t·ª´ng th·∫•y m·ªôt m·∫°ng neural network c√≥ ƒë·∫øn 30% neurons "ch·∫øt" sau v√†i epoch, v√† ƒëi·ªÅu n√†y l√†m gi·∫£m ƒë√°ng k·ªÉ kh·∫£ nƒÉng h·ªçc c·ªßa m√¥ h√¨nh!



### 2.1. V√≠ d·ª• T√≠nh tay:  Dying ReLU


Ch√∫ng ta s·∫Ω xem x√©t m·ªôt v√≠ d·ª• MLP c·ª• th·ªÉ v·ªõi ki·∫øn tr√∫c r√µ r√†ng v√† minh h·ªça qu√° tr√¨nh lan truy·ªÅn ng∆∞·ª£c khi m·ªôt node b·ªã ch·∫øt.

**C·∫•u h√¨nh M√¥ h√¨nh v√† D·ªØ li·ªáu:**

- **ƒê·∫ßu v√†o**: $x = [1.5, 0.2]$ (sau khi th√™m bias: $x = [1.0, 1.5, 0.2]$)
- **Nh√£n**: $y = 0$ (m√£ h√≥a one-hot: $y = [1, 0, 0]$)
- **C·∫•u tr√∫c m·∫°ng**:
  - L·ªõp ·∫©n 1: 2 neurons ($h_1, h_2$) s·ª≠ d·ª•ng ReLU
  - L·ªõp ƒë·∫ßu ra: 3 neurons s·ª≠ d·ª•ng Softmax

**Tr·ªçng s·ªë v√† Bias ban ƒë·∫ßu (L·ªõp ·∫®n 1):**

Ma tr·∫≠n tr·ªçng s·ªë $M$ (k√≠ch th∆∞·ªõc $3 \times 2$, bao g·ªìm bias ·ªü h√†ng ƒë·∫ßu ti√™n):

$$M = \begin{bmatrix}
0.0 & 0.0 \\
0.86 & -1.04 \\
0.41 & -0.65
\end{bmatrix}$$

Bias $b_m = [0.0, 0.0]$ (ƒë√£ ƒë∆∞·ª£c g·ªôp v√†o $M$ trong ph√©p t√≠nh $h = x \times M$)

**Tr·ªçng s·ªë l·ªõp ƒë·∫ßu ra** (t·ª´ hidden layer ƒë·∫øn output layer):

$$W_{out} = \begin{bmatrix}
0.5 & 0.3 & -0.2 \\
-0.1 & 0.4 & 0.6
\end{bmatrix}$$

### B∆∞·ªõc 1: Lan truy·ªÅn thu·∫≠n (Forward Propagation)

**1. T√≠nh to√°n ƒë·∫ßu v√†o cho l·ªõp ·∫©n (pre-activation):**

$$h = x \times M = [1.0, 1.5, 0.2] \times \begin{bmatrix}
0.0 & 0.0 \\
0.86 & -1.04 \\
0.41 & -0.65
\end{bmatrix}$$

T√≠nh t·ª´ng ph·∫ßn t·ª≠:

- $h_1 = (1.0 \times 0.0) + (1.5 \times 0.86) + (0.2 \times 0.41) = 0 + 1.29 + 0.082 = 1.372$
- $h_2 = (1.0 \times 0.0) + (1.5 \times -1.04) + (0.2 \times -0.65) = 0 - 1.56 - 0.13 = -1.69$

V·∫≠y: $h = [1.372, -1.69]$

**2. √Åp d·ª•ng ReLU:**

$$\text{ReLU}(h) = [\max(0, 1.372), \max(0, -1.69)] = [1.372, 0.0]$$

**K·∫øt qu·∫£ quan tr·ªçng**: Node th·ª© hai ($h_2$) ƒë√£ "ch·∫øt" v√¨ ƒë·∫ßu v√†o pre-activation c·ªßa n√≥ l√† √¢m ($-1.69$), khi·∫øn ƒë·∫ßu ra ReLU b·∫±ng 0.
**3. Lan truy·ªÅn ƒë·∫øn l·ªõp ƒë·∫ßu ra:**

ƒê·∫ßu v√†o l·ªõp ƒë·∫ßu ra: 
$$
z = \text{ReLU}(\mathbf{h}) \cdot \mathbf{W}_{\text{out}} =
\begin{bmatrix} 1.372 & 0 \end{bmatrix}
\begin{bmatrix}
0.5 & 0.3 & -0.2 \\
-0.1 & 0.4 & 0.6
\end{bmatrix}
$$


T√≠nh t·ª´ng ph·∫ßn t·ª≠:

- $z_1 = (1.372 \times 0.5) + (0.0 \times -0.1) = 0.686 + 0 = 0.686$

**4. √Åp d·ª•ng Softmax v√† t√≠nh Loss:**

Softmax probabilities:
- $p_1 = \frac{e^{0.686}}{e^{0.686} + e^{0.412} + e^{-0.274}} = \frac{1.986}{1.986 + 1.510 + 0.760} = \frac{1.986}{4.256} = 0.467$
- $p_2 = \frac{1.510}{4.256} = 0.355$
- $p_3 = \frac{0.760}{4.256} = 0.178$

Cross-Entropy Loss v·ªõi nh√£n $y = [1, 0, 0]$:
$$L = -\log(p_1) = -\log(0.467) = 0.761$$

### B∆∞·ªõc 2: Lan truy·ªÅn ng∆∞·ª£c (Backward Propagation) - V·∫•n ƒë·ªÅ xu·∫•t hi·ªán

Trong lan truy·ªÅn ng∆∞·ª£c, ch√∫ng ta t√≠nh to√°n gradient c·ªßa m·∫•t m√°t ($L$) ƒë·ªëi v·ªõi c√°c tr·ªçng s·ªë.

**1. T√≠nh $\nabla_z L$ (Gradient l·ªõp ƒë·∫ßu ra):**

V·ªõi Cross-Entropy Loss v√† Softmax:
$$\nabla_z L = \text{softmax}(z) - y = [0.467, 0.355, 0.178] - [1, 0, 0] = [-0.533, 0.355, 0.178]$$

**2. T√≠nh $\nabla_{\text{ReLU}} L$ (Gradient sau ReLU):**

ƒê√¢y l√† gradient c·ªßa m·∫•t m√°t ƒë·ªëi v·ªõi ƒë·∫ßu ra c·ªßa l·ªõp ·∫©n:

$$\nabla_{\text{ReLU}} L = W_{out}^T \times \nabla_z L = \begin{bmatrix}
0.5 & -0.1 \\
0.3 & 0.4 \\
-0.2 & 0.6
\end{bmatrix} \times \begin{bmatrix}
-0.533 \\
0.355 \\
0.178
\end{bmatrix}$$

T√≠nh t·ª´ng ph·∫ßn t·ª≠:

- $\nabla_{\text{ReLU}_1} L = (0.5 \times -0.533) + (0.3 \times 0.355) + (-0.2 \times 0.178) = -0.267 + 0.107 - 0.036 = -0.196$
- $\nabla_{\text{ReLU}_2} L = (-0.1 \times -0.533) + (0.4 \times 0.355) + (0.6 \times 0.178) = 0.053 + 0.142 + 0.107 = 0.302$

V·∫≠y: $\nabla_{\text{ReLU}} L = [-0.196, 0.302]$

**3. T√≠nh $\nabla_h L$ (Gradient tr∆∞·ªõc ReLU) - ƒê√¢y l√† ƒëi·ªÉm x·∫£y ra v·∫•n ƒë·ªÅ Dying ReLU:**

Ta s·ª≠ d·ª•ng ƒë·∫°o h√†m c·ªßa ReLU:

$$\frac{\partial L}{\partial h_j} = \frac{\partial L}{\partial \text{ReLU}_j} \times \text{ReLU}'(h_j)$$

- **ƒê·ªëi v·ªõi node $h_1$ (s·ªëng)**: $h_1 = 1.372 > 0$, n√™n $\text{ReLU}'(h_1) = 1$

$$\frac{\partial L}{\partial h_1} = \nabla_{\text{ReLU}_1} L \times 1 = -0.196 \times 1 = -0.196$$

- **ƒê·ªëi v·ªõi node $h_2$ (ch·∫øt)**: $h_2 = -1.69 \leq 0$, n√™n $\text{ReLU}'(h_2) = 0$

$$\frac{\partial L}{\partial h_2} = \nabla_{\text{ReLU}_2} L \times 0 = 0.302 \times 0 = 0.0$$

**K·∫øt qu·∫£ quan tr·ªçng**: Gradient c·ªßa node $h_2$ b·∫±ng 0, m·∫∑c d√π gradient t·ª´ l·ªõp sau ($\nabla_{\text{ReLU}_2} L = 0.302$) kh√¥ng ph·∫£i l√† 0!

**4. T√≠nh $\nabla_M L$ (Gradient c·ªßa tr·ªçng s·ªë l·ªõp ·∫©n):**

Gradient c·ªßa tr·ªçng s·ªë ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n $\nabla_h L$ v√† ƒë·∫ßu v√†o $x$:

$$\nabla_M L = x^T \times \nabla_h L = \begin{bmatrix}
1.0 \\
1.5 \\
0.2
\end{bmatrix} \times [-0.196, 0.0]$$

T√≠nh t·ª´ng ph·∫ßn t·ª≠:

**C·ªôt 1 (k·∫øt n·ªëi v·ªõi $h_1$ - node s·ªëng):**
- $\frac{\partial L}{\partial m_{11}} = 1.0 \times -0.196 = -0.196$
- $\frac{\partial L}{\partial m_{21}} = 1.5 \times -0.196 = -0.294$
- $\frac{\partial L}{\partial m_{31}} = 0.2 \times -0.196 = -0.039$

**C·ªôt 2 (k·∫øt n·ªëi v·ªõi $h_2$ - node ch·∫øt):**
- $\frac{\partial L}{\partial m_{12}} = 1.0 \times 0.0 = 0.0$
- $\frac{\partial L}{\partial m_{22}} = 1.5 \times 0.0 = 0.0$
- $\frac{\partial L}{\partial m_{32}} = 0.2 \times 0.0 = 0.0$

V·∫≠y ma tr·∫≠n gradient:

$$\nabla_M L = \begin{bmatrix}
-0.196 & 0.0 \\
-0.294 & 0.0 \\
-0.039 & 0.0
\end{bmatrix}$$

**K·∫øt lu·∫≠n Dying ReLU:**

Do gradient cho c√°c tr·ªçng s·ªë d·∫´n ƒë·∫øn node $h_2$ b·∫±ng 0, nh·ªØng tr·ªçng s·ªë ƒë√≥ s·∫Ω kh√¥ng bao gi·ªù ƒë∆∞·ª£c c·∫≠p nh·∫≠t:

- N·∫øu learning rate l√† $\eta = 0.01$, th√¨ c·∫≠p nh·∫≠t tr·ªçng s·ªë:
  - $\Delta m_{12} = -\eta \times 0.0 = 0.0$
  - $\Delta m_{22} = -\eta \times 0.0 = 0.0$
  - $\Delta m_{32} = -\eta \times 0.0 = 0.0$

Node $h_2$ m√£i m√£i n·∫±m trong v√πng √¢m khi lan truy·ªÅn thu·∫≠n (v√¨ tr·ªçng s·ªë kh√¥ng thay ƒë·ªïi), v√† m√£i m√£i c√≥ gradient b·∫±ng 0 khi lan truy·ªÅn ng∆∞·ª£c. Node n√†y ƒë√£ "ch·∫øt" v√† kh√¥ng c√≤n ƒë√≥ng g√≥p v√†o qu√° tr√¨nh h·ªçc c·ªßa m√¥ h√¨nh.

## T√≥m L∆∞·ª£c v√† Gi·∫£i Ph√°p Kh·∫Øc Ph·ª•c

Sau khi hi·ªÉu r√µ c∆° ch·∫ø c·ªßa hai v·∫•n ƒë·ªÅ n√†y, m√¨nh ƒë√£ t·ªïng h·ª£p l·∫°i b·∫£ng so s√°nh v√† gi·∫£i ph√°p:

| V·∫•n ƒë·ªÅ | H√†m k√≠ch ho·∫°t li√™n quan | Nguy√™n nh√¢n c∆° ch·∫ø | ƒê·∫°o h√†m (Max) | Gi·∫£i ph√°p g·ª£i √Ω |
|--------|------------------------|-------------------|---------------|-----------------|
| **Gradient Vanishing** | Sigmoid, Tanh | ƒê·∫°o h√†m r·∫•t nh·ªè khi b·ªã b√£o h√≤a (Saturation) | Sigmoid: 0.25<br>Tanh: 1.0 | D√πng ReLU, GELU, SWISH, Skip Connection |
| **Dying ReLU** | ReLU | ƒê·∫ßu v√†o pre-activation √¢m ($\leq 0$), khi·∫øn ƒë·∫°o h√†m b·∫±ng 0 | ReLU: 1 (khi $x > 0$)<br>0 (khi $x \leq 0$) | D√πng LeakyReLU, PReLU, ELU (cho ƒë·∫°o h√†m nh·ªè h∆°n 0.01)) |


## 3. V·∫•n ƒë·ªÅ Zero Initialization (Kh·ªüi t·∫°o b·∫±ng 0) - T·∫°i sao MLP kh√¥ng th·ªÉ kh·ªüi t·∫°o b·∫±ng 0?

Khi m·ªõi b·∫Øt ƒë·∫ßu h·ªçc deep learning, m√¨nh ƒë√£ t·ª´ng th·∫Øc m·∫Øc: "T·∫°i sao Linear Regression c√≥ th·ªÉ kh·ªüi t·∫°o tr·ªçng s·ªë b·∫±ng 0, nh∆∞ng MLP th√¨ kh√¥ng?" C√¢u tr·∫£ l·ªùi n·∫±m ·ªü v·∫•n ƒë·ªÅ **Symmetry Problem (V·∫•n ƒë·ªÅ ƒê·ªëi x·ª©ng)**. H√£y c√πng m√¨nh ch·ª©ng minh ƒëi·ªÅu n√†y qua c√°c v√≠ d·ª• t√≠nh to√°n c·ª• th·ªÉ.
### 3.1. T·∫°i sao Linear Regression c√≥ th·ªÉ kh·ªüi t·∫°o b·∫±ng 0?

**M√¥ h√¨nh Linear Regression:**

$$\hat{y} = w_1x_1 + w_2x_2 + w_3x_3 + b$$

**Loss Function (MSE):**

$$L = \frac{1}{2}(\hat{y} - y)^2$$

**Gradient:**

$$\frac{\partial L}{\partial w_1} = (\hat{y} - y) \times x_1$$
$$\frac{\partial L}{\partial w_2} = (\hat{y} - y) \times x_2$$
$$\frac{\partial L}{\partial w_3} = (\hat{y} - y) \times x_3$$
$$\frac{\partial L}{\partial b} = (\hat{y} - y)$$

**V√≠ d·ª• t√≠nh to√°n v·ªõi kh·ªüi t·∫°o b·∫±ng 0:**

- ƒê·∫ßu v√†o: $x = [2.0, 3.0, 1.0]$
- Nh√£n th·ª±c t·∫ø: $y = 5.0$
- Kh·ªüi t·∫°o: $w_1 = 0, w_2 = 0, w_3 = 0, b = 0$

**Forward pass:**
$$\hat{y} = 0 \times 2.0 + 0 \times 3.0 + 0 \times 1.0 + 0 = 0$$

**Backward pass:**
- $\frac{\partial L}{\partial w_1} = (0 - 5.0) \times 2.0 = -10.0$
- $\frac{\partial L}{\partial w_2} = (0 - 5.0) \times 3.0 = -15.0$
- $\frac{\partial L}{\partial w_3} = (0 - 5.0) \times 1.0 = -5.0$
- $\frac{\partial L}{\partial b} = (0 - 5.0) = -5.0$

**C·∫≠p nh·∫≠t (learning rate $\eta = 0.01$):**
- $w_1 = 0 - 0.01 \times (-10.0) = 0.1$
- $w_2 = 0 - 0.01 \times (-15.0) = 0.15$
- $w_3 = 0 - 0.01 \times (-5.0) = 0.05$
- $b = 0 - 0.01 \times (-5.0) = 0.05$

**K·∫øt lu·∫≠n**: M·ªói tr·ªçng s·ªë nh·∫≠n ƒë∆∞·ª£c gradient **kh√°c nhau** d·ª±a tr√™n gi√° tr·ªã ƒë·∫ßu v√†o t∆∞∆°ng ·ª©ng ($x_1, x_2, x_3$). Do ƒë√≥, ch√∫ng s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t theo c√°c h∆∞·ªõng kh√°c nhau v√† m√¥ h√¨nh c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c.

### 3.2. T·∫°i sao Logistic Regression c√≥ th·ªÉ kh·ªüi t·∫°o b·∫±ng 0?

**M√¥ h√¨nh Logistic Regression:**

$$\hat{y} = \sigma(w_1x_1 + w_2x_2 + b) = \frac{1}{1 + e^{-(w_1x_1 + w_2x_2 + b)}}$$

**Loss Function (Cross-Entropy):**

$$L = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$$

**Gradient:**

$$\frac{\partial L}{\partial w_1} = (\hat{y} - y) \times x_1$$
$$\frac{\partial L}{\partial w_2} = (\hat{y} - y) \times x_2$$
$$\frac{\partial L}{\partial b} = (\hat{y} - y)$$

**V√≠ d·ª• t√≠nh to√°n v·ªõi kh·ªüi t·∫°o b·∫±ng 0:**

- ƒê·∫ßu v√†o: $x = [1.5, 2.0]$
- Nh√£n th·ª±c t·∫ø: $y = 1$
- Kh·ªüi t·∫°o: $w_1 = 0, w_2 = 0, b = 0$

**Forward pass:**
- $z = 0 \times 1.5 + 0 \times 2.0 + 0 = 0$
- $\hat{y} = \sigma(0) = \frac{1}{1 + e^0} = 0.5$

**Backward pass:**
- $\frac{\partial L}{\partial w_1} = (0.5 - 1) \times 1.5 = -0.75$
- $\frac{\partial L}{\partial w_2} = (0.5 - 1) \times 2.0 = -1.0$
- $\frac{\partial L}{\partial b} = (0.5 - 1) = -0.5$

**C·∫≠p nh·∫≠t (learning rate $\eta = 0.01$):**
- $w_1 = 0 - 0.01 \times (-0.75) = 0.0075$
- $w_2 = 0 - 0.01 \times (-1.0) = 0.01$
- $b = 0 - 0.01 \times (-0.5) = 0.005$

**K·∫øt lu·∫≠n**: T∆∞∆°ng t·ª± Linear Regression, m·ªói tr·ªçng s·ªë nh·∫≠n ƒë∆∞·ª£c gradient kh√°c nhau v√† c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c.

### 3.3. T·∫°i sao Softmax Regression c√≥ th·ªÉ kh·ªüi t·∫°o b·∫±ng 0?

**M√¥ h√¨nh Softmax Regression (3 classes):**

$$z_1 = w_{11}x_1 + w_{12}x_2 + b_1$$
$$z_2 = w_{21}x_1 + w_{22}x_2 + b_2$$
$$z_3 = w_{31}x_1 + w_{32}x_2 + b_3$$

$$\hat{y}_1 = \frac{e^{z_1}}{e^{z_1} + e^{z_2} + e^{z_3}}$$
$$\hat{y}_2 = \frac{e^{z_2}}{e^{z_1} + e^{z_2} + e^{z_3}}$$
$$\hat{y}_3 = \frac{e^{z_3}}{e^{z_1} + e^{z_2} + e^{z_3}}$$

**V√≠ d·ª• t√≠nh to√°n v·ªõi kh·ªüi t·∫°o b·∫±ng 0:**

- ƒê·∫ßu v√†o: $x = [1.0, 2.0]$
- Nh√£n th·ª±c t·∫ø: $y = [1, 0, 0]$ (one-hot)
- Kh·ªüi t·∫°o: T·∫•t c·∫£ $w_{ij} = 0$, $b_i = 0$

**Forward pass:**
- $z_1 = 0 \times 1.0 + 0 \times 2.0 + 0 = 0$
- $z_2 = 0 \times 1.0 + 0 \times 2.0 + 0 = 0$
- $z_3 = 0 \times 1.0 + 0 \times 2.0 + 0 = 0$

- $\hat{y}_1 = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} = 0.333$
- $\hat{y}_2 = \frac{1}{3} = 0.333$
- $\hat{y}_3 = \frac{1}{3} = 0.333$

**Backward pass (Cross-Entropy Loss):**

$$\frac{\partial L}{\partial z_i} = \hat{y}_i - y_i$$

- $\frac{\partial L}{\partial z_1} = 0.333 - 1 = -0.667$
- $\frac{\partial L}{\partial z_2} = 0.333 - 0 = 0.333$
- $\frac{\partial L}{\partial z_3} = 0.333 - 0 = 0.333$

**Gradient cho tr·ªçng s·ªë:**

- $\frac{\partial L}{\partial w_{11}} = \frac{\partial L}{\partial z_1} \times x_1 = -0.667 \times 1.0 = -0.667$
- $\frac{\partial L}{\partial w_{12}} = \frac{\partial L}{\partial z_1} \times x_2 = -0.667 \times 2.0 = -1.334$
- $\frac{\partial L}{\partial w_{21}} = 0.333 \times 1.0 = 0.333$
- $\frac{\partial L}{\partial w_{22}} = 0.333 \times 2.0 = 0.666$
- $\frac{\partial L}{\partial w_{31}} = 0.333 \times 1.0 = 0.333$
- $\frac{\partial L}{\partial w_{32}} = 0.333 \times 2.0 = 0.666$

**K·∫øt lu·∫≠n**: M·∫∑c d√π t·∫•t c·∫£ $z_i$ ban ƒë·∫ßu b·∫±ng nhau (d·∫´n ƒë·∫øn $\hat{y}_i$ b·∫±ng nhau), nh∆∞ng gradient cho m·ªói l·ªõp **kh√°c nhau** d·ª±a tr√™n nh√£n th·ª±c t·∫ø. Do ƒë√≥, c√°c tr·ªçng s·ªë s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t kh√°c nhau v√† m√¥ h√¨nh c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c.


### 3.4. T·∫°i sao MLP KH√îNG TH·ªÇ kh·ªüi t·∫°o b·∫±ng 0? - V·∫•n ƒë·ªÅ ƒê·ªëi x·ª©ng

B√¢y gi·ªù, h√£y xem ƒëi·ªÅu g√¨ x·∫£y ra v·ªõi MLP khi kh·ªüi t·∫°o b·∫±ng 0.

**C·∫•u h√¨nh M√¥ h√¨nh:**

- **L·ªõp ƒë·∫ßu v√†o**: 2 features ($x_1$, $x_2$) + 1 bias
- **L·ªõp ·∫©n**: 2 neurons ($h_1, h_2$) s·ª≠ d·ª•ng ReLU
- **L·ªõp ƒë·∫ßu ra**: 3 neurons ($z_1$, $z_2$, $z_3$) s·ª≠ d·ª•ng Softmax

**D·ªØ li·ªáu ƒë·∫ßu v√†o (3 samples):**

$$x = \begin{bmatrix}
x_1^{(1)} & x_2^{(1)} \\
x_1^{(2)} & x_2^{(2)} \\
x_1^{(3)} & x_2^{(3)}
\end{bmatrix} = \begin{bmatrix}
1.5 & 0.2 \\
4.7 & 1.6 \\
5.6 & 2.2
\end{bmatrix}$$

Sau khi th√™m bias (augment v·ªõi c·ªôt 1 ·ªü ƒë·∫ßu): $[1, x_1, x_2]$ cho m·ªói sample

**Nh√£n th·ª±c t·∫ø (one-hot encoding):**

$$y = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}$$

(Sample 1 thu·ªôc class 0, Sample 2 thu·ªôc class 1, Sample 3 thu·ªôc class 2)

**Tr·ªçng s·ªë kh·ªüi t·∫°o b·∫±ng 0:**

Ma tr·∫≠n tr·ªçng s·ªë l·ªõp ·∫©n $M$ (k√≠ch th∆∞·ªõc $3 \times 2$, bao g·ªìm bias ·ªü h√†ng ƒë·∫ßu ti√™n):

$$M = \begin{bmatrix}
bm_1 & bm_2 \\
m_{11} & m_{12} \\
m_{21} & m_{22}
\end{bmatrix} = \begin{bmatrix}
0.0 & 0.0 \\
0.0 & 0.0 \\
0.0 & 0.0
\end{bmatrix}$$

Ma tr·∫≠n tr·ªçng s·ªë l·ªõp ƒë·∫ßu ra $W$ (k√≠ch th∆∞·ªõc $2 \times 3$, kh√¥ng bao g·ªìm bias - bias ƒë∆∞·ª£c th√™m ri√™ng):

$$W = \begin{bmatrix}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23}
\end{bmatrix} = \begin{bmatrix}
0.0 & 0.0 & 0.0 \\
0.0 & 0.0 & 0.0
\end{bmatrix}$$

Bias cho l·ªõp ƒë·∫ßu ra: $bw = [bw_1, bw_2, bw_3] = [0.0, 0.0, 0.0]$

### B∆∞·ªõc 1: Lan truy·ªÅn thu·∫≠n (Forward Propagation)

**1. T√≠nh to√°n ƒë·∫ßu v√†o cho l·ªõp ·∫©n (pre-activation):**

V·ªõi m·ªói sample, ta t√≠nh $h = [1, x_1, x_2] \times M$. V√¨ t·∫•t c·∫£ tr·ªçng s·ªë trong $M$ ƒë·ªÅu b·∫±ng 0, k·∫øt qu·∫£ cho c·∫£ 3 samples l√†:

$$h = \begin{bmatrix}
h_1^{(1)} & h_2^{(1)} \\
h_1^{(2)} & h_2^{(2)} \\
h_1^{(3)} & h_2^{(3)}
\end{bmatrix} = \begin{bmatrix}
0.0 & 0.0 \\
0.0 & 0.0 \\
0.0 & 0.0
\end{bmatrix}$$

T√≠nh chi ti·∫øt cho sample 1:
- $h_1^{(1)} = 1 \times 0.0 + 1.5 \times 0.0 + 0.2 \times 0.0 = 0.0$
- $h_2^{(1)} = 1 \times 0.0 + 1.5 \times 0.0 + 0.2 \times 0.0 = 0.0$

T∆∞∆°ng t·ª±, t·∫•t c·∫£ c√°c samples ƒë·ªÅu c√≥ $h = [0.0, 0.0]$

**2. √Åp d·ª•ng ReLU:**

$$\text{ReLU}(h) = \begin{bmatrix}
\max(0, 0.0) & \max(0, 0.0) \\
\max(0, 0.0) & \max(0, 0.0) \\
\max(0, 0.0) & \max(0, 0.0)
\end{bmatrix} = \begin{bmatrix}
0.0 & 0.0 \\
0.0 & 0.0 \\
0.0 & 0.0
\end{bmatrix}$$

**K·∫øt qu·∫£ quan tr·ªçng**: T·∫•t c·∫£ neurons trong t·∫•t c·∫£ samples ƒë·ªÅu c√≥ ƒë·∫ßu ra b·∫±ng 0!

**3. Lan truy·ªÅn ƒë·∫øn l·ªõp ƒë·∫ßu ra (pre-activation):**

V·ªõi m·ªói sample: $z = \text{ReLU}(h) \times W + bw$. V√¨ $\text{ReLU}(h)$, $W$, v√† $bw$ ƒë·ªÅu b·∫±ng 0:

$$z = \begin{bmatrix}
z_1^{(1)} & z_2^{(1)} & z_3^{(1)} \\
z_1^{(2)} & z_2^{(2)} & z_3^{(2)} \\
z_1^{(3)} & z_2^{(3)} & z_3^{(3)}
\end{bmatrix} = \begin{bmatrix}
0.0 & 0.0 & 0.0 \\
0.0 & 0.0 & 0.0 \\
0.0 & 0.0 & 0.0
\end{bmatrix}$$

T√≠nh chi ti·∫øt cho sample 1:
- $z_1^{(1)} = (0.0 \times 0.0) + (0.0 \times 0.0) + 0.0 = 0.0$
- $z_2^{(1)} = (0.0 \times 0.0) + (0.0 \times 0.0) + 0.0 = 0.0$
- $z_3^{(1)} = (0.0 \times 0.0) + (0.0 \times 0.0) + 0.0 = 0.0$

**4. √Åp d·ª•ng Softmax:**

V√¨ t·∫•t c·∫£ $z_i$ ƒë·ªÅu b·∫±ng 0 cho m·ªói sample, Softmax s·∫Ω cho x√°c su·∫•t ƒë·ªìng ƒë·ªÅu:

$$\hat{y} = \begin{bmatrix}
\hat{y}_1^{(1)} & \hat{y}_2^{(1)} & \hat{y}_3^{(1)} \\
\hat{y}_1^{(2)} & \hat{y}_2^{(2)} & \hat{y}_3^{(2)} \\
\hat{y}_1^{(3)} & \hat{y}_2^{(3)} & \hat{y}_3^{(3)}
\end{bmatrix} = \begin{bmatrix}
0.333 & 0.333 & 0.333 \\
0.333 & 0.333 & 0.333 \\
0.333 & 0.333 & 0.333
\end{bmatrix}$$

**5. T√≠nh Loss (Cross-Entropy):**

V·ªõi c√¥ng th·ª©c $L = -\sum_i y_i \log(\hat{y}_i)$, loss cho m·ªói sample l√†:

$$L = \begin{bmatrix}
-\log(0.333) \\
-\log(0.333) \\
-\log(0.333)
\end{bmatrix}$$

M√¥ h√¨nh d·ª± ƒëo√°n m·ªçi sample v·ªõi x√°c su·∫•t b·∫±ng nhau cho m·ªói l·ªõp, b·∫•t k·ªÉ nh√£n th·ª±c t·∫ø!

### B∆∞·ªõc 2: Lan truy·ªÅn ng∆∞·ª£c (Backward Propagation) - V·∫•n ƒë·ªÅ ƒê·ªëi x·ª©ng xu·∫•t hi·ªán

**1. T√≠nh $\nabla_z L$ (Gradient l·ªõp ƒë·∫ßu ra):**

V·ªõi Cross-Entropy Loss v√† Softmax, gradient cho m·ªói sample: $\nabla_z L = \hat{y} - y$

$$\nabla_z L = \begin{bmatrix}
\nabla_{z_1}^{(1)} & \nabla_{z_2}^{(1)} & \nabla_{z_3}^{(1)} \\
\nabla_{z_1}^{(2)} & \nabla_{z_2}^{(2)} & \nabla_{z_3}^{(2)} \\
\nabla_{z_1}^{(3)} & \nabla_{z_2}^{(3)} & \nabla_{z_3}^{(3)}
\end{bmatrix} = \begin{bmatrix}
0.333 & 0.333 & 0.333 \\
0.333 & 0.333 & 0.333 \\
0.333 & 0.333 & 0.333
\end{bmatrix} - \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} = \begin{bmatrix}
-0.667 & 0.333 & 0.333 \\
0.333 & -0.667 & 0.333 \\
0.333 & 0.333 & -0.667
\end{bmatrix}$$

**2. T√≠nh $\nabla_{W} L$ (Gradient tr·ªçng s·ªë l·ªõp ƒë·∫ßu ra):**

Gradient ƒë∆∞·ª£c t√≠nh b·∫±ng t·ªïng gradient t·ª´ t·∫•t c·∫£ samples. V√¨ $\text{ReLU}(h)$ cho t·∫•t c·∫£ samples ƒë·ªÅu b·∫±ng 0:

$$\nabla_{W} L = \sum_{i=1}^{3} \text{ReLU}(h^{(i)})^T \times \nabla_z^{(i)} L = \begin{bmatrix}
0.0 \\
0.0
\end{bmatrix} \times (\text{t·ªïng c√°c gradient}) = \begin{bmatrix}
0.0 & 0.0 & 0.0 \\
0.0 & 0.0 & 0.0
\end{bmatrix}$$

T√≠nh chi ti·∫øt:
- $\frac{\partial L}{\partial w_{11}} = 0.0 \times (-0.667) + 0.0 \times 0.333 + 0.0 \times 0.333 = 0.0$
- $\frac{\partial L}{\partial w_{12}} = 0.0 \times 0.333 + 0.0 \times (-0.667) + 0.0 \times 0.333 = 0.0$
- T∆∞∆°ng t·ª±, t·∫•t c·∫£ gradient c·ªßa $W$ ƒë·ªÅu b·∫±ng 0.0

**3. T√≠nh $\nabla_{\text{ReLU}} L$ (Gradient sau ReLU):**

V√¨ $W$ c≈©ng b·∫±ng 0, gradient sau ReLU cho t·∫•t c·∫£ samples:

$$\nabla_{\text{ReLU}} L = W^T \times \nabla_z L = \begin{bmatrix}
0.0 & 0.0 \\
0.0 & 0.0 \\
0.0 & 0.0
\end{bmatrix} \times \nabla_z L = \begin{bmatrix}
0.0 & 0.0 \\
0.0 & 0.0 \\
0.0 & 0.0
\end{bmatrix}$$

**4. T√≠nh $\nabla_h L$ (Gradient tr∆∞·ªõc ReLU):**

V√¨ t·∫•t c·∫£ $h$ ƒë·ªÅu b·∫±ng 0.0, v√† ReLU'(0) = 0:

$$\frac{\partial L}{\partial h_j} = \nabla_{\text{ReLU}_j} L \times \text{ReLU}'(0) = 0.0 \times 0 = 0.0$$

Cho t·∫•t c·∫£ samples v√† t·∫•t c·∫£ neurons: 

$$
\nabla_h L =
\begin{bmatrix}
0.0 & 0.0 \\
0.0 & 0.0 \\
0.0 & 0.0
\end{bmatrix}
$$
**5. T√≠nh $\nabla_M L$ (Gradient tr·ªçng s·ªë l·ªõp ·∫©n):**

Gradient ƒë∆∞·ª£c t√≠nh b·∫±ng t·ªïng gradient t·ª´ t·∫•t c·∫£ samples:

$$\nabla_M L = \sum_{i=1}^{3} [1, x_1^{(i)}, x_2^{(i)}]^T \times \nabla_h^{(i)} L$$

V√¨ t·∫•t c·∫£ $\nabla_h^{(i)} L = [0.0, 0.0]$, n√™n:

$$\nabla_M L = \begin{bmatrix}
0.0 & 0.0 \\
0.0 & 0.0 \\
0.0 & 0.0
\end{bmatrix}$$

**K·∫øt qu·∫£ th·∫£m h·ªça:**

T·∫•t c·∫£ gradient ƒë·ªÅu b·∫±ng 0 cho t·∫•t c·∫£ samples! ƒêi·ªÅu n√†y c√≥ nghƒ©a l√†:
- **T·∫•t c·∫£ tr·ªçng s·ªë s·∫Ω kh√¥ng ƒë∆∞·ª£c c·∫≠p nh·∫≠t**: $\Delta w = -\eta \times 0 = 0$ cho m·ªçi tr·ªçng s·ªë
- **T·∫•t c·∫£ neurons trong l·ªõp ·∫©n s·∫Ω m√£i m√£i c√≥ c√πng gi√° tr·ªã (0.0)** cho m·ªçi sample
- **Ch√∫ng s·∫Ω nh·∫≠n ƒë∆∞·ª£c c√πng gradient (0.0)** trong m·ªçi l·∫ßn c·∫≠p nh·∫≠t, b·∫•t k·ªÉ d·ªØ li·ªáu ƒë·∫ßu v√†o l√† g√¨
- **M·∫°ng s·∫Ω kh√¥ng bao gi·ªù h·ªçc ƒë∆∞·ª£c g√¨!** M√¥ h√¨nh s·∫Ω m√£i m√£i d·ª± ƒëo√°n x√°c su·∫•t ƒë·ªìng ƒë·ªÅu [0.333, 0.333, 0.333] cho m·ªçi sample

**V·∫•n ƒë·ªÅ ƒë·ªëi x·ª©ng**: T·∫•t c·∫£ neurons trong c√πng m·ªôt l·ªõp ·∫©n ƒë·ªÅu c√≥ c√πng gi√° tr·ªã (0.0) v√† nh·∫≠n c√πng gradient (0.0), khi·∫øn ch√∫ng kh√¥ng th·ªÉ ph√¢n bi·ªát v√† h·ªçc c√°c ƒë·∫∑c tr∆∞ng kh√°c nhau. ƒê√¢y ch√≠nh l√† **Symmetry Problem** - v·∫•n ƒë·ªÅ ƒë·ªëi x·ª©ng khi kh·ªüi t·∫°o b·∫±ng 0.

### L·ªùi K·∫øt

Deep learning kh√¥ng ph·∫£i l√† "magic" - m·ªói v·∫•n ƒë·ªÅ ƒë·ªÅu c√≥ nguy√™n nh√¢n to√°n h·ªçc r√µ r√†ng. B·∫±ng c√°ch hi·ªÉu s√¢u c√°c c∆° ch·∫ø n√†y, ch√∫ng ta c√≥ th·ªÉ x√¢y d·ª±ng nh·ªØng m√¥ h√¨nh m·∫°nh m·∫Ω v√† hi·ªáu qu·∫£ h∆°n. 

Hy v·ªçng b√†i vi·∫øt n√†y gi√∫p c√°c b·∫°n hi·ªÉu r√µ h∆°n v·ªÅ ba v·∫•n ƒë·ªÅ "kinh ƒëi·ªÉn" n√†y v√† c√°ch kh·∫Øc ph·ª•c ch√∫ng. N·∫øu c√≥ c√¢u h·ªèi ho·∫∑c mu·ªën th·∫£o lu·∫≠n th√™m, ƒë·ª´ng ng·∫ßn ng·∫°i ƒë·ªÉ l·∫°i comment nh√©!

Happy learning! üöÄ
**References**
[1] ·∫¢nh ƒë∆∞·ª£c l·∫•y t·ª´ slide Insight into Multi-layer Perceptron, AIO Module 06 ‚Äì Tu·∫ßn 3, AIO 2025, Dr. Quang Vinh.