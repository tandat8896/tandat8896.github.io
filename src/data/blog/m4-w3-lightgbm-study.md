---
title: "LightGBM Study - Thu·∫≠t to√°n Gradient Boosting hi·ªáu qu·∫£"
pubDatetime: 2025-01-16T14:00:00Z
featured: false
description: "T√¨m hi·ªÉu chi ti·∫øt v·ªÅ LightGBM, thu·∫≠t to√°n gradient boosting v·ªõi GOSS sampling v√† histogram-based splitting"
tags: ["machine-learning", "lightgbm", "gradient-boosting", "algorithm", "optimization"]
---

# LightGBM Study - Thu·∫≠t to√°n Gradient Boosting hi·ªáu qu·∫£

## H√†m m·ª•c ti√™u (Objective) trong LightGBM

* **H√†m m·∫•t m√°t cho Regression:**

$$
\sum_{i=1}^{N} \mathcal{L}(y_i, \bar{y}_i) \quad \text{Where} \quad \mathcal{L}(y_i, \bar{y}_i) = \frac{1}{2}(y_i - \bar{y}_i)^2
$$

* **Gradient v√† Hessian:**

$$
g_i = \frac{\partial \mathcal{L}(y_i, \bar{y}_i)}{\partial \bar{y}_i} = \bar{y}_i - y_i, \quad h_i = \frac{\partial^2 \mathcal{L}(y_i, \bar{y}_i)}{\partial \bar{y}_i^2} = 1
$$

* **Gain c·ªßa LightGBM:**

$$
\text{Gain} = \frac{1}{2}\left( \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} \right) - \gamma
$$

* **Leaf weight:**

$$
w_j^* = -\frac{G_j}{H_j+\lambda}
$$

* **Model update:**

$$
F_t(x) = F_{t-1}(x) + \eta \cdot f_t(x)
$$

---

## V√≠ D·ª• T√≠nh Tay - LightGBM (3 Bins)

### **Step 1: Initialization**

| Age (X) | Chol (y) |
|:---:|:---:|
| 25 | 180 |
| 29 | 204 |
| 35 | 220 |
| 39 | 203 |
| 42 | 240 |
| 45 | 250 |
| 48 | 234 |
| 52 | 280 |
| 55 | 290 |
| 59 | 260 |
| 62 | 300 |
| 67 | 269 |
| 70 | 320 |
| 75 | 350 |
| 80 | 380 |

**F0(x) l√† gi√° tr·ªã t·ªëi ∆∞u:**
$$F_0 = \frac{1}{N} \sum_{i=1}^{N} y_i = \frac{180 + 204 + 220 + 203 + 240 + 250 + 234 + 280 + 290 + 260 + 300 + 269 + 320 + 350 + 380}{15} = 262.13$$

### **Step 2A: Gradients & Hessians**

**T√≠nh gradients:**
$$g_i = F_0 - y_i$$

| Age (X) | Chol (y) | Gradients ($g_i$) | \|Gradients\| |
|:---:|:---:|:---:|:---:|
| 25 | 180 | $262.13 - 180 = 82.13$ | 82.13 |
| 29 | 204 | $262.13 - 204 = 58.13$ | 58.13 |
| 35 | 220 | $262.13 - 220 = 42.13$ | 42.13 |
| 39 | 203 | $262.13 - 203 = 59.13$ | 59.13 |
| 42 | 240 | $262.13 - 240 = 22.13$ | 22.13 |
| 45 | 250 | $262.13 - 250 = 12.13$ | 12.13 |
| 48 | 234 | $262.13 - 234 = 28.13$ | 28.13 |
| 52 | 280 | $262.13 - 280 = -17.87$ | 17.87 |
| 55 | 290 | $262.13 - 290 = -27.87$ | 27.87 |
| 59 | 260 | $262.13 - 260 = 2.13$ | 2.13 |
| 62 | 300 | $262.13 - 300 = -37.87$ | 37.87 |
| 67 | 269 | $262.13 - 269 = -6.87$ | 6.87 |
| 70 | 320 | $262.13 - 320 = -57.87$ | 57.87 |
| 75 | 350 | $262.13 - 350 = -87.87$ | 87.87 |
| 80 | 380 | $262.13 - 380 = -117.87$ | 117.87 |

### **Step 2B: GOSS Sampling**

| Rank | Age | Gradient | \|Gradient\| | Selection |
|:---:|:---:|:---:|:---:|:---:|
| 1 | 80 | -117.87 | 117.87 | ‚úÖ Top 20% |
| 2 | 25 | 82.13 | 82.13 | ‚úÖ Top 20% |
| 3 | 75 | -87.87 | 87.87 | ‚úÖ Top 20% |
| 4 | 29 | 58.13 | 58.13 | |
| 5 | 39 | 59.13 | 59.13 | ‚úÖ Random 10% |
| 6 | 70 | -57.87 | 57.87 | ‚úÖ Random 10% |
| 7 | 35 | 42.13 | 42.13 | ‚úÖ Random 10% |
| 8 | 62 | -37.87 | 37.87 | |
| 9 | 55 | -27.87 | 27.87 | |
| 10 | 48 | 28.13 | 28.13 | |
| 11 | 42 | 22.13 | 22.13 | |
| 12 | 45 | 12.13 | 12.13 | |
| 13 | 52 | -17.87 | 17.87 | |
| 14 | 67 | -6.87 | 6.87 | |
| 15 | 59 | 2.13 | 2.13 | |

- **Selected samples:** [80, 25, 75, 39, 70, 35]
- **Weights:** [1.0, 1.0, 1.0, 6.67, 6.67, 6.67] (v√¨ 6.67 = (1-0.2)/0.1)

### **Step 2C: Histogram-based Threshold Finding (3 Bins)**

**Ch·ªâ t·∫°o histogram t·ª´ samples ƒë∆∞·ª£c GOSS ch·ªçn:**
- **GOSS selected samples:** [80, 25, 75, 39, 70, 35]
- **S·∫Øp x·∫øp theo Age:** [25, 35, 39, 70, 75, 80]
- **Min Age:** 25, **Max Age:** 80
- **Bin width:** (80 - 25) / 3 = 18.33
- **3 Bins:** [25-43.33], [43.33-61.67], [61.67-80]
- **Thresholds:** [43.33, 61.67] (ch·ªâ 2 thresholds cho 3 bins)

### **Step 2D: Gains - T√¨m Best Split**

**T√≠nh Gain cho t·ª´ng threshold (ch·ªâ v·ªõi samples ƒë∆∞·ª£c ch·ªçn):**

| Thresh | Left Samples | Right Samples | $G_L$ | $H_L$ | $G_R$ | $H_R$ | Gain |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 43.33 | [25,35,39] | [70,75,80] | 82.13+42.13+59.13 = 183.39 | 3 | -57.87-87.87-117.87 = -263.61 | 3 | **6789.45** |
| 61.67 | [25,35,39,70] | [75,80] | 183.39-57.87 = 125.52 | 4 | -87.87-117.87 = -205.74 | 2 | 4123.89 |

**Best threshold: 43.33 (Gain = 6789.45)**

### **Step 2E: Split - Leaf-wise Growth**

**Best threshold: 43.33 (Gain = 6789.45)**

### **Step 2F: Leaf Weights**

**Left node (Age ‚â§ 43.33):**
$$w_L = -\frac{183.39}{3+1} = -45.85$$

**Right node (Age > 43.33):**
$$w_R = -\frac{-263.61}{3+1} = 65.90$$

### **Step 2G: Model Update**

**Learning rate Œ∑ = 0.1:**

C√¥ng th·ª©c c·∫≠p nh·∫≠t m√¥ h√¨nh:
- F1(x) = F0(x) + Œ∑ √ó f1(x)
- F1(x) = 262.13 + 0.1 √ó f1(x)

**K·∫øt qu·∫£ sau v√≤ng l·∫∑p 1:**

| Age (X) | Chol (y) | F1(x) |
|:---:|:---:|:---:|
| 25 | 180 | 257.55 |
| 29 | 204 | 257.55 |
| 35 | 220 | 257.55 |
| 39 | 203 | 257.55 |
| 42 | 240 | 257.55 |
| 45 | 250 | 268.72 |
| 48 | 234 | 268.72 |
| 52 | 280 | 268.72 |
| 55 | 290 | 268.72 |
| 59 | 260 | 268.72 |
| 62 | 300 | 268.72 |
| 67 | 269 | 268.72 |
| 70 | 320 | 268.72 |
| 75 | 350 | 268.72 |
| 80 | 380 | 268.72 |

---

## **Step 3: Loop 2 - Leaf-wise Growth**

### **Step 3A: Gradients & Hessians cho Loop 2**

| Age (X) | Chol (y) | $F_1(x)$ | Gradients ($g_i = F_1(x) - y_i$) | \|Gradients\| |
|:---:|:---:|:---:|:---:|:---:|
| 25 | 180 | 257.55 | $257.55 - 180 = 77.55$ | 77.55 |
| 29 | 204 | 257.55 | $257.55 - 204 = 53.55$ | 53.55 |
| 35 | 220 | 257.55 | $257.55 - 220 = 37.55$ | 37.55 |
| 39 | 203 | 257.55 | $257.55 - 203 = 54.55$ | 54.55 |
| 42 | 240 | 257.55 | $257.55 - 240 = 17.55$ | 17.55 |
| 45 | 250 | 268.72 | $268.72 - 250 = 18.72$ | 18.72 |
| 48 | 234 | 268.72 | $268.72 - 234 = 34.72$ | 34.72 |
| 52 | 280 | 268.72 | $268.72 - 280 = -11.28$ | 11.28 |
| 55 | 290 | 268.72 | $268.72 - 290 = -21.28$ | 21.28 |
| 59 | 260 | 268.72 | $268.72 - 260 = 8.72$ | 8.72 |
| 62 | 300 | 268.72 | $268.72 - 300 = -31.28$ | 31.28 |
| 67 | 269 | 268.72 | $268.72 - 269 = -0.28$ | 0.28 |
| 70 | 320 | 268.72 | $268.72 - 320 = -51.28$ | 51.28 |
| 75 | 350 | 268.72 | $268.72 - 350 = -81.28$ | 81.28 |
| 80 | 380 | 268.72 | $268.72 - 380 = -111.28$ | 111.28 |

### **Step 3B: GOSS Sampling cho Loop 2**

| Rank | Age | Gradient | \|Gradient\| | Selection |
|:---:|:---:|:---:|:---:|:---:|
| 1 | 80 | -111.28 | 111.28 | ‚úÖ Top 20% |
| 2 | 25 | 77.55 | 77.55 | ‚úÖ Top 20% |
| 3 | 75 | -81.28 | 81.28 | ‚úÖ Top 20% |
| 4 | 29 | 53.55 | 53.55 | |
| 5 | 39 | 54.55 | 54.55 | ‚úÖ Random 10% |
| 6 | 70 | -51.28 | 51.28 | ‚úÖ Random 10% |
| 7 | 35 | 37.55 | 37.55 | ‚úÖ Random 10% |
| 8 | 62 | -31.28 | 31.28 | |
| 9 | 48 | 34.72 | 34.72 | |
| 10 | 55 | -21.28 | 21.28 | |
| 11 | 45 | 18.72 | 18.72 | |
| 12 | 42 | 17.55 | 17.55 | |
| 13 | 52 | -11.28 | 11.28 | |
| 14 | 59 | 8.72 | 8.72 | |
| 15 | 67 | -0.28 | 0.28 | |

- **Selected samples:** [80, 25, 75, 39, 70, 35]
- **Weights:** [1.0, 1.0, 1.0, 6.67, 6.67, 6.67]

### **Step 3C: Histogram-based Threshold Finding cho Loop 2 (3 Bins)**

**Ch·ªâ t·∫°o histogram t·ª´ samples ƒë∆∞·ª£c GOSS ch·ªçn:**
- **GOSS selected samples:** [80, 25, 75, 39, 70, 35]
- **S·∫Øp x·∫øp theo Age:** [25, 35, 39, 70, 75, 80]
- **Min Age:** 25, **Max Age:** 80
- **Bin width:** (80 - 25) / 3 = 18.33
- **3 Bins:** [25-43.33], [43.33-61.67], [61.67-80]
- **Thresholds:** [43.33, 61.67] (ch·ªâ 2 thresholds cho 3 bins)

### **Step 3D: Gains - T√¨m Best Split cho Loop 2**

**T√≠nh Gain cho t·ª´ng threshold (ch·ªâ v·ªõi samples ƒë∆∞·ª£c ch·ªçn):**

| Thresh | Left Samples | Right Samples | $G_L$ | $H_L$ | $G_R$ | $H_R$ | Gain |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 43.33 | [25,35,39] | [70,75,80] | 77.55+37.55+54.55 = 169.65 | 3 | -51.28-81.28-111.28 = -243.84 | 3 | **6789.45** |
| 61.67 | [25,35,39,70] | [75,80] | 169.65-51.28 = 118.37 | 4 | -81.28-111.28 = -192.56 | 2 | 4123.89 |

**Best threshold: 43.33 (Gain = 6789.45)**

### **Step 3E: Split - Leaf-wise Growth cho Loop 2**

**Best threshold: 43.33 (Gain = 6789.45)**

### **Step 3F: Leaf Weights cho Loop 2**

**Left node (Age ‚â§ 43.33):**
$$w_L = -\frac{169.65}{3+1} = -42.41$$

**Right node (Age > 43.33):**
$$w_R = -\frac{-243.84}{3+1} = 60.96$$

### **Step 3G: Model Update cho Loop 2**

**Learning rate Œ∑ = 0.1:**

C√¥ng th·ª©c c·∫≠p nh·∫≠t m√¥ h√¨nh:
- F2(x) = F1(x) + Œ∑ √ó f2(x)
- F2(x) = F1(x) + 0.1 √ó f2(x)

**K·∫øt qu·∫£ sau v√≤ng l·∫∑p 2:**

| Age (X) | Chol (y) | F1(x) | F2(x) |
|:---:|:---:|:---:|:---:|
| 25 | 180 | 257.55 | 253.31 |
| 29 | 204 | 257.55 | 253.31 |
| 35 | 220 | 257.55 | 253.31 |
| 39 | 203 | 257.55 | 253.31 |
| 42 | 240 | 257.55 | 253.31 |
| 45 | 250 | 268.72 | 274.82 |
| 48 | 234 | 268.72 | 274.82 |
| 52 | 280 | 268.72 | 274.82 |
| 55 | 290 | 268.72 | 274.82 |
| 59 | 260 | 268.72 | 274.82 |
| 62 | 300 | 268.72 | 274.82 |
| 67 | 269 | 268.72 | 274.82 |
| 70 | 320 | 268.72 | 274.82 |
| 75 | 350 | 268.72 | 274.82 |
| 80 | 380 | 268.72 | 274.82 |

---

## **Final Model**

$$F_M(x) = F_0(x) + \sum_{t=1}^{M} \eta \cdot f_t(x)$$

---

## **K·∫øt lu·∫≠n**

### **üéØ Nh·ªØng ƒëi·ªÉm n·ªïi b·∫≠t c·ªßa LightGBM**

**1. GOSS Sampling (Gradient-based One-Side Sampling)**
- Ch·ªçn 20% samples c√≥ gradient l·ªõn nh·∫•t (quan tr·ªçng nh·∫•t)
- Ch·ªçn ng·∫´u nhi√™n 10% samples c√≤n l·∫°i
- Gi·∫£m ƒë√°ng k·ªÉ th·ªùi gian training m√† v·∫´n gi·ªØ ƒë∆∞·ª£c ƒë·ªô ch√≠nh x√°c

**2. Histogram-based Algorithm**
- Chia d·ªØ li·ªáu th√†nh c√°c bins thay v√¨ x·ª≠ l√Ω t·ª´ng gi√° tr·ªã
- Gi·∫£m s·ªë l∆∞·ª£ng thresholds c·∫ßn ki·ªÉm tra
- TƒÉng t·ªëc ƒë·ªô t√¨m ki·∫øm split point t·ªëi ∆∞u

**3. Leaf-wise Growth**
- X√¢y d·ª±ng c√¢y theo chi·ªÅu s√¢u thay v√¨ level-wise
- T·∫≠p trung v√†o c√°c leaf c√≥ loss cao nh·∫•t
- T·∫°o ra c√¢y c√¢n b·∫±ng v√† hi·ªáu qu·∫£ h∆°n

### **‚ö° So s√°nh v·ªõi XGBoost**

| ƒê·∫∑c ƒëi·ªÉm | XGBoost | LightGBM |
|:---:|:---:|:---:|
| **Sampling** | T·∫•t c·∫£ samples | GOSS sampling |
| **Splitting** | Pre-sorted algorithm | Histogram-based |
| **Growth** | Level-wise | Leaf-wise |
| **Memory** | Cao h∆°n | Th·∫•p h∆°n |
| **Speed** | Ch·∫≠m h∆°n | Nhanh h∆°n |
| **Accuracy** | T∆∞∆°ng ƒë∆∞∆°ng | T∆∞∆°ng ƒë∆∞∆°ng |

### **üîß Hyperparameters quan tr·ªçng**

- **`num_leaves`**: S·ªë l√° t·ªëi ƒëa (31-255)
- **`learning_rate`**: T·ªëc ƒë·ªô h·ªçc (0.01-0.3)
- **`feature_fraction`**: T·ª∑ l·ªá features s·ª≠ d·ª•ng (0.5-1.0)
- **`bagging_fraction`**: T·ª∑ l·ªá samples s·ª≠ d·ª•ng (0.5-1.0)
- **`lambda_l1`, `lambda_l2`**: Regularization

### **üí° Khi n√†o n√™n s·ª≠ d·ª•ng LightGBM**

‚úÖ **N√™n d√πng khi:**
- Dataset l·ªõn (>10K samples)
- C·∫ßn training nhanh
- Memory h·∫°n ch·∫ø
- C·∫ßn model nh·∫π

‚ùå **Kh√¥ng n√™n d√πng khi:**
- Dataset nh·ªè (<1K samples)
- C·∫ßn interpretability cao
- Overfitting d·ªÖ x·∫£y ra

### **üöÄ T∆∞∆°ng lai c·ªßa LightGBM**

LightGBM ti·∫øp t·ª•c ƒë∆∞·ª£c ph√°t tri·ªÉn v·ªõi:
- **GPU support** cho training nhanh h∆°n
- **Categorical features** handling t·ªët h∆°n
- **Distributed training** cho big data
- **AutoML integration** v·ªõi c√°c framework kh√°c

---

> **üìö T√†i li·ªáu tham kh·∫£o:**
> - [LightGBM Official Documentation](https://lightgbm.readthedocs.io/)
> - [Gradient-based One-Side Sampling Paper](https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html)
> - [Histogram-based Algorithm Paper](https://arxiv.org/abs/1603.02754)
